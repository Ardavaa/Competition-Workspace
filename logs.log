2024-08-28 11:14:07,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-28 11:14:07,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-28 11:14:07,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-28 11:14:07,799:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-28 11:14:08,257:INFO:PyCaret ClassificationExperiment
2024-08-28 11:14:08,257:INFO:Logging name: clf-default-name
2024-08-28 11:14:08,258:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-28 11:14:08,258:INFO:version 3.3.2
2024-08-28 11:14:08,258:INFO:Initializing setup()
2024-08-28 11:14:08,258:INFO:self.USI: c723
2024-08-28 11:14:08,258:INFO:self._variable_keys: {'y', 'fold_shuffle_param', 'fold_groups_param', 'X_train', 'X_test', '_ml_usecase', 'logging_param', 'html_param', 'fold_generator', 'data', 'seed', 'X', 'n_jobs_param', 'log_plots_param', '_available_plots', 'target_param', 'exp_id', 'is_multiclass', 'exp_name_log', 'pipeline', 'idx', 'USI', 'fix_imbalance', 'memory', 'gpu_param', 'y_test', 'gpu_n_jobs_param', 'y_train'}
2024-08-28 11:14:08,258:INFO:Checking environment
2024-08-28 11:14:08,258:INFO:python_version: 3.11.9
2024-08-28 11:14:08,258:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-28 11:14:08,258:INFO:machine: AMD64
2024-08-28 11:14:08,258:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-28 11:14:08,263:INFO:Memory: svmem(total=16867028992, available=4095430656, percent=75.7, used=12771598336, free=4095430656)
2024-08-28 11:14:08,263:INFO:Physical Core: 6
2024-08-28 11:14:08,263:INFO:Logical Core: 12
2024-08-28 11:14:08,263:INFO:Checking libraries
2024-08-28 11:14:08,263:INFO:System:
2024-08-28 11:14:08,263:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-28 11:14:08,263:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-28 11:14:08,263:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-28 11:14:08,263:INFO:PyCaret required dependencies:
2024-08-28 11:14:08,336:INFO:                 pip: 23.2.1
2024-08-28 11:14:08,336:INFO:          setuptools: 67.8.0
2024-08-28 11:14:08,336:INFO:             pycaret: 3.3.2
2024-08-28 11:14:08,336:INFO:             IPython: 8.14.0
2024-08-28 11:14:08,337:INFO:          ipywidgets: 8.1.5
2024-08-28 11:14:08,337:INFO:                tqdm: 4.66.5
2024-08-28 11:14:08,337:INFO:               numpy: 1.24.3
2024-08-28 11:14:08,337:INFO:              pandas: 2.0.3
2024-08-28 11:14:08,337:INFO:              jinja2: 3.1.4
2024-08-28 11:14:08,337:INFO:               scipy: 1.10.1
2024-08-28 11:14:08,337:INFO:              joblib: 1.2.0
2024-08-28 11:14:08,337:INFO:             sklearn: 1.4.2
2024-08-28 11:14:08,337:INFO:                pyod: 2.0.1
2024-08-28 11:14:08,337:INFO:            imblearn: 0.12.3
2024-08-28 11:14:08,337:INFO:   category_encoders: 2.6.3
2024-08-28 11:14:08,337:INFO:            lightgbm: 4.5.0
2024-08-28 11:14:08,337:INFO:               numba: 0.60.0
2024-08-28 11:14:08,337:INFO:            requests: 2.32.3
2024-08-28 11:14:08,337:INFO:          matplotlib: 3.7.1
2024-08-28 11:14:08,337:INFO:          scikitplot: 0.3.7
2024-08-28 11:14:08,337:INFO:         yellowbrick: 1.5
2024-08-28 11:14:08,337:INFO:              plotly: 5.16.1
2024-08-28 11:14:08,337:INFO:    plotly-resampler: Not installed
2024-08-28 11:14:08,337:INFO:             kaleido: 0.2.1
2024-08-28 11:14:08,337:INFO:           schemdraw: 0.15
2024-08-28 11:14:08,337:INFO:         statsmodels: 0.14.2
2024-08-28 11:14:08,337:INFO:              sktime: 0.26.0
2024-08-28 11:14:08,337:INFO:               tbats: 1.1.3
2024-08-28 11:14:08,337:INFO:            pmdarima: 2.0.4
2024-08-28 11:14:08,337:INFO:              psutil: 5.9.0
2024-08-28 11:14:08,337:INFO:          markupsafe: 2.1.3
2024-08-28 11:14:08,337:INFO:             pickle5: Not installed
2024-08-28 11:14:08,337:INFO:         cloudpickle: 3.0.0
2024-08-28 11:14:08,337:INFO:         deprecation: 2.1.0
2024-08-28 11:14:08,337:INFO:              xxhash: 3.5.0
2024-08-28 11:14:08,337:INFO:           wurlitzer: Not installed
2024-08-28 11:14:08,337:INFO:PyCaret optional dependencies:
2024-08-28 11:14:11,763:INFO:                shap: Not installed
2024-08-28 11:14:11,763:INFO:           interpret: Not installed
2024-08-28 11:14:11,764:INFO:                umap: Not installed
2024-08-28 11:14:11,764:INFO:     ydata_profiling: Not installed
2024-08-28 11:14:11,764:INFO:  explainerdashboard: Not installed
2024-08-28 11:14:11,764:INFO:             autoviz: Not installed
2024-08-28 11:14:11,764:INFO:           fairlearn: Not installed
2024-08-28 11:14:11,764:INFO:          deepchecks: Not installed
2024-08-28 11:14:11,764:INFO:             xgboost: 2.0.2
2024-08-28 11:14:11,764:INFO:            catboost: Not installed
2024-08-28 11:14:11,764:INFO:              kmodes: Not installed
2024-08-28 11:14:11,764:INFO:             mlxtend: Not installed
2024-08-28 11:14:11,764:INFO:       statsforecast: Not installed
2024-08-28 11:14:11,764:INFO:        tune_sklearn: Not installed
2024-08-28 11:14:11,764:INFO:                 ray: Not installed
2024-08-28 11:14:11,764:INFO:            hyperopt: Not installed
2024-08-28 11:14:11,764:INFO:              optuna: Not installed
2024-08-28 11:14:11,764:INFO:               skopt: Not installed
2024-08-28 11:14:11,764:INFO:              mlflow: Not installed
2024-08-28 11:14:11,764:INFO:              gradio: 4.41.0
2024-08-28 11:14:11,764:INFO:             fastapi: 0.112.1
2024-08-28 11:14:11,764:INFO:             uvicorn: 0.30.6
2024-08-28 11:14:11,764:INFO:              m2cgen: Not installed
2024-08-28 11:14:11,764:INFO:           evidently: Not installed
2024-08-28 11:14:11,764:INFO:               fugue: Not installed
2024-08-28 11:14:11,764:INFO:           streamlit: Not installed
2024-08-28 11:14:11,764:INFO:             prophet: Not installed
2024-08-28 11:14:11,764:INFO:None
2024-08-28 11:14:11,764:INFO:Set up data.
2024-08-28 11:15:47,911:INFO:PyCaret ClassificationExperiment
2024-08-28 11:15:47,911:INFO:Logging name: clf-default-name
2024-08-28 11:15:47,911:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-28 11:15:47,911:INFO:version 3.3.2
2024-08-28 11:15:47,911:INFO:Initializing setup()
2024-08-28 11:15:47,911:INFO:self.USI: 4c29
2024-08-28 11:15:47,911:INFO:self._variable_keys: {'y', 'fold_shuffle_param', 'fold_groups_param', 'X_train', 'X_test', '_ml_usecase', 'logging_param', 'html_param', 'fold_generator', 'data', 'seed', 'X', 'n_jobs_param', 'log_plots_param', '_available_plots', 'target_param', 'exp_id', 'is_multiclass', 'exp_name_log', 'pipeline', 'idx', 'USI', 'fix_imbalance', 'memory', 'gpu_param', 'y_test', 'gpu_n_jobs_param', 'y_train'}
2024-08-28 11:15:47,911:INFO:Checking environment
2024-08-28 11:15:47,911:INFO:python_version: 3.11.9
2024-08-28 11:15:47,911:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-28 11:15:47,911:INFO:machine: AMD64
2024-08-28 11:15:47,911:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-28 11:15:47,916:INFO:Memory: svmem(total=16867028992, available=3638771712, percent=78.4, used=13228257280, free=3638771712)
2024-08-28 11:15:47,916:INFO:Physical Core: 6
2024-08-28 11:15:47,916:INFO:Logical Core: 12
2024-08-28 11:15:47,916:INFO:Checking libraries
2024-08-28 11:15:47,916:INFO:System:
2024-08-28 11:15:47,916:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-28 11:15:47,916:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-28 11:15:47,916:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-28 11:15:47,916:INFO:PyCaret required dependencies:
2024-08-28 11:15:47,916:INFO:                 pip: 23.2.1
2024-08-28 11:15:47,916:INFO:          setuptools: 67.8.0
2024-08-28 11:15:47,916:INFO:             pycaret: 3.3.2
2024-08-28 11:15:47,918:INFO:             IPython: 8.14.0
2024-08-28 11:15:47,918:INFO:          ipywidgets: 8.1.5
2024-08-28 11:15:47,918:INFO:                tqdm: 4.66.5
2024-08-28 11:15:47,918:INFO:               numpy: 1.24.3
2024-08-28 11:15:47,918:INFO:              pandas: 2.0.3
2024-08-28 11:15:47,918:INFO:              jinja2: 3.1.4
2024-08-28 11:15:47,918:INFO:               scipy: 1.10.1
2024-08-28 11:15:47,918:INFO:              joblib: 1.2.0
2024-08-28 11:15:47,918:INFO:             sklearn: 1.4.2
2024-08-28 11:15:47,918:INFO:                pyod: 2.0.1
2024-08-28 11:15:47,918:INFO:            imblearn: 0.12.3
2024-08-28 11:15:47,918:INFO:   category_encoders: 2.6.3
2024-08-28 11:15:47,918:INFO:            lightgbm: 4.5.0
2024-08-28 11:15:47,918:INFO:               numba: 0.60.0
2024-08-28 11:15:47,918:INFO:            requests: 2.32.3
2024-08-28 11:15:47,918:INFO:          matplotlib: 3.7.1
2024-08-28 11:15:47,918:INFO:          scikitplot: 0.3.7
2024-08-28 11:15:47,918:INFO:         yellowbrick: 1.5
2024-08-28 11:15:47,918:INFO:              plotly: 5.16.1
2024-08-28 11:15:47,918:INFO:    plotly-resampler: Not installed
2024-08-28 11:15:47,918:INFO:             kaleido: 0.2.1
2024-08-28 11:15:47,918:INFO:           schemdraw: 0.15
2024-08-28 11:15:47,918:INFO:         statsmodels: 0.14.2
2024-08-28 11:15:47,918:INFO:              sktime: 0.26.0
2024-08-28 11:15:47,918:INFO:               tbats: 1.1.3
2024-08-28 11:15:47,918:INFO:            pmdarima: 2.0.4
2024-08-28 11:15:47,918:INFO:              psutil: 5.9.0
2024-08-28 11:15:47,918:INFO:          markupsafe: 2.1.3
2024-08-28 11:15:47,918:INFO:             pickle5: Not installed
2024-08-28 11:15:47,918:INFO:         cloudpickle: 3.0.0
2024-08-28 11:15:47,918:INFO:         deprecation: 2.1.0
2024-08-28 11:15:47,918:INFO:              xxhash: 3.5.0
2024-08-28 11:15:47,918:INFO:           wurlitzer: Not installed
2024-08-28 11:15:47,919:INFO:PyCaret optional dependencies:
2024-08-28 11:15:47,919:INFO:                shap: Not installed
2024-08-28 11:15:47,919:INFO:           interpret: Not installed
2024-08-28 11:15:47,919:INFO:                umap: Not installed
2024-08-28 11:15:47,919:INFO:     ydata_profiling: Not installed
2024-08-28 11:15:47,919:INFO:  explainerdashboard: Not installed
2024-08-28 11:15:47,919:INFO:             autoviz: Not installed
2024-08-28 11:15:47,919:INFO:           fairlearn: Not installed
2024-08-28 11:15:47,919:INFO:          deepchecks: Not installed
2024-08-28 11:15:47,919:INFO:             xgboost: 2.0.2
2024-08-28 11:15:47,919:INFO:            catboost: Not installed
2024-08-28 11:15:47,919:INFO:              kmodes: Not installed
2024-08-28 11:15:47,919:INFO:             mlxtend: Not installed
2024-08-28 11:15:47,919:INFO:       statsforecast: Not installed
2024-08-28 11:15:47,919:INFO:        tune_sklearn: Not installed
2024-08-28 11:15:47,919:INFO:                 ray: Not installed
2024-08-28 11:15:47,919:INFO:            hyperopt: Not installed
2024-08-28 11:15:47,919:INFO:              optuna: Not installed
2024-08-28 11:15:47,919:INFO:               skopt: Not installed
2024-08-28 11:15:47,919:INFO:              mlflow: Not installed
2024-08-28 11:15:47,919:INFO:              gradio: 4.41.0
2024-08-28 11:15:47,919:INFO:             fastapi: 0.112.1
2024-08-28 11:15:47,919:INFO:             uvicorn: 0.30.6
2024-08-28 11:15:47,919:INFO:              m2cgen: Not installed
2024-08-28 11:15:47,919:INFO:           evidently: Not installed
2024-08-28 11:15:47,919:INFO:               fugue: Not installed
2024-08-28 11:15:47,919:INFO:           streamlit: Not installed
2024-08-28 11:15:47,919:INFO:             prophet: Not installed
2024-08-28 11:15:47,919:INFO:None
2024-08-28 11:15:47,919:INFO:Set up data.
2024-08-28 11:15:47,933:INFO:Set up folding strategy.
2024-08-28 11:15:47,933:INFO:Set up train/test split.
2024-08-28 11:15:47,949:INFO:Set up index.
2024-08-28 11:15:47,949:INFO:Assigning column types.
2024-08-28 11:15:47,958:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-28 11:15:48,022:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,026:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,058:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,096:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,120:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,123:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-28 11:15:48,159:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,181:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,184:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,221:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-28 11:15:48,244:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,246:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,246:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-28 11:15:48,303:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,366:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,369:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,371:INFO:Preparing preprocessing pipeline...
2024-08-28 11:15:48,373:INFO:Set up simple imputation.
2024-08-28 11:15:48,375:INFO:Set up column name cleaning.
2024-08-28 11:15:48,411:INFO:Finished creating preprocessing pipeline.
2024-08-28 11:15:48,416:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(ad...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-28 11:15:48,416:INFO:Creating final display dataframe.
2024-08-28 11:15:48,517:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26069, 15)
4        Transformed data shape       (26069, 15)
5   Transformed train set shape       (18248, 15)
6    Transformed test set shape        (7821, 15)
7              Numeric features                14
8      Rows with missing values              1.8%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              4c29
2024-08-28 11:15:48,587:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,649:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-28 11:15:48,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-28 11:15:48,652:INFO:setup() successfully completed in 0.74s...............
2024-08-28 11:15:48,652:INFO:Initializing compare_models()
2024-08-28 11:15:48,652:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-28 11:15:48,652:INFO:Checking exceptions
2024-08-28 11:15:48,660:INFO:Preparing display monitor
2024-08-28 11:15:48,680:INFO:Initializing Logistic Regression
2024-08-28 11:15:48,681:INFO:Total runtime is 0.0 minutes
2024-08-28 11:15:48,684:INFO:SubProcess create_model() called ==================================
2024-08-28 11:15:48,684:INFO:Initializing create_model()
2024-08-28 11:15:48,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:15:48,684:INFO:Checking exceptions
2024-08-28 11:15:48,684:INFO:Importing libraries
2024-08-28 11:15:48,684:INFO:Copying training dataset
2024-08-28 11:15:48,701:INFO:Defining folds
2024-08-28 11:15:48,701:INFO:Declaring metric variables
2024-08-28 11:15:48,706:INFO:Importing untrained model
2024-08-28 11:15:48,711:INFO:Logistic Regression Imported successfully
2024-08-28 11:15:48,719:INFO:Starting cross validation
2024-08-28 11:15:48,721:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:15:56,575:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,635:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,650:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,663:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,671:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,694:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,698:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,720:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,726:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-28 11:15:56,770:INFO:Calculating mean and std
2024-08-28 11:15:56,773:INFO:Creating metrics dataframe
2024-08-28 11:15:56,777:INFO:Uploading results into container
2024-08-28 11:15:56,778:INFO:Uploading model into container now
2024-08-28 11:15:56,779:INFO:_master_model_container: 1
2024-08-28 11:15:56,779:INFO:_display_container: 2
2024-08-28 11:15:56,780:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-28 11:15:56,780:INFO:create_model() successfully completed......................................
2024-08-28 11:15:56,932:INFO:SubProcess create_model() end ==================================
2024-08-28 11:15:56,932:INFO:Creating metrics dataframe
2024-08-28 11:15:56,938:INFO:Initializing K Neighbors Classifier
2024-08-28 11:15:56,938:INFO:Total runtime is 0.13761899868647257 minutes
2024-08-28 11:15:56,941:INFO:SubProcess create_model() called ==================================
2024-08-28 11:15:56,941:INFO:Initializing create_model()
2024-08-28 11:15:56,941:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:15:56,941:INFO:Checking exceptions
2024-08-28 11:15:56,941:INFO:Importing libraries
2024-08-28 11:15:56,941:INFO:Copying training dataset
2024-08-28 11:15:56,953:INFO:Defining folds
2024-08-28 11:15:56,953:INFO:Declaring metric variables
2024-08-28 11:15:56,956:INFO:Importing untrained model
2024-08-28 11:15:56,959:INFO:K Neighbors Classifier Imported successfully
2024-08-28 11:15:56,963:INFO:Starting cross validation
2024-08-28 11:15:56,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:15:59,165:INFO:Calculating mean and std
2024-08-28 11:15:59,166:INFO:Creating metrics dataframe
2024-08-28 11:15:59,168:INFO:Uploading results into container
2024-08-28 11:15:59,168:INFO:Uploading model into container now
2024-08-28 11:15:59,169:INFO:_master_model_container: 2
2024-08-28 11:15:59,169:INFO:_display_container: 2
2024-08-28 11:15:59,170:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-28 11:15:59,170:INFO:create_model() successfully completed......................................
2024-08-28 11:15:59,291:INFO:SubProcess create_model() end ==================================
2024-08-28 11:15:59,291:INFO:Creating metrics dataframe
2024-08-28 11:15:59,298:INFO:Initializing Naive Bayes
2024-08-28 11:15:59,298:INFO:Total runtime is 0.17696491877237955 minutes
2024-08-28 11:15:59,301:INFO:SubProcess create_model() called ==================================
2024-08-28 11:15:59,301:INFO:Initializing create_model()
2024-08-28 11:15:59,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:15:59,301:INFO:Checking exceptions
2024-08-28 11:15:59,301:INFO:Importing libraries
2024-08-28 11:15:59,301:INFO:Copying training dataset
2024-08-28 11:15:59,312:INFO:Defining folds
2024-08-28 11:15:59,313:INFO:Declaring metric variables
2024-08-28 11:15:59,317:INFO:Importing untrained model
2024-08-28 11:15:59,320:INFO:Naive Bayes Imported successfully
2024-08-28 11:15:59,325:INFO:Starting cross validation
2024-08-28 11:15:59,326:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:15:59,453:INFO:Calculating mean and std
2024-08-28 11:15:59,454:INFO:Creating metrics dataframe
2024-08-28 11:15:59,455:INFO:Uploading results into container
2024-08-28 11:15:59,456:INFO:Uploading model into container now
2024-08-28 11:15:59,456:INFO:_master_model_container: 3
2024-08-28 11:15:59,456:INFO:_display_container: 2
2024-08-28 11:15:59,457:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-28 11:15:59,457:INFO:create_model() successfully completed......................................
2024-08-28 11:15:59,574:INFO:SubProcess create_model() end ==================================
2024-08-28 11:15:59,574:INFO:Creating metrics dataframe
2024-08-28 11:15:59,581:INFO:Initializing Decision Tree Classifier
2024-08-28 11:15:59,581:INFO:Total runtime is 0.1816823601722717 minutes
2024-08-28 11:15:59,584:INFO:SubProcess create_model() called ==================================
2024-08-28 11:15:59,584:INFO:Initializing create_model()
2024-08-28 11:15:59,584:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:15:59,584:INFO:Checking exceptions
2024-08-28 11:15:59,584:INFO:Importing libraries
2024-08-28 11:15:59,584:INFO:Copying training dataset
2024-08-28 11:15:59,596:INFO:Defining folds
2024-08-28 11:15:59,597:INFO:Declaring metric variables
2024-08-28 11:15:59,599:INFO:Importing untrained model
2024-08-28 11:15:59,602:INFO:Decision Tree Classifier Imported successfully
2024-08-28 11:15:59,609:INFO:Starting cross validation
2024-08-28 11:15:59,610:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:15:59,821:INFO:Calculating mean and std
2024-08-28 11:15:59,822:INFO:Creating metrics dataframe
2024-08-28 11:15:59,823:INFO:Uploading results into container
2024-08-28 11:15:59,824:INFO:Uploading model into container now
2024-08-28 11:15:59,824:INFO:_master_model_container: 4
2024-08-28 11:15:59,825:INFO:_display_container: 2
2024-08-28 11:15:59,825:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-28 11:15:59,825:INFO:create_model() successfully completed......................................
2024-08-28 11:15:59,941:INFO:SubProcess create_model() end ==================================
2024-08-28 11:15:59,941:INFO:Creating metrics dataframe
2024-08-28 11:15:59,948:INFO:Initializing SVM - Linear Kernel
2024-08-28 11:15:59,948:INFO:Total runtime is 0.18778899908065794 minutes
2024-08-28 11:15:59,951:INFO:SubProcess create_model() called ==================================
2024-08-28 11:15:59,951:INFO:Initializing create_model()
2024-08-28 11:15:59,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:15:59,951:INFO:Checking exceptions
2024-08-28 11:15:59,951:INFO:Importing libraries
2024-08-28 11:15:59,952:INFO:Copying training dataset
2024-08-28 11:15:59,964:INFO:Defining folds
2024-08-28 11:15:59,964:INFO:Declaring metric variables
2024-08-28 11:15:59,967:INFO:Importing untrained model
2024-08-28 11:15:59,969:INFO:SVM - Linear Kernel Imported successfully
2024-08-28 11:15:59,975:INFO:Starting cross validation
2024-08-28 11:15:59,976:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:00,657:INFO:Calculating mean and std
2024-08-28 11:16:00,658:INFO:Creating metrics dataframe
2024-08-28 11:16:00,660:INFO:Uploading results into container
2024-08-28 11:16:00,660:INFO:Uploading model into container now
2024-08-28 11:16:00,661:INFO:_master_model_container: 5
2024-08-28 11:16:00,661:INFO:_display_container: 2
2024-08-28 11:16:00,661:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-28 11:16:00,661:INFO:create_model() successfully completed......................................
2024-08-28 11:16:00,788:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:00,788:INFO:Creating metrics dataframe
2024-08-28 11:16:00,794:INFO:Initializing Ridge Classifier
2024-08-28 11:16:00,794:INFO:Total runtime is 0.20189284483591713 minutes
2024-08-28 11:16:00,798:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:00,798:INFO:Initializing create_model()
2024-08-28 11:16:00,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:00,798:INFO:Checking exceptions
2024-08-28 11:16:00,798:INFO:Importing libraries
2024-08-28 11:16:00,799:INFO:Copying training dataset
2024-08-28 11:16:00,813:INFO:Defining folds
2024-08-28 11:16:00,813:INFO:Declaring metric variables
2024-08-28 11:16:00,816:INFO:Importing untrained model
2024-08-28 11:16:00,820:INFO:Ridge Classifier Imported successfully
2024-08-28 11:16:00,826:INFO:Starting cross validation
2024-08-28 11:16:00,827:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:00,993:INFO:Calculating mean and std
2024-08-28 11:16:00,994:INFO:Creating metrics dataframe
2024-08-28 11:16:00,997:INFO:Uploading results into container
2024-08-28 11:16:00,997:INFO:Uploading model into container now
2024-08-28 11:16:00,998:INFO:_master_model_container: 6
2024-08-28 11:16:00,998:INFO:_display_container: 2
2024-08-28 11:16:00,998:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-28 11:16:00,998:INFO:create_model() successfully completed......................................
2024-08-28 11:16:01,148:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:01,148:INFO:Creating metrics dataframe
2024-08-28 11:16:01,155:INFO:Initializing Random Forest Classifier
2024-08-28 11:16:01,155:INFO:Total runtime is 0.20790400107701618 minutes
2024-08-28 11:16:01,158:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:01,159:INFO:Initializing create_model()
2024-08-28 11:16:01,159:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:01,159:INFO:Checking exceptions
2024-08-28 11:16:01,159:INFO:Importing libraries
2024-08-28 11:16:01,159:INFO:Copying training dataset
2024-08-28 11:16:01,171:INFO:Defining folds
2024-08-28 11:16:01,171:INFO:Declaring metric variables
2024-08-28 11:16:01,175:INFO:Importing untrained model
2024-08-28 11:16:01,179:INFO:Random Forest Classifier Imported successfully
2024-08-28 11:16:01,185:INFO:Starting cross validation
2024-08-28 11:16:01,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:03,853:INFO:Calculating mean and std
2024-08-28 11:16:03,853:INFO:Creating metrics dataframe
2024-08-28 11:16:03,856:INFO:Uploading results into container
2024-08-28 11:16:03,856:INFO:Uploading model into container now
2024-08-28 11:16:03,857:INFO:_master_model_container: 7
2024-08-28 11:16:03,857:INFO:_display_container: 2
2024-08-28 11:16:03,857:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-28 11:16:03,858:INFO:create_model() successfully completed......................................
2024-08-28 11:16:03,983:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:03,983:INFO:Creating metrics dataframe
2024-08-28 11:16:03,991:INFO:Initializing Quadratic Discriminant Analysis
2024-08-28 11:16:03,991:INFO:Total runtime is 0.25518014033635456 minutes
2024-08-28 11:16:03,993:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:03,993:INFO:Initializing create_model()
2024-08-28 11:16:03,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:03,993:INFO:Checking exceptions
2024-08-28 11:16:03,995:INFO:Importing libraries
2024-08-28 11:16:03,995:INFO:Copying training dataset
2024-08-28 11:16:04,005:INFO:Defining folds
2024-08-28 11:16:04,006:INFO:Declaring metric variables
2024-08-28 11:16:04,009:INFO:Importing untrained model
2024-08-28 11:16:04,012:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-28 11:16:04,018:INFO:Starting cross validation
2024-08-28 11:16:04,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:04,086:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,086:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,093:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,101:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,102:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,107:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,112:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,118:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,125:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,127:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-28 11:16:04,157:INFO:Calculating mean and std
2024-08-28 11:16:04,159:INFO:Creating metrics dataframe
2024-08-28 11:16:04,161:INFO:Uploading results into container
2024-08-28 11:16:04,162:INFO:Uploading model into container now
2024-08-28 11:16:04,162:INFO:_master_model_container: 8
2024-08-28 11:16:04,162:INFO:_display_container: 2
2024-08-28 11:16:04,163:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-28 11:16:04,163:INFO:create_model() successfully completed......................................
2024-08-28 11:16:04,283:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:04,283:INFO:Creating metrics dataframe
2024-08-28 11:16:04,290:INFO:Initializing Ada Boost Classifier
2024-08-28 11:16:04,290:INFO:Total runtime is 0.26016256809234617 minutes
2024-08-28 11:16:04,293:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:04,293:INFO:Initializing create_model()
2024-08-28 11:16:04,293:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:04,293:INFO:Checking exceptions
2024-08-28 11:16:04,293:INFO:Importing libraries
2024-08-28 11:16:04,293:INFO:Copying training dataset
2024-08-28 11:16:04,305:INFO:Defining folds
2024-08-28 11:16:04,305:INFO:Declaring metric variables
2024-08-28 11:16:04,308:INFO:Importing untrained model
2024-08-28 11:16:04,312:INFO:Ada Boost Classifier Imported successfully
2024-08-28 11:16:04,317:INFO:Starting cross validation
2024-08-28 11:16:04,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:04,364:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,368:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,374:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,377:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,385:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,386:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,389:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,397:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,399:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:04,405:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-28 11:16:05,322:INFO:Calculating mean and std
2024-08-28 11:16:05,323:INFO:Creating metrics dataframe
2024-08-28 11:16:05,325:INFO:Uploading results into container
2024-08-28 11:16:05,325:INFO:Uploading model into container now
2024-08-28 11:16:05,325:INFO:_master_model_container: 9
2024-08-28 11:16:05,326:INFO:_display_container: 2
2024-08-28 11:16:05,326:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-28 11:16:05,326:INFO:create_model() successfully completed......................................
2024-08-28 11:16:05,443:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:05,443:INFO:Creating metrics dataframe
2024-08-28 11:16:05,451:INFO:Initializing Gradient Boosting Classifier
2024-08-28 11:16:05,451:INFO:Total runtime is 0.27951395114262895 minutes
2024-08-28 11:16:05,454:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:05,454:INFO:Initializing create_model()
2024-08-28 11:16:05,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:05,454:INFO:Checking exceptions
2024-08-28 11:16:05,454:INFO:Importing libraries
2024-08-28 11:16:05,454:INFO:Copying training dataset
2024-08-28 11:16:05,466:INFO:Defining folds
2024-08-28 11:16:05,466:INFO:Declaring metric variables
2024-08-28 11:16:05,469:INFO:Importing untrained model
2024-08-28 11:16:05,473:INFO:Gradient Boosting Classifier Imported successfully
2024-08-28 11:16:05,479:INFO:Starting cross validation
2024-08-28 11:16:05,479:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:08,175:INFO:Calculating mean and std
2024-08-28 11:16:08,176:INFO:Creating metrics dataframe
2024-08-28 11:16:08,177:INFO:Uploading results into container
2024-08-28 11:16:08,178:INFO:Uploading model into container now
2024-08-28 11:16:08,178:INFO:_master_model_container: 10
2024-08-28 11:16:08,178:INFO:_display_container: 2
2024-08-28 11:16:08,178:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-28 11:16:08,178:INFO:create_model() successfully completed......................................
2024-08-28 11:16:08,295:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:08,296:INFO:Creating metrics dataframe
2024-08-28 11:16:08,303:INFO:Initializing Linear Discriminant Analysis
2024-08-28 11:16:08,303:INFO:Total runtime is 0.3270480116208394 minutes
2024-08-28 11:16:08,306:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:08,306:INFO:Initializing create_model()
2024-08-28 11:16:08,306:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:08,306:INFO:Checking exceptions
2024-08-28 11:16:08,307:INFO:Importing libraries
2024-08-28 11:16:08,307:INFO:Copying training dataset
2024-08-28 11:16:08,318:INFO:Defining folds
2024-08-28 11:16:08,318:INFO:Declaring metric variables
2024-08-28 11:16:08,322:INFO:Importing untrained model
2024-08-28 11:16:08,325:INFO:Linear Discriminant Analysis Imported successfully
2024-08-28 11:16:08,332:INFO:Starting cross validation
2024-08-28 11:16:08,333:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:08,518:INFO:Calculating mean and std
2024-08-28 11:16:08,519:INFO:Creating metrics dataframe
2024-08-28 11:16:08,520:INFO:Uploading results into container
2024-08-28 11:16:08,521:INFO:Uploading model into container now
2024-08-28 11:16:08,521:INFO:_master_model_container: 11
2024-08-28 11:16:08,521:INFO:_display_container: 2
2024-08-28 11:16:08,521:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-28 11:16:08,521:INFO:create_model() successfully completed......................................
2024-08-28 11:16:08,640:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:08,640:INFO:Creating metrics dataframe
2024-08-28 11:16:08,649:INFO:Initializing Extra Trees Classifier
2024-08-28 11:16:08,649:INFO:Total runtime is 0.3328025897343953 minutes
2024-08-28 11:16:08,652:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:08,652:INFO:Initializing create_model()
2024-08-28 11:16:08,652:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:08,652:INFO:Checking exceptions
2024-08-28 11:16:08,652:INFO:Importing libraries
2024-08-28 11:16:08,652:INFO:Copying training dataset
2024-08-28 11:16:08,663:INFO:Defining folds
2024-08-28 11:16:08,663:INFO:Declaring metric variables
2024-08-28 11:16:08,667:INFO:Importing untrained model
2024-08-28 11:16:08,669:INFO:Extra Trees Classifier Imported successfully
2024-08-28 11:16:08,674:INFO:Starting cross validation
2024-08-28 11:16:08,675:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:10,891:INFO:Calculating mean and std
2024-08-28 11:16:10,893:INFO:Creating metrics dataframe
2024-08-28 11:16:10,894:INFO:Uploading results into container
2024-08-28 11:16:10,895:INFO:Uploading model into container now
2024-08-28 11:16:10,896:INFO:_master_model_container: 12
2024-08-28 11:16:10,896:INFO:_display_container: 2
2024-08-28 11:16:10,896:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-28 11:16:10,896:INFO:create_model() successfully completed......................................
2024-08-28 11:16:11,039:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:11,039:INFO:Creating metrics dataframe
2024-08-28 11:16:11,048:INFO:Initializing Extreme Gradient Boosting
2024-08-28 11:16:11,048:INFO:Total runtime is 0.3728002071380615 minutes
2024-08-28 11:16:11,052:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:11,052:INFO:Initializing create_model()
2024-08-28 11:16:11,052:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:11,052:INFO:Checking exceptions
2024-08-28 11:16:11,054:INFO:Importing libraries
2024-08-28 11:16:11,054:INFO:Copying training dataset
2024-08-28 11:16:11,066:INFO:Defining folds
2024-08-28 11:16:11,066:INFO:Declaring metric variables
2024-08-28 11:16:11,070:INFO:Importing untrained model
2024-08-28 11:16:11,077:INFO:Extreme Gradient Boosting Imported successfully
2024-08-28 11:16:11,089:INFO:Starting cross validation
2024-08-28 11:16:11,091:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:11,669:INFO:Calculating mean and std
2024-08-28 11:16:11,670:INFO:Creating metrics dataframe
2024-08-28 11:16:11,672:INFO:Uploading results into container
2024-08-28 11:16:11,672:INFO:Uploading model into container now
2024-08-28 11:16:11,673:INFO:_master_model_container: 13
2024-08-28 11:16:11,673:INFO:_display_container: 2
2024-08-28 11:16:11,674:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-28 11:16:11,674:INFO:create_model() successfully completed......................................
2024-08-28 11:16:11,788:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:11,789:INFO:Creating metrics dataframe
2024-08-28 11:16:11,797:INFO:Initializing Light Gradient Boosting Machine
2024-08-28 11:16:11,797:INFO:Total runtime is 0.38527121146519977 minutes
2024-08-28 11:16:11,800:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:11,800:INFO:Initializing create_model()
2024-08-28 11:16:11,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:11,800:INFO:Checking exceptions
2024-08-28 11:16:11,800:INFO:Importing libraries
2024-08-28 11:16:11,800:INFO:Copying training dataset
2024-08-28 11:16:11,811:INFO:Defining folds
2024-08-28 11:16:11,811:INFO:Declaring metric variables
2024-08-28 11:16:11,815:INFO:Importing untrained model
2024-08-28 11:16:11,817:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:11,824:INFO:Starting cross validation
2024-08-28 11:16:11,825:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:13,035:INFO:Calculating mean and std
2024-08-28 11:16:13,037:INFO:Creating metrics dataframe
2024-08-28 11:16:13,040:INFO:Uploading results into container
2024-08-28 11:16:13,041:INFO:Uploading model into container now
2024-08-28 11:16:13,041:INFO:_master_model_container: 14
2024-08-28 11:16:13,042:INFO:_display_container: 2
2024-08-28 11:16:13,043:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:13,043:INFO:create_model() successfully completed......................................
2024-08-28 11:16:13,190:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:13,190:INFO:Creating metrics dataframe
2024-08-28 11:16:13,198:INFO:Initializing Dummy Classifier
2024-08-28 11:16:13,198:INFO:Total runtime is 0.4086225907007853 minutes
2024-08-28 11:16:13,200:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:13,201:INFO:Initializing create_model()
2024-08-28 11:16:13,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163DA8B7FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:13,201:INFO:Checking exceptions
2024-08-28 11:16:13,201:INFO:Importing libraries
2024-08-28 11:16:13,201:INFO:Copying training dataset
2024-08-28 11:16:13,212:INFO:Defining folds
2024-08-28 11:16:13,212:INFO:Declaring metric variables
2024-08-28 11:16:13,215:INFO:Importing untrained model
2024-08-28 11:16:13,218:INFO:Dummy Classifier Imported successfully
2024-08-28 11:16:13,223:INFO:Starting cross validation
2024-08-28 11:16:13,224:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:13,300:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,307:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,311:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,317:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,318:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,319:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,323:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,331:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,331:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,334:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-28 11:16:13,340:INFO:Calculating mean and std
2024-08-28 11:16:13,340:INFO:Creating metrics dataframe
2024-08-28 11:16:13,342:INFO:Uploading results into container
2024-08-28 11:16:13,342:INFO:Uploading model into container now
2024-08-28 11:16:13,343:INFO:_master_model_container: 15
2024-08-28 11:16:13,343:INFO:_display_container: 2
2024-08-28 11:16:13,343:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-28 11:16:13,343:INFO:create_model() successfully completed......................................
2024-08-28 11:16:13,464:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:13,465:INFO:Creating metrics dataframe
2024-08-28 11:16:13,482:INFO:Initializing create_model()
2024-08-28 11:16:13,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:13,482:INFO:Checking exceptions
2024-08-28 11:16:13,484:INFO:Importing libraries
2024-08-28 11:16:13,484:INFO:Copying training dataset
2024-08-28 11:16:13,495:INFO:Defining folds
2024-08-28 11:16:13,495:INFO:Declaring metric variables
2024-08-28 11:16:13,495:INFO:Importing untrained model
2024-08-28 11:16:13,495:INFO:Declaring custom model
2024-08-28 11:16:13,496:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:13,496:INFO:Cross validation set to False
2024-08-28 11:16:13,496:INFO:Fitting Model
2024-08-28 11:16:13,527:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-28 11:16:13,527:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-28 11:16:13,529:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.
2024-08-28 11:16:13,529:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-28 11:16:13,529:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-28 11:16:13,529:INFO:[LightGBM] [Info] Total Bins 632
2024-08-28 11:16:13,530:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-28 11:16:13,530:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-28 11:16:13,530:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-28 11:16:13,653:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:13,653:INFO:create_model() successfully completed......................................
2024-08-28 11:16:13,823:INFO:_master_model_container: 15
2024-08-28 11:16:13,823:INFO:_display_container: 2
2024-08-28 11:16:13,823:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:13,823:INFO:compare_models() successfully completed......................................
2024-08-28 11:16:13,824:INFO:Initializing create_model()
2024-08-28 11:16:13,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:13,824:INFO:Checking exceptions
2024-08-28 11:16:13,834:INFO:Importing libraries
2024-08-28 11:16:13,834:INFO:Copying training dataset
2024-08-28 11:16:13,845:INFO:Defining folds
2024-08-28 11:16:13,845:INFO:Declaring metric variables
2024-08-28 11:16:13,848:INFO:Importing untrained model
2024-08-28 11:16:13,848:INFO:Declaring custom model
2024-08-28 11:16:13,854:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:13,861:INFO:Starting cross validation
2024-08-28 11:16:13,861:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:15,250:INFO:Calculating mean and std
2024-08-28 11:16:15,252:INFO:Creating metrics dataframe
2024-08-28 11:16:15,262:INFO:Finalizing model
2024-08-28 11:16:15,311:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-28 11:16:15,312:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-28 11:16:15,316:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.
2024-08-28 11:16:15,316:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-28 11:16:15,316:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-28 11:16:15,316:INFO:[LightGBM] [Info] Total Bins 632
2024-08-28 11:16:15,316:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-28 11:16:15,317:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-28 11:16:15,317:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-28 11:16:15,461:INFO:Uploading results into container
2024-08-28 11:16:15,462:INFO:Uploading model into container now
2024-08-28 11:16:15,475:INFO:_master_model_container: 16
2024-08-28 11:16:15,475:INFO:_display_container: 3
2024-08-28 11:16:15,476:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:15,476:INFO:create_model() successfully completed......................................
2024-08-28 11:16:15,633:INFO:Initializing tune_model()
2024-08-28 11:16:15,633:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-28 11:16:15,633:INFO:Checking exceptions
2024-08-28 11:16:15,649:INFO:Copying training dataset
2024-08-28 11:16:15,659:INFO:Checking base model
2024-08-28 11:16:15,659:INFO:Base model : Light Gradient Boosting Machine
2024-08-28 11:16:15,664:INFO:Declaring metric variables
2024-08-28 11:16:15,668:INFO:Defining Hyperparameters
2024-08-28 11:16:15,820:INFO:Tuning with n_jobs=-1
2024-08-28 11:16:15,820:INFO:Initializing RandomizedSearchCV
2024-08-28 11:16:38,040:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-28 11:16:38,042:INFO:Hyperparameter search completed
2024-08-28 11:16:38,042:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:38,043:INFO:Initializing create_model()
2024-08-28 11:16:38,043:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000163D909B010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-28 11:16:38,043:INFO:Checking exceptions
2024-08-28 11:16:38,043:INFO:Importing libraries
2024-08-28 11:16:38,044:INFO:Copying training dataset
2024-08-28 11:16:38,067:INFO:Defining folds
2024-08-28 11:16:38,067:INFO:Declaring metric variables
2024-08-28 11:16:38,072:INFO:Importing untrained model
2024-08-28 11:16:38,072:INFO:Declaring custom model
2024-08-28 11:16:38,077:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:38,087:INFO:Starting cross validation
2024-08-28 11:16:38,089:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:40,601:INFO:Calculating mean and std
2024-08-28 11:16:40,603:INFO:Creating metrics dataframe
2024-08-28 11:16:40,609:INFO:Finalizing model
2024-08-28 11:16:40,644:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-28 11:16:40,644:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-28 11:16:40,644:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-28 11:16:40,654:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-28 11:16:40,654:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-28 11:16:40,655:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-28 11:16:40,655:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-28 11:16:40,655:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-28 11:16:40,657:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.
2024-08-28 11:16:40,657:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-28 11:16:40,657:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-28 11:16:40,658:INFO:[LightGBM] [Info] Total Bins 632
2024-08-28 11:16:40,658:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-28 11:16:40,658:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-28 11:16:40,658:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-28 11:16:40,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,836:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,854:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-28 11:16:40,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-28 11:16:40,948:INFO:Uploading results into container
2024-08-28 11:16:40,950:INFO:Uploading model into container now
2024-08-28 11:16:40,951:INFO:_master_model_container: 17
2024-08-28 11:16:40,951:INFO:_display_container: 4
2024-08-28 11:16:40,951:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:40,951:INFO:create_model() successfully completed......................................
2024-08-28 11:16:41,117:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:41,117:INFO:choose_better activated
2024-08-28 11:16:41,121:INFO:SubProcess create_model() called ==================================
2024-08-28 11:16:41,122:INFO:Initializing create_model()
2024-08-28 11:16:41,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:41,122:INFO:Checking exceptions
2024-08-28 11:16:41,123:INFO:Importing libraries
2024-08-28 11:16:41,123:INFO:Copying training dataset
2024-08-28 11:16:41,137:INFO:Defining folds
2024-08-28 11:16:41,137:INFO:Declaring metric variables
2024-08-28 11:16:41,137:INFO:Importing untrained model
2024-08-28 11:16:41,137:INFO:Declaring custom model
2024-08-28 11:16:41,138:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:41,138:INFO:Starting cross validation
2024-08-28 11:16:41,138:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-28 11:16:42,751:INFO:Calculating mean and std
2024-08-28 11:16:42,751:INFO:Creating metrics dataframe
2024-08-28 11:16:42,754:INFO:Finalizing model
2024-08-28 11:16:42,793:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-28 11:16:42,793:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-28 11:16:42,795:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000955 seconds.
2024-08-28 11:16:42,796:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-28 11:16:42,796:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-28 11:16:42,796:INFO:[LightGBM] [Info] Total Bins 632
2024-08-28 11:16:42,796:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-28 11:16:42,796:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-28 11:16:42,796:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-28 11:16:42,916:INFO:Uploading results into container
2024-08-28 11:16:42,917:INFO:Uploading model into container now
2024-08-28 11:16:42,917:INFO:_master_model_container: 18
2024-08-28 11:16:42,917:INFO:_display_container: 5
2024-08-28 11:16:42,918:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:42,918:INFO:create_model() successfully completed......................................
2024-08-28 11:16:43,076:INFO:SubProcess create_model() end ==================================
2024-08-28 11:16:43,077:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8645
2024-08-28 11:16:43,077:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8641
2024-08-28 11:16:43,078:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-28 11:16:43,078:INFO:choose_better completed
2024-08-28 11:16:43,078:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-28 11:16:43,087:INFO:_master_model_container: 18
2024-08-28 11:16:43,088:INFO:_display_container: 4
2024-08-28 11:16:43,088:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:43,088:INFO:tune_model() successfully completed......................................
2024-08-28 11:16:43,225:INFO:Initializing finalize_model()
2024-08-28 11:16:43,225:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-28 11:16:43,225:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-28 11:16:43,231:INFO:Initializing create_model()
2024-08-28 11:16:43,231:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-28 11:16:43,231:INFO:Checking exceptions
2024-08-28 11:16:43,232:INFO:Importing libraries
2024-08-28 11:16:43,232:INFO:Copying training dataset
2024-08-28 11:16:43,233:INFO:Defining folds
2024-08-28 11:16:43,233:INFO:Declaring metric variables
2024-08-28 11:16:43,233:INFO:Importing untrained model
2024-08-28 11:16:43,233:INFO:Declaring custom model
2024-08-28 11:16:43,233:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-28 11:16:43,234:INFO:Cross validation set to False
2024-08-28 11:16:43,234:INFO:Fitting Model
2024-08-28 11:16:43,265:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-28 11:16:43,265:INFO:[LightGBM] [Info] Number of positive: 6279, number of negative: 19790
2024-08-28 11:16:43,267:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000659 seconds.
2024-08-28 11:16:43,267:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-28 11:16:43,267:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-28 11:16:43,268:INFO:[LightGBM] [Info] Total Bins 648
2024-08-28 11:16:43,268:INFO:[LightGBM] [Info] Number of data points in the train set: 26069, number of used features: 13
2024-08-28 11:16:43,268:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240861 -> initscore=-1.147966
2024-08-28 11:16:43,268:INFO:[LightGBM] [Info] Start training from score -1.147966
2024-08-28 11:16:43,399:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-28 11:16:43,399:INFO:create_model() successfully completed......................................
2024-08-28 11:16:43,550:INFO:_master_model_container: 18
2024-08-28 11:16:43,550:INFO:_display_container: 4
2024-08-28 11:16:43,554:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-28 11:16:43,554:INFO:finalize_model() successfully completed......................................
2024-08-28 11:16:43,684:INFO:Initializing predict_model()
2024-08-28 11:16:43,684:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000163DA762520>)
2024-08-28 11:16:43,684:INFO:Checking exceptions
2024-08-28 11:16:43,684:INFO:Preloading libraries
2024-08-28 11:16:43,686:INFO:Set up data.
2024-08-28 11:16:43,690:INFO:Set up index.
2024-08-28 11:16:43,887:INFO:Initializing evaluate_model()
2024-08-28 11:16:43,887:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-28 11:16:43,903:INFO:Initializing plot_model()
2024-08-28 11:16:43,904:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:16:43,904:INFO:Checking exceptions
2024-08-28 11:16:43,908:INFO:Preloading libraries
2024-08-28 11:16:43,912:INFO:Copying training dataset
2024-08-28 11:16:43,912:INFO:Plot type: pipeline
2024-08-28 11:16:44,065:INFO:Visual Rendered Successfully
2024-08-28 11:16:44,186:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:01,855:INFO:Initializing plot_model()
2024-08-28 11:17:01,855:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:01,855:INFO:Checking exceptions
2024-08-28 11:17:01,859:INFO:Preloading libraries
2024-08-28 11:17:01,864:INFO:Copying training dataset
2024-08-28 11:17:01,864:INFO:Plot type: feature
2024-08-28 11:17:01,865:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:02,042:INFO:Visual Rendered Successfully
2024-08-28 11:17:02,157:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:13,365:INFO:Initializing plot_model()
2024-08-28 11:17:13,365:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:13,365:INFO:Checking exceptions
2024-08-28 11:17:13,370:INFO:Preloading libraries
2024-08-28 11:17:13,375:INFO:Copying training dataset
2024-08-28 11:17:13,375:INFO:Plot type: feature_all
2024-08-28 11:17:13,422:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:13,580:INFO:Visual Rendered Successfully
2024-08-28 11:17:13,695:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:16,917:INFO:Initializing plot_model()
2024-08-28 11:17:16,918:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:16,918:INFO:Checking exceptions
2024-08-28 11:17:16,923:INFO:Preloading libraries
2024-08-28 11:17:16,928:INFO:Copying training dataset
2024-08-28 11:17:16,928:INFO:Plot type: feature
2024-08-28 11:17:16,928:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:17,092:INFO:Visual Rendered Successfully
2024-08-28 11:17:17,211:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:18,076:INFO:Initializing plot_model()
2024-08-28 11:17:18,076:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:18,076:INFO:Checking exceptions
2024-08-28 11:17:18,081:INFO:Preloading libraries
2024-08-28 11:17:18,085:INFO:Copying training dataset
2024-08-28 11:17:18,085:INFO:Plot type: feature_all
2024-08-28 11:17:18,131:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:18,293:INFO:Visual Rendered Successfully
2024-08-28 11:17:18,405:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:20,114:INFO:Initializing plot_model()
2024-08-28 11:17:20,114:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:20,114:INFO:Checking exceptions
2024-08-28 11:17:20,118:INFO:Preloading libraries
2024-08-28 11:17:20,123:INFO:Copying training dataset
2024-08-28 11:17:20,123:INFO:Plot type: feature
2024-08-28 11:17:20,124:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:20,287:INFO:Visual Rendered Successfully
2024-08-28 11:17:20,400:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:21,351:INFO:Initializing plot_model()
2024-08-28 11:17:21,351:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:21,351:INFO:Checking exceptions
2024-08-28 11:17:21,357:INFO:Preloading libraries
2024-08-28 11:17:21,362:INFO:Copying training dataset
2024-08-28 11:17:21,362:INFO:Plot type: feature_all
2024-08-28 11:17:21,406:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:21,563:INFO:Visual Rendered Successfully
2024-08-28 11:17:21,675:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:25,111:INFO:Initializing plot_model()
2024-08-28 11:17:25,112:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:25,112:INFO:Checking exceptions
2024-08-28 11:17:25,116:INFO:Preloading libraries
2024-08-28 11:17:25,121:INFO:Copying training dataset
2024-08-28 11:17:25,121:INFO:Plot type: feature
2024-08-28 11:17:25,121:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:25,288:INFO:Visual Rendered Successfully
2024-08-28 11:17:25,402:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:27,624:INFO:Initializing plot_model()
2024-08-28 11:17:27,624:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:27,624:INFO:Checking exceptions
2024-08-28 11:17:27,629:INFO:Preloading libraries
2024-08-28 11:17:27,633:INFO:Copying training dataset
2024-08-28 11:17:27,633:INFO:Plot type: feature_all
2024-08-28 11:17:27,682:WARNING:No coef_ found. Trying feature_importances_
2024-08-28 11:17:27,850:INFO:Visual Rendered Successfully
2024-08-28 11:17:27,968:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:28,133:INFO:Initializing plot_model()
2024-08-28 11:17:28,133:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:28,133:INFO:Checking exceptions
2024-08-28 11:17:28,137:INFO:Preloading libraries
2024-08-28 11:17:28,142:INFO:Copying training dataset
2024-08-28 11:17:28,142:INFO:Plot type: boundary
2024-08-28 11:17:28,230:INFO:Fitting StandardScaler()
2024-08-28 11:17:28,240:INFO:Fitting PCA()
2024-08-28 11:17:28,366:INFO:Fitting Model
2024-08-28 11:17:28,373:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-28 11:17:28,374:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.
2024-08-28 11:17:28,374:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-28 11:17:28,374:INFO:[LightGBM] [Info] Total Bins 510
2024-08-28 11:17:28,374:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 2
2024-08-28 11:17:28,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-28 11:17:28,374:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-28 11:17:29,453:INFO:Visual Rendered Successfully
2024-08-28 11:17:29,612:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:35,277:INFO:Initializing plot_model()
2024-08-28 11:17:35,277:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=lift, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:35,277:INFO:Checking exceptions
2024-08-28 11:17:35,281:INFO:Preloading libraries
2024-08-28 11:17:35,285:INFO:Copying training dataset
2024-08-28 11:17:35,285:INFO:Plot type: lift
2024-08-28 11:17:35,286:INFO:Generating predictions / predict_proba on X_test
2024-08-28 11:17:35,529:INFO:Visual Rendered Successfully
2024-08-28 11:17:35,674:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:38,616:INFO:Initializing plot_model()
2024-08-28 11:17:38,616:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:38,616:INFO:Checking exceptions
2024-08-28 11:17:38,621:INFO:Preloading libraries
2024-08-28 11:17:38,624:INFO:Copying training dataset
2024-08-28 11:17:38,624:INFO:Plot type: calibration
2024-08-28 11:17:38,639:INFO:Scoring test/hold-out set
2024-08-28 11:17:38,855:INFO:Visual Rendered Successfully
2024-08-28 11:17:38,976:INFO:plot_model() successfully completed......................................
2024-08-28 11:17:43,528:INFO:Initializing plot_model()
2024-08-28 11:17:43,529:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000163CFFBE890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-28 11:17:43,529:INFO:Checking exceptions
2024-08-28 11:17:43,537:INFO:Preloading libraries
2024-08-28 11:17:43,558:INFO:Copying training dataset
2024-08-28 11:17:43,558:INFO:Plot type: pr
2024-08-28 11:17:43,698:INFO:Fitting Model
2024-08-28 11:17:43,698:INFO:Scoring test/hold-out set
2024-08-28 11:17:43,900:INFO:Visual Rendered Successfully
2024-08-28 11:17:44,019:INFO:plot_model() successfully completed......................................
2024-08-31 17:24:43,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:24:43,927:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:24:43,927:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:24:43,927:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:24:44,467:INFO:PyCaret ClassificationExperiment
2024-08-31 17:24:44,467:INFO:Logging name: clf-default-name
2024-08-31 17:24:44,467:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 17:24:44,467:INFO:version 3.3.2
2024-08-31 17:24:44,467:INFO:Initializing setup()
2024-08-31 17:24:44,467:INFO:self.USI: ac0a
2024-08-31 17:24:44,467:INFO:self._variable_keys: {'y_test', 'data', 'fold_shuffle_param', 'n_jobs_param', 'gpu_n_jobs_param', '_ml_usecase', 'X_train', 'y', 'gpu_param', 'seed', 'y_train', 'fold_generator', 'target_param', 'X_test', 'is_multiclass', 'idx', 'memory', 'X', 'logging_param', 'log_plots_param', '_available_plots', 'fix_imbalance', 'USI', 'pipeline', 'exp_name_log', 'exp_id', 'html_param', 'fold_groups_param'}
2024-08-31 17:24:44,467:INFO:Checking environment
2024-08-31 17:24:44,467:INFO:python_version: 3.11.9
2024-08-31 17:24:44,467:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 17:24:44,468:INFO:machine: AMD64
2024-08-31 17:24:44,468:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 17:24:44,473:INFO:Memory: svmem(total=16867028992, available=5114540032, percent=69.7, used=11752488960, free=5114540032)
2024-08-31 17:24:44,473:INFO:Physical Core: 6
2024-08-31 17:24:44,473:INFO:Logical Core: 12
2024-08-31 17:24:44,473:INFO:Checking libraries
2024-08-31 17:24:44,473:INFO:System:
2024-08-31 17:24:44,473:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 17:24:44,473:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 17:24:44,473:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 17:24:44,473:INFO:PyCaret required dependencies:
2024-08-31 17:24:44,545:INFO:                 pip: 23.2.1
2024-08-31 17:24:44,545:INFO:          setuptools: 67.8.0
2024-08-31 17:24:44,545:INFO:             pycaret: 3.3.2
2024-08-31 17:24:44,545:INFO:             IPython: 8.14.0
2024-08-31 17:24:44,545:INFO:          ipywidgets: 8.1.5
2024-08-31 17:24:44,545:INFO:                tqdm: 4.66.5
2024-08-31 17:24:44,545:INFO:               numpy: 1.24.3
2024-08-31 17:24:44,545:INFO:              pandas: 2.0.3
2024-08-31 17:24:44,545:INFO:              jinja2: 3.1.4
2024-08-31 17:24:44,545:INFO:               scipy: 1.10.1
2024-08-31 17:24:44,545:INFO:              joblib: 1.2.0
2024-08-31 17:24:44,545:INFO:             sklearn: 1.4.2
2024-08-31 17:24:44,546:INFO:                pyod: 2.0.1
2024-08-31 17:24:44,546:INFO:            imblearn: 0.12.3
2024-08-31 17:24:44,546:INFO:   category_encoders: 2.6.3
2024-08-31 17:24:44,546:INFO:            lightgbm: 4.5.0
2024-08-31 17:24:44,546:INFO:               numba: 0.60.0
2024-08-31 17:24:44,546:INFO:            requests: 2.32.3
2024-08-31 17:24:44,546:INFO:          matplotlib: 3.7.1
2024-08-31 17:24:44,546:INFO:          scikitplot: 0.3.7
2024-08-31 17:24:44,546:INFO:         yellowbrick: 1.5
2024-08-31 17:24:44,546:INFO:              plotly: 5.16.1
2024-08-31 17:24:44,546:INFO:    plotly-resampler: Not installed
2024-08-31 17:24:44,546:INFO:             kaleido: 0.2.1
2024-08-31 17:24:44,546:INFO:           schemdraw: 0.15
2024-08-31 17:24:44,546:INFO:         statsmodels: 0.14.2
2024-08-31 17:24:44,546:INFO:              sktime: 0.26.0
2024-08-31 17:24:44,546:INFO:               tbats: 1.1.3
2024-08-31 17:24:44,546:INFO:            pmdarima: 2.0.4
2024-08-31 17:24:44,546:INFO:              psutil: 5.9.0
2024-08-31 17:24:44,546:INFO:          markupsafe: 2.1.3
2024-08-31 17:24:44,546:INFO:             pickle5: Not installed
2024-08-31 17:24:44,546:INFO:         cloudpickle: 3.0.0
2024-08-31 17:24:44,546:INFO:         deprecation: 2.1.0
2024-08-31 17:24:44,546:INFO:              xxhash: 3.5.0
2024-08-31 17:24:44,546:INFO:           wurlitzer: Not installed
2024-08-31 17:24:44,546:INFO:PyCaret optional dependencies:
2024-08-31 17:24:48,443:INFO:                shap: Not installed
2024-08-31 17:24:48,443:INFO:           interpret: Not installed
2024-08-31 17:24:48,443:INFO:                umap: Not installed
2024-08-31 17:24:48,443:INFO:     ydata_profiling: Not installed
2024-08-31 17:24:48,443:INFO:  explainerdashboard: Not installed
2024-08-31 17:24:48,443:INFO:             autoviz: Not installed
2024-08-31 17:24:48,443:INFO:           fairlearn: Not installed
2024-08-31 17:24:48,443:INFO:          deepchecks: Not installed
2024-08-31 17:24:48,443:INFO:             xgboost: 2.0.2
2024-08-31 17:24:48,443:INFO:            catboost: Not installed
2024-08-31 17:24:48,443:INFO:              kmodes: Not installed
2024-08-31 17:24:48,443:INFO:             mlxtend: Not installed
2024-08-31 17:24:48,443:INFO:       statsforecast: Not installed
2024-08-31 17:24:48,443:INFO:        tune_sklearn: Not installed
2024-08-31 17:24:48,443:INFO:                 ray: Not installed
2024-08-31 17:24:48,443:INFO:            hyperopt: Not installed
2024-08-31 17:24:48,443:INFO:              optuna: Not installed
2024-08-31 17:24:48,443:INFO:               skopt: Not installed
2024-08-31 17:24:48,443:INFO:              mlflow: Not installed
2024-08-31 17:24:48,443:INFO:              gradio: 4.41.0
2024-08-31 17:24:48,443:INFO:             fastapi: 0.112.1
2024-08-31 17:24:48,444:INFO:             uvicorn: 0.30.6
2024-08-31 17:24:48,444:INFO:              m2cgen: Not installed
2024-08-31 17:24:48,444:INFO:           evidently: Not installed
2024-08-31 17:24:48,444:INFO:               fugue: Not installed
2024-08-31 17:24:48,444:INFO:           streamlit: Not installed
2024-08-31 17:24:48,444:INFO:             prophet: Not installed
2024-08-31 17:24:48,444:INFO:None
2024-08-31 17:24:48,444:INFO:Set up data.
2024-08-31 17:24:48,453:INFO:Set up folding strategy.
2024-08-31 17:24:48,453:INFO:Set up train/test split.
2024-08-31 17:24:48,467:INFO:Set up index.
2024-08-31 17:24:48,469:INFO:Assigning column types.
2024-08-31 17:24:48,475:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 17:24:48,512:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,515:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,546:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,584:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,585:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,607:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,609:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 17:24:48,645:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,667:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,707:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:24:48,729:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,731:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,731:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 17:24:48,789:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,791:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,851:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:48,854:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:48,855:INFO:Preparing preprocessing pipeline...
2024-08-31 17:24:48,858:INFO:Set up simple imputation.
2024-08-31 17:24:48,859:INFO:Set up column name cleaning.
2024-08-31 17:24:48,910:INFO:Finished creating preprocessing pipeline.
2024-08-31 17:24:48,916:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(ad...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 17:24:48,916:INFO:Creating final display dataframe.
2024-08-31 17:24:49,033:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26069, 15)
4        Transformed data shape       (26069, 15)
5   Transformed train set shape       (18248, 15)
6    Transformed test set shape        (7821, 15)
7              Numeric features                14
8      Rows with missing values              1.8%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              ac0a
2024-08-31 17:24:49,107:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:49,110:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:49,177:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:24:49,180:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:24:49,181:INFO:setup() successfully completed in 4.71s...............
2024-08-31 17:24:49,181:INFO:Initializing compare_models()
2024-08-31 17:24:49,181:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022690015990>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022690015990>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 17:24:49,181:INFO:Checking exceptions
2024-08-31 17:24:49,190:INFO:Preparing display monitor
2024-08-31 17:24:49,222:INFO:Initializing Logistic Regression
2024-08-31 17:24:49,222:INFO:Total runtime is 0.0 minutes
2024-08-31 17:24:49,227:INFO:SubProcess create_model() called ==================================
2024-08-31 17:24:49,228:INFO:Initializing create_model()
2024-08-31 17:24:49,228:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022690015990>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002269944ADD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:24:49,228:INFO:Checking exceptions
2024-08-31 17:24:49,228:INFO:Importing libraries
2024-08-31 17:24:49,228:INFO:Copying training dataset
2024-08-31 17:24:49,246:INFO:Defining folds
2024-08-31 17:24:49,246:INFO:Declaring metric variables
2024-08-31 17:24:49,251:INFO:Importing untrained model
2024-08-31 17:24:49,254:INFO:Logistic Regression Imported successfully
2024-08-31 17:24:49,265:INFO:Starting cross validation
2024-08-31 17:24:49,266:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:24:58,264:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,298:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,312:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,345:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,402:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,412:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,432:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,506:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,548:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,558:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:24:58,585:INFO:Calculating mean and std
2024-08-31 17:24:58,587:INFO:Creating metrics dataframe
2024-08-31 17:24:58,590:INFO:Uploading results into container
2024-08-31 17:24:58,591:INFO:Uploading model into container now
2024-08-31 17:24:58,592:INFO:_master_model_container: 1
2024-08-31 17:24:58,592:INFO:_display_container: 2
2024-08-31 17:24:58,592:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 17:24:58,592:INFO:create_model() successfully completed......................................
2024-08-31 17:24:58,716:INFO:SubProcess create_model() end ==================================
2024-08-31 17:24:58,717:INFO:Creating metrics dataframe
2024-08-31 17:24:58,723:INFO:Initializing K Neighbors Classifier
2024-08-31 17:24:58,723:INFO:Total runtime is 0.15834129254023235 minutes
2024-08-31 17:24:58,726:INFO:SubProcess create_model() called ==================================
2024-08-31 17:24:58,726:INFO:Initializing create_model()
2024-08-31 17:24:58,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022690015990>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002269944ADD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:24:58,726:INFO:Checking exceptions
2024-08-31 17:24:58,726:INFO:Importing libraries
2024-08-31 17:24:58,727:INFO:Copying training dataset
2024-08-31 17:24:58,739:INFO:Defining folds
2024-08-31 17:24:58,739:INFO:Declaring metric variables
2024-08-31 17:24:58,742:INFO:Importing untrained model
2024-08-31 17:24:58,746:INFO:K Neighbors Classifier Imported successfully
2024-08-31 17:24:58,754:INFO:Starting cross validation
2024-08-31 17:24:58,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:10,111:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:25:10,112:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:25:10,112:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:25:10,112:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 17:25:10,422:INFO:PyCaret ClassificationExperiment
2024-08-31 17:25:10,422:INFO:Logging name: clf-default-name
2024-08-31 17:25:10,422:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 17:25:10,422:INFO:version 3.3.2
2024-08-31 17:25:10,422:INFO:Initializing setup()
2024-08-31 17:25:10,422:INFO:self.USI: d85e
2024-08-31 17:25:10,422:INFO:self._variable_keys: {'USI', 'log_plots_param', 'fold_shuffle_param', 'html_param', 'data', 'y', 'X', 'is_multiclass', 'n_jobs_param', 'fold_groups_param', 'X_test', 'memory', 'y_train', '_ml_usecase', 'idx', 'pipeline', 'seed', 'exp_id', 'fix_imbalance', 'y_test', 'gpu_param', 'X_train', 'logging_param', '_available_plots', 'target_param', 'gpu_n_jobs_param', 'fold_generator', 'exp_name_log'}
2024-08-31 17:25:10,422:INFO:Checking environment
2024-08-31 17:25:10,422:INFO:python_version: 3.11.9
2024-08-31 17:25:10,422:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 17:25:10,422:INFO:machine: AMD64
2024-08-31 17:25:10,422:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 17:25:10,427:INFO:Memory: svmem(total=16867028992, available=4939362304, percent=70.7, used=11927666688, free=4939362304)
2024-08-31 17:25:10,427:INFO:Physical Core: 6
2024-08-31 17:25:10,427:INFO:Logical Core: 12
2024-08-31 17:25:10,427:INFO:Checking libraries
2024-08-31 17:25:10,427:INFO:System:
2024-08-31 17:25:10,427:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 17:25:10,427:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 17:25:10,427:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 17:25:10,427:INFO:PyCaret required dependencies:
2024-08-31 17:25:10,479:INFO:                 pip: 23.2.1
2024-08-31 17:25:10,479:INFO:          setuptools: 67.8.0
2024-08-31 17:25:10,479:INFO:             pycaret: 3.3.2
2024-08-31 17:25:10,479:INFO:             IPython: 8.14.0
2024-08-31 17:25:10,479:INFO:          ipywidgets: 8.1.5
2024-08-31 17:25:10,479:INFO:                tqdm: 4.66.5
2024-08-31 17:25:10,479:INFO:               numpy: 1.24.3
2024-08-31 17:25:10,479:INFO:              pandas: 2.0.3
2024-08-31 17:25:10,479:INFO:              jinja2: 3.1.4
2024-08-31 17:25:10,479:INFO:               scipy: 1.10.1
2024-08-31 17:25:10,479:INFO:              joblib: 1.2.0
2024-08-31 17:25:10,479:INFO:             sklearn: 1.4.2
2024-08-31 17:25:10,479:INFO:                pyod: 2.0.1
2024-08-31 17:25:10,479:INFO:            imblearn: 0.12.3
2024-08-31 17:25:10,479:INFO:   category_encoders: 2.6.3
2024-08-31 17:25:10,479:INFO:            lightgbm: 4.5.0
2024-08-31 17:25:10,479:INFO:               numba: 0.60.0
2024-08-31 17:25:10,479:INFO:            requests: 2.32.3
2024-08-31 17:25:10,479:INFO:          matplotlib: 3.7.1
2024-08-31 17:25:10,479:INFO:          scikitplot: 0.3.7
2024-08-31 17:25:10,479:INFO:         yellowbrick: 1.5
2024-08-31 17:25:10,479:INFO:              plotly: 5.16.1
2024-08-31 17:25:10,480:INFO:    plotly-resampler: Not installed
2024-08-31 17:25:10,480:INFO:             kaleido: 0.2.1
2024-08-31 17:25:10,480:INFO:           schemdraw: 0.15
2024-08-31 17:25:10,480:INFO:         statsmodels: 0.14.2
2024-08-31 17:25:10,480:INFO:              sktime: 0.26.0
2024-08-31 17:25:10,480:INFO:               tbats: 1.1.3
2024-08-31 17:25:10,480:INFO:            pmdarima: 2.0.4
2024-08-31 17:25:10,480:INFO:              psutil: 5.9.0
2024-08-31 17:25:10,480:INFO:          markupsafe: 2.1.3
2024-08-31 17:25:10,480:INFO:             pickle5: Not installed
2024-08-31 17:25:10,480:INFO:         cloudpickle: 3.0.0
2024-08-31 17:25:10,480:INFO:         deprecation: 2.1.0
2024-08-31 17:25:10,480:INFO:              xxhash: 3.5.0
2024-08-31 17:25:10,480:INFO:           wurlitzer: Not installed
2024-08-31 17:25:10,480:INFO:PyCaret optional dependencies:
2024-08-31 17:25:13,583:INFO:                shap: Not installed
2024-08-31 17:25:13,583:INFO:           interpret: Not installed
2024-08-31 17:25:13,583:INFO:                umap: Not installed
2024-08-31 17:25:13,583:INFO:     ydata_profiling: Not installed
2024-08-31 17:25:13,583:INFO:  explainerdashboard: Not installed
2024-08-31 17:25:13,583:INFO:             autoviz: Not installed
2024-08-31 17:25:13,583:INFO:           fairlearn: Not installed
2024-08-31 17:25:13,583:INFO:          deepchecks: Not installed
2024-08-31 17:25:13,583:INFO:             xgboost: 2.0.2
2024-08-31 17:25:13,583:INFO:            catboost: Not installed
2024-08-31 17:25:13,583:INFO:              kmodes: Not installed
2024-08-31 17:25:13,583:INFO:             mlxtend: Not installed
2024-08-31 17:25:13,583:INFO:       statsforecast: Not installed
2024-08-31 17:25:13,583:INFO:        tune_sklearn: Not installed
2024-08-31 17:25:13,583:INFO:                 ray: Not installed
2024-08-31 17:25:13,583:INFO:            hyperopt: Not installed
2024-08-31 17:25:13,583:INFO:              optuna: Not installed
2024-08-31 17:25:13,583:INFO:               skopt: Not installed
2024-08-31 17:25:13,583:INFO:              mlflow: Not installed
2024-08-31 17:25:13,583:INFO:              gradio: 4.41.0
2024-08-31 17:25:13,583:INFO:             fastapi: 0.112.1
2024-08-31 17:25:13,583:INFO:             uvicorn: 0.30.6
2024-08-31 17:25:13,583:INFO:              m2cgen: Not installed
2024-08-31 17:25:13,583:INFO:           evidently: Not installed
2024-08-31 17:25:13,583:INFO:               fugue: Not installed
2024-08-31 17:25:13,584:INFO:           streamlit: Not installed
2024-08-31 17:25:13,584:INFO:             prophet: Not installed
2024-08-31 17:25:13,584:INFO:None
2024-08-31 17:25:13,584:INFO:Set up data.
2024-08-31 17:25:13,593:INFO:Set up folding strategy.
2024-08-31 17:25:13,593:INFO:Set up train/test split.
2024-08-31 17:25:13,604:INFO:Set up index.
2024-08-31 17:25:13,605:INFO:Assigning column types.
2024-08-31 17:25:13,612:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 17:25:13,647:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,649:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,681:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,683:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,718:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,719:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,740:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,742:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,743:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 17:25:13,779:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,802:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,839:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 17:25:13,860:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,863:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,863:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 17:25:13,923:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,986:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:13,988:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:13,990:INFO:Preparing preprocessing pipeline...
2024-08-31 17:25:13,992:INFO:Set up simple imputation.
2024-08-31 17:25:13,993:INFO:Set up column name cleaning.
2024-08-31 17:25:14,025:INFO:Finished creating preprocessing pipeline.
2024-08-31 17:25:14,028:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(ad...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 17:25:14,030:INFO:Creating final display dataframe.
2024-08-31 17:25:14,128:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26069, 15)
4        Transformed data shape       (26069, 15)
5   Transformed train set shape       (18248, 15)
6    Transformed test set shape        (7821, 15)
7              Numeric features                14
8      Rows with missing values              1.8%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              d85e
2024-08-31 17:25:14,194:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:14,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:14,255:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 17:25:14,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 17:25:14,259:INFO:setup() successfully completed in 3.84s...............
2024-08-31 17:25:14,259:INFO:Initializing compare_models()
2024-08-31 17:25:14,259:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 17:25:14,259:INFO:Checking exceptions
2024-08-31 17:25:14,267:INFO:Preparing display monitor
2024-08-31 17:25:14,288:INFO:Initializing Logistic Regression
2024-08-31 17:25:14,288:INFO:Total runtime is 0.0 minutes
2024-08-31 17:25:14,291:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:14,291:INFO:Initializing create_model()
2024-08-31 17:25:14,291:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:14,291:INFO:Checking exceptions
2024-08-31 17:25:14,291:INFO:Importing libraries
2024-08-31 17:25:14,291:INFO:Copying training dataset
2024-08-31 17:25:14,308:INFO:Defining folds
2024-08-31 17:25:14,308:INFO:Declaring metric variables
2024-08-31 17:25:14,315:INFO:Importing untrained model
2024-08-31 17:25:14,321:INFO:Logistic Regression Imported successfully
2024-08-31 17:25:14,333:INFO:Starting cross validation
2024-08-31 17:25:14,335:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:22,998:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,002:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,034:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,077:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,294:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,338:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,345:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,377:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,481:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,488:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 17:25:23,516:INFO:Calculating mean and std
2024-08-31 17:25:23,518:INFO:Creating metrics dataframe
2024-08-31 17:25:23,521:INFO:Uploading results into container
2024-08-31 17:25:23,522:INFO:Uploading model into container now
2024-08-31 17:25:23,522:INFO:_master_model_container: 1
2024-08-31 17:25:23,522:INFO:_display_container: 2
2024-08-31 17:25:23,523:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 17:25:23,524:INFO:create_model() successfully completed......................................
2024-08-31 17:25:23,664:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:23,664:INFO:Creating metrics dataframe
2024-08-31 17:25:23,670:INFO:Initializing K Neighbors Classifier
2024-08-31 17:25:23,670:INFO:Total runtime is 0.15636665026346844 minutes
2024-08-31 17:25:23,673:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:23,674:INFO:Initializing create_model()
2024-08-31 17:25:23,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:23,674:INFO:Checking exceptions
2024-08-31 17:25:23,674:INFO:Importing libraries
2024-08-31 17:25:23,674:INFO:Copying training dataset
2024-08-31 17:25:23,687:INFO:Defining folds
2024-08-31 17:25:23,687:INFO:Declaring metric variables
2024-08-31 17:25:23,690:INFO:Importing untrained model
2024-08-31 17:25:23,694:INFO:K Neighbors Classifier Imported successfully
2024-08-31 17:25:23,703:INFO:Starting cross validation
2024-08-31 17:25:23,704:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:26,172:INFO:Calculating mean and std
2024-08-31 17:25:26,173:INFO:Creating metrics dataframe
2024-08-31 17:25:26,175:INFO:Uploading results into container
2024-08-31 17:25:26,176:INFO:Uploading model into container now
2024-08-31 17:25:26,177:INFO:_master_model_container: 2
2024-08-31 17:25:26,177:INFO:_display_container: 2
2024-08-31 17:25:26,177:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 17:25:26,177:INFO:create_model() successfully completed......................................
2024-08-31 17:25:26,297:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:26,298:INFO:Creating metrics dataframe
2024-08-31 17:25:26,305:INFO:Initializing Naive Bayes
2024-08-31 17:25:26,305:INFO:Total runtime is 0.20027733643849693 minutes
2024-08-31 17:25:26,308:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:26,308:INFO:Initializing create_model()
2024-08-31 17:25:26,308:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:26,308:INFO:Checking exceptions
2024-08-31 17:25:26,308:INFO:Importing libraries
2024-08-31 17:25:26,308:INFO:Copying training dataset
2024-08-31 17:25:26,318:INFO:Defining folds
2024-08-31 17:25:26,318:INFO:Declaring metric variables
2024-08-31 17:25:26,322:INFO:Importing untrained model
2024-08-31 17:25:26,325:INFO:Naive Bayes Imported successfully
2024-08-31 17:25:26,330:INFO:Starting cross validation
2024-08-31 17:25:26,331:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:26,495:INFO:Calculating mean and std
2024-08-31 17:25:26,496:INFO:Creating metrics dataframe
2024-08-31 17:25:26,498:INFO:Uploading results into container
2024-08-31 17:25:26,499:INFO:Uploading model into container now
2024-08-31 17:25:26,499:INFO:_master_model_container: 3
2024-08-31 17:25:26,499:INFO:_display_container: 2
2024-08-31 17:25:26,499:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 17:25:26,499:INFO:create_model() successfully completed......................................
2024-08-31 17:25:26,624:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:26,625:INFO:Creating metrics dataframe
2024-08-31 17:25:26,631:INFO:Initializing Decision Tree Classifier
2024-08-31 17:25:26,632:INFO:Total runtime is 0.20572700500488283 minutes
2024-08-31 17:25:26,635:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:26,635:INFO:Initializing create_model()
2024-08-31 17:25:26,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:26,635:INFO:Checking exceptions
2024-08-31 17:25:26,635:INFO:Importing libraries
2024-08-31 17:25:26,635:INFO:Copying training dataset
2024-08-31 17:25:26,646:INFO:Defining folds
2024-08-31 17:25:26,647:INFO:Declaring metric variables
2024-08-31 17:25:26,650:INFO:Importing untrained model
2024-08-31 17:25:26,652:INFO:Decision Tree Classifier Imported successfully
2024-08-31 17:25:26,660:INFO:Starting cross validation
2024-08-31 17:25:26,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:26,889:INFO:Calculating mean and std
2024-08-31 17:25:26,891:INFO:Creating metrics dataframe
2024-08-31 17:25:26,893:INFO:Uploading results into container
2024-08-31 17:25:26,893:INFO:Uploading model into container now
2024-08-31 17:25:26,894:INFO:_master_model_container: 4
2024-08-31 17:25:26,894:INFO:_display_container: 2
2024-08-31 17:25:26,894:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 17:25:26,894:INFO:create_model() successfully completed......................................
2024-08-31 17:25:27,023:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:27,023:INFO:Creating metrics dataframe
2024-08-31 17:25:27,031:INFO:Initializing SVM - Linear Kernel
2024-08-31 17:25:27,031:INFO:Total runtime is 0.21238364775975546 minutes
2024-08-31 17:25:27,034:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:27,035:INFO:Initializing create_model()
2024-08-31 17:25:27,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:27,035:INFO:Checking exceptions
2024-08-31 17:25:27,035:INFO:Importing libraries
2024-08-31 17:25:27,035:INFO:Copying training dataset
2024-08-31 17:25:27,049:INFO:Defining folds
2024-08-31 17:25:27,049:INFO:Declaring metric variables
2024-08-31 17:25:27,053:INFO:Importing untrained model
2024-08-31 17:25:27,056:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 17:25:27,064:INFO:Starting cross validation
2024-08-31 17:25:27,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:27,997:INFO:Calculating mean and std
2024-08-31 17:25:27,998:INFO:Creating metrics dataframe
2024-08-31 17:25:28,000:INFO:Uploading results into container
2024-08-31 17:25:28,001:INFO:Uploading model into container now
2024-08-31 17:25:28,001:INFO:_master_model_container: 5
2024-08-31 17:25:28,002:INFO:_display_container: 2
2024-08-31 17:25:28,002:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 17:25:28,002:INFO:create_model() successfully completed......................................
2024-08-31 17:25:28,119:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:28,120:INFO:Creating metrics dataframe
2024-08-31 17:25:28,126:INFO:Initializing Ridge Classifier
2024-08-31 17:25:28,126:INFO:Total runtime is 0.23064113060633343 minutes
2024-08-31 17:25:28,129:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:28,129:INFO:Initializing create_model()
2024-08-31 17:25:28,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:28,129:INFO:Checking exceptions
2024-08-31 17:25:28,129:INFO:Importing libraries
2024-08-31 17:25:28,129:INFO:Copying training dataset
2024-08-31 17:25:28,144:INFO:Defining folds
2024-08-31 17:25:28,144:INFO:Declaring metric variables
2024-08-31 17:25:28,148:INFO:Importing untrained model
2024-08-31 17:25:28,153:INFO:Ridge Classifier Imported successfully
2024-08-31 17:25:28,163:INFO:Starting cross validation
2024-08-31 17:25:28,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:28,338:INFO:Calculating mean and std
2024-08-31 17:25:28,339:INFO:Creating metrics dataframe
2024-08-31 17:25:28,341:INFO:Uploading results into container
2024-08-31 17:25:28,342:INFO:Uploading model into container now
2024-08-31 17:25:28,342:INFO:_master_model_container: 6
2024-08-31 17:25:28,342:INFO:_display_container: 2
2024-08-31 17:25:28,342:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 17:25:28,342:INFO:create_model() successfully completed......................................
2024-08-31 17:25:28,474:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:28,475:INFO:Creating metrics dataframe
2024-08-31 17:25:28,482:INFO:Initializing Random Forest Classifier
2024-08-31 17:25:28,482:INFO:Total runtime is 0.23657408952713013 minutes
2024-08-31 17:25:28,485:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:28,485:INFO:Initializing create_model()
2024-08-31 17:25:28,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:28,485:INFO:Checking exceptions
2024-08-31 17:25:28,486:INFO:Importing libraries
2024-08-31 17:25:28,486:INFO:Copying training dataset
2024-08-31 17:25:28,498:INFO:Defining folds
2024-08-31 17:25:28,500:INFO:Declaring metric variables
2024-08-31 17:25:28,503:INFO:Importing untrained model
2024-08-31 17:25:28,506:INFO:Random Forest Classifier Imported successfully
2024-08-31 17:25:28,514:INFO:Starting cross validation
2024-08-31 17:25:28,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:31,596:INFO:Calculating mean and std
2024-08-31 17:25:31,598:INFO:Creating metrics dataframe
2024-08-31 17:25:31,601:INFO:Uploading results into container
2024-08-31 17:25:31,602:INFO:Uploading model into container now
2024-08-31 17:25:31,603:INFO:_master_model_container: 7
2024-08-31 17:25:31,603:INFO:_display_container: 2
2024-08-31 17:25:31,604:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 17:25:31,604:INFO:create_model() successfully completed......................................
2024-08-31 17:25:31,745:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:31,745:INFO:Creating metrics dataframe
2024-08-31 17:25:31,753:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 17:25:31,753:INFO:Total runtime is 0.29107858339945475 minutes
2024-08-31 17:25:31,756:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:31,756:INFO:Initializing create_model()
2024-08-31 17:25:31,756:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:31,756:INFO:Checking exceptions
2024-08-31 17:25:31,756:INFO:Importing libraries
2024-08-31 17:25:31,756:INFO:Copying training dataset
2024-08-31 17:25:31,771:INFO:Defining folds
2024-08-31 17:25:31,771:INFO:Declaring metric variables
2024-08-31 17:25:31,774:INFO:Importing untrained model
2024-08-31 17:25:31,777:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 17:25:31,784:INFO:Starting cross validation
2024-08-31 17:25:31,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:31,907:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,907:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,911:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,913:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,914:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,914:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,914:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,916:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,924:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,926:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 17:25:31,960:INFO:Calculating mean and std
2024-08-31 17:25:31,961:INFO:Creating metrics dataframe
2024-08-31 17:25:31,963:INFO:Uploading results into container
2024-08-31 17:25:31,963:INFO:Uploading model into container now
2024-08-31 17:25:31,964:INFO:_master_model_container: 8
2024-08-31 17:25:31,964:INFO:_display_container: 2
2024-08-31 17:25:31,964:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 17:25:31,964:INFO:create_model() successfully completed......................................
2024-08-31 17:25:32,086:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:32,086:INFO:Creating metrics dataframe
2024-08-31 17:25:32,095:INFO:Initializing Ada Boost Classifier
2024-08-31 17:25:32,095:INFO:Total runtime is 0.2967827836672465 minutes
2024-08-31 17:25:32,098:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:32,099:INFO:Initializing create_model()
2024-08-31 17:25:32,099:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:32,099:INFO:Checking exceptions
2024-08-31 17:25:32,099:INFO:Importing libraries
2024-08-31 17:25:32,099:INFO:Copying training dataset
2024-08-31 17:25:32,113:INFO:Defining folds
2024-08-31 17:25:32,114:INFO:Declaring metric variables
2024-08-31 17:25:32,116:INFO:Importing untrained model
2024-08-31 17:25:32,119:INFO:Ada Boost Classifier Imported successfully
2024-08-31 17:25:32,125:INFO:Starting cross validation
2024-08-31 17:25:32,127:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:32,181:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,183:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,185:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,192:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,200:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,200:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,207:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,208:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,212:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:32,215:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 17:25:33,159:INFO:Calculating mean and std
2024-08-31 17:25:33,161:INFO:Creating metrics dataframe
2024-08-31 17:25:33,164:INFO:Uploading results into container
2024-08-31 17:25:33,165:INFO:Uploading model into container now
2024-08-31 17:25:33,165:INFO:_master_model_container: 9
2024-08-31 17:25:33,166:INFO:_display_container: 2
2024-08-31 17:25:33,166:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 17:25:33,166:INFO:create_model() successfully completed......................................
2024-08-31 17:25:33,307:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:33,307:INFO:Creating metrics dataframe
2024-08-31 17:25:33,316:INFO:Initializing Gradient Boosting Classifier
2024-08-31 17:25:33,316:INFO:Total runtime is 0.31714088122049966 minutes
2024-08-31 17:25:33,319:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:33,320:INFO:Initializing create_model()
2024-08-31 17:25:33,320:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:33,320:INFO:Checking exceptions
2024-08-31 17:25:33,320:INFO:Importing libraries
2024-08-31 17:25:33,320:INFO:Copying training dataset
2024-08-31 17:25:33,335:INFO:Defining folds
2024-08-31 17:25:33,335:INFO:Declaring metric variables
2024-08-31 17:25:33,339:INFO:Importing untrained model
2024-08-31 17:25:33,342:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 17:25:33,349:INFO:Starting cross validation
2024-08-31 17:25:33,350:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:36,174:INFO:Calculating mean and std
2024-08-31 17:25:36,175:INFO:Creating metrics dataframe
2024-08-31 17:25:36,177:INFO:Uploading results into container
2024-08-31 17:25:36,178:INFO:Uploading model into container now
2024-08-31 17:25:36,178:INFO:_master_model_container: 10
2024-08-31 17:25:36,178:INFO:_display_container: 2
2024-08-31 17:25:36,179:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 17:25:36,179:INFO:create_model() successfully completed......................................
2024-08-31 17:25:36,308:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:36,308:INFO:Creating metrics dataframe
2024-08-31 17:25:36,317:INFO:Initializing Linear Discriminant Analysis
2024-08-31 17:25:36,317:INFO:Total runtime is 0.36715784072875973 minutes
2024-08-31 17:25:36,320:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:36,320:INFO:Initializing create_model()
2024-08-31 17:25:36,320:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:36,320:INFO:Checking exceptions
2024-08-31 17:25:36,320:INFO:Importing libraries
2024-08-31 17:25:36,320:INFO:Copying training dataset
2024-08-31 17:25:36,334:INFO:Defining folds
2024-08-31 17:25:36,334:INFO:Declaring metric variables
2024-08-31 17:25:36,337:INFO:Importing untrained model
2024-08-31 17:25:36,340:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 17:25:36,346:INFO:Starting cross validation
2024-08-31 17:25:36,346:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:36,549:INFO:Calculating mean and std
2024-08-31 17:25:36,550:INFO:Creating metrics dataframe
2024-08-31 17:25:36,552:INFO:Uploading results into container
2024-08-31 17:25:36,553:INFO:Uploading model into container now
2024-08-31 17:25:36,553:INFO:_master_model_container: 11
2024-08-31 17:25:36,553:INFO:_display_container: 2
2024-08-31 17:25:36,554:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 17:25:36,554:INFO:create_model() successfully completed......................................
2024-08-31 17:25:36,674:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:36,674:INFO:Creating metrics dataframe
2024-08-31 17:25:36,682:INFO:Initializing Extra Trees Classifier
2024-08-31 17:25:36,682:INFO:Total runtime is 0.37323605616887406 minutes
2024-08-31 17:25:36,685:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:36,685:INFO:Initializing create_model()
2024-08-31 17:25:36,685:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:36,685:INFO:Checking exceptions
2024-08-31 17:25:36,686:INFO:Importing libraries
2024-08-31 17:25:36,686:INFO:Copying training dataset
2024-08-31 17:25:36,697:INFO:Defining folds
2024-08-31 17:25:36,698:INFO:Declaring metric variables
2024-08-31 17:25:36,701:INFO:Importing untrained model
2024-08-31 17:25:36,704:INFO:Extra Trees Classifier Imported successfully
2024-08-31 17:25:36,710:INFO:Starting cross validation
2024-08-31 17:25:36,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:39,135:INFO:Calculating mean and std
2024-08-31 17:25:39,137:INFO:Creating metrics dataframe
2024-08-31 17:25:39,140:INFO:Uploading results into container
2024-08-31 17:25:39,141:INFO:Uploading model into container now
2024-08-31 17:25:39,142:INFO:_master_model_container: 12
2024-08-31 17:25:39,142:INFO:_display_container: 2
2024-08-31 17:25:39,143:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 17:25:39,143:INFO:create_model() successfully completed......................................
2024-08-31 17:25:39,299:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:39,299:INFO:Creating metrics dataframe
2024-08-31 17:25:39,314:INFO:Initializing Extreme Gradient Boosting
2024-08-31 17:25:39,314:INFO:Total runtime is 0.41709628899892165 minutes
2024-08-31 17:25:39,317:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:39,318:INFO:Initializing create_model()
2024-08-31 17:25:39,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:39,318:INFO:Checking exceptions
2024-08-31 17:25:39,318:INFO:Importing libraries
2024-08-31 17:25:39,318:INFO:Copying training dataset
2024-08-31 17:25:39,333:INFO:Defining folds
2024-08-31 17:25:39,334:INFO:Declaring metric variables
2024-08-31 17:25:39,338:INFO:Importing untrained model
2024-08-31 17:25:39,343:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 17:25:39,352:INFO:Starting cross validation
2024-08-31 17:25:39,353:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:39,923:INFO:Calculating mean and std
2024-08-31 17:25:39,924:INFO:Creating metrics dataframe
2024-08-31 17:25:39,926:INFO:Uploading results into container
2024-08-31 17:25:39,927:INFO:Uploading model into container now
2024-08-31 17:25:39,928:INFO:_master_model_container: 13
2024-08-31 17:25:39,928:INFO:_display_container: 2
2024-08-31 17:25:39,929:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 17:25:39,929:INFO:create_model() successfully completed......................................
2024-08-31 17:25:40,063:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:40,063:INFO:Creating metrics dataframe
2024-08-31 17:25:40,073:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 17:25:40,073:INFO:Total runtime is 0.42975507179896033 minutes
2024-08-31 17:25:40,076:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:40,076:INFO:Initializing create_model()
2024-08-31 17:25:40,076:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:40,076:INFO:Checking exceptions
2024-08-31 17:25:40,078:INFO:Importing libraries
2024-08-31 17:25:40,078:INFO:Copying training dataset
2024-08-31 17:25:40,090:INFO:Defining folds
2024-08-31 17:25:40,090:INFO:Declaring metric variables
2024-08-31 17:25:40,094:INFO:Importing untrained model
2024-08-31 17:25:40,098:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:25:40,107:INFO:Starting cross validation
2024-08-31 17:25:40,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:41,627:INFO:Calculating mean and std
2024-08-31 17:25:41,628:INFO:Creating metrics dataframe
2024-08-31 17:25:41,632:INFO:Uploading results into container
2024-08-31 17:25:41,634:INFO:Uploading model into container now
2024-08-31 17:25:41,635:INFO:_master_model_container: 14
2024-08-31 17:25:41,635:INFO:_display_container: 2
2024-08-31 17:25:41,636:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:25:41,636:INFO:create_model() successfully completed......................................
2024-08-31 17:25:41,781:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:41,781:INFO:Creating metrics dataframe
2024-08-31 17:25:41,791:INFO:Initializing Dummy Classifier
2024-08-31 17:25:41,791:INFO:Total runtime is 0.4583831469217936 minutes
2024-08-31 17:25:41,794:INFO:SubProcess create_model() called ==================================
2024-08-31 17:25:41,795:INFO:Initializing create_model()
2024-08-31 17:25:41,795:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A22036CE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:41,795:INFO:Checking exceptions
2024-08-31 17:25:41,795:INFO:Importing libraries
2024-08-31 17:25:41,795:INFO:Copying training dataset
2024-08-31 17:25:41,808:INFO:Defining folds
2024-08-31 17:25:41,808:INFO:Declaring metric variables
2024-08-31 17:25:41,812:INFO:Importing untrained model
2024-08-31 17:25:41,815:INFO:Dummy Classifier Imported successfully
2024-08-31 17:25:41,822:INFO:Starting cross validation
2024-08-31 17:25:41,824:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:41,894:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,913:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,915:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,940:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,940:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,944:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,951:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,953:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,955:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,959:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 17:25:41,965:INFO:Calculating mean and std
2024-08-31 17:25:41,967:INFO:Creating metrics dataframe
2024-08-31 17:25:41,969:INFO:Uploading results into container
2024-08-31 17:25:41,970:INFO:Uploading model into container now
2024-08-31 17:25:41,970:INFO:_master_model_container: 15
2024-08-31 17:25:41,970:INFO:_display_container: 2
2024-08-31 17:25:41,970:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 17:25:41,971:INFO:create_model() successfully completed......................................
2024-08-31 17:25:42,093:INFO:SubProcess create_model() end ==================================
2024-08-31 17:25:42,093:INFO:Creating metrics dataframe
2024-08-31 17:25:42,112:INFO:Initializing create_model()
2024-08-31 17:25:42,112:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:42,113:INFO:Checking exceptions
2024-08-31 17:25:42,114:INFO:Importing libraries
2024-08-31 17:25:42,114:INFO:Copying training dataset
2024-08-31 17:25:42,125:INFO:Defining folds
2024-08-31 17:25:42,125:INFO:Declaring metric variables
2024-08-31 17:25:42,125:INFO:Importing untrained model
2024-08-31 17:25:42,125:INFO:Declaring custom model
2024-08-31 17:25:42,127:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:25:42,127:INFO:Cross validation set to False
2024-08-31 17:25:42,127:INFO:Fitting Model
2024-08-31 17:25:42,160:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 17:25:42,161:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-31 17:25:42,163:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.
2024-08-31 17:25:42,163:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 17:25:42,163:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 17:25:42,163:INFO:[LightGBM] [Info] Total Bins 632
2024-08-31 17:25:42,163:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-31 17:25:42,164:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-31 17:25:42,164:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-31 17:25:42,298:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:25:42,299:INFO:create_model() successfully completed......................................
2024-08-31 17:25:42,479:INFO:_master_model_container: 15
2024-08-31 17:25:42,479:INFO:_display_container: 2
2024-08-31 17:25:42,480:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:25:42,480:INFO:compare_models() successfully completed......................................
2024-08-31 17:25:42,481:INFO:Initializing create_model()
2024-08-31 17:25:42,481:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:25:42,481:INFO:Checking exceptions
2024-08-31 17:25:42,491:INFO:Importing libraries
2024-08-31 17:25:42,491:INFO:Copying training dataset
2024-08-31 17:25:42,511:INFO:Defining folds
2024-08-31 17:25:42,511:INFO:Declaring metric variables
2024-08-31 17:25:42,515:INFO:Importing untrained model
2024-08-31 17:25:42,515:INFO:Declaring custom model
2024-08-31 17:25:42,520:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:25:42,528:INFO:Starting cross validation
2024-08-31 17:25:42,529:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:25:44,113:INFO:Calculating mean and std
2024-08-31 17:25:44,116:INFO:Creating metrics dataframe
2024-08-31 17:25:44,127:INFO:Finalizing model
2024-08-31 17:25:44,180:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 17:25:44,180:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-31 17:25:44,183:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.
2024-08-31 17:25:44,183:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 17:25:44,183:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 17:25:44,184:INFO:[LightGBM] [Info] Total Bins 632
2024-08-31 17:25:44,184:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-31 17:25:44,184:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-31 17:25:44,184:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-31 17:25:44,304:INFO:Uploading results into container
2024-08-31 17:25:44,305:INFO:Uploading model into container now
2024-08-31 17:25:44,319:INFO:_master_model_container: 16
2024-08-31 17:25:44,320:INFO:_display_container: 3
2024-08-31 17:25:44,320:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:25:44,321:INFO:create_model() successfully completed......................................
2024-08-31 17:25:44,472:INFO:Initializing tune_model()
2024-08-31 17:25:44,472:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 17:25:44,473:INFO:Checking exceptions
2024-08-31 17:25:44,488:INFO:Copying training dataset
2024-08-31 17:25:44,502:INFO:Checking base model
2024-08-31 17:25:44,503:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 17:25:44,531:INFO:Declaring metric variables
2024-08-31 17:25:44,537:INFO:Defining Hyperparameters
2024-08-31 17:25:44,670:INFO:Tuning with n_jobs=-1
2024-08-31 17:25:44,670:INFO:Initializing RandomizedSearchCV
2024-08-31 17:26:05,463:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 17:26:05,464:INFO:Hyperparameter search completed
2024-08-31 17:26:05,464:INFO:SubProcess create_model() called ==================================
2024-08-31 17:26:05,465:INFO:Initializing create_model()
2024-08-31 17:26:05,465:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A217BEA510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 17:26:05,465:INFO:Checking exceptions
2024-08-31 17:26:05,466:INFO:Importing libraries
2024-08-31 17:26:05,466:INFO:Copying training dataset
2024-08-31 17:26:05,488:INFO:Defining folds
2024-08-31 17:26:05,488:INFO:Declaring metric variables
2024-08-31 17:26:05,492:INFO:Importing untrained model
2024-08-31 17:26:05,492:INFO:Declaring custom model
2024-08-31 17:26:05,498:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:26:05,506:INFO:Starting cross validation
2024-08-31 17:26:05,508:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:26:07,494:INFO:Calculating mean and std
2024-08-31 17:26:07,496:INFO:Creating metrics dataframe
2024-08-31 17:26:07,505:INFO:Finalizing model
2024-08-31 17:26:07,540:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 17:26:07,540:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 17:26:07,540:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 17:26:07,550:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 17:26:07,550:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 17:26:07,551:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 17:26:07,551:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 17:26:07,551:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-31 17:26:07,553:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.
2024-08-31 17:26:07,553:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 17:26:07,553:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 17:26:07,554:INFO:[LightGBM] [Info] Total Bins 632
2024-08-31 17:26:07,554:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-31 17:26:07,554:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-31 17:26:07,554:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-31 17:26:07,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,733:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,781:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,808:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,811:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,814:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,816:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,819:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,825:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 17:26:07,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 17:26:07,844:INFO:Uploading results into container
2024-08-31 17:26:07,845:INFO:Uploading model into container now
2024-08-31 17:26:07,846:INFO:_master_model_container: 17
2024-08-31 17:26:07,846:INFO:_display_container: 4
2024-08-31 17:26:07,848:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:26:07,848:INFO:create_model() successfully completed......................................
2024-08-31 17:26:08,004:INFO:SubProcess create_model() end ==================================
2024-08-31 17:26:08,004:INFO:choose_better activated
2024-08-31 17:26:08,007:INFO:SubProcess create_model() called ==================================
2024-08-31 17:26:08,008:INFO:Initializing create_model()
2024-08-31 17:26:08,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:26:08,008:INFO:Checking exceptions
2024-08-31 17:26:08,010:INFO:Importing libraries
2024-08-31 17:26:08,010:INFO:Copying training dataset
2024-08-31 17:26:08,023:INFO:Defining folds
2024-08-31 17:26:08,024:INFO:Declaring metric variables
2024-08-31 17:26:08,024:INFO:Importing untrained model
2024-08-31 17:26:08,024:INFO:Declaring custom model
2024-08-31 17:26:08,024:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:26:08,024:INFO:Starting cross validation
2024-08-31 17:26:08,025:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 17:26:09,397:INFO:Calculating mean and std
2024-08-31 17:26:09,397:INFO:Creating metrics dataframe
2024-08-31 17:26:09,399:INFO:Finalizing model
2024-08-31 17:26:09,440:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 17:26:09,441:INFO:[LightGBM] [Info] Number of positive: 4395, number of negative: 13853
2024-08-31 17:26:09,443:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.
2024-08-31 17:26:09,444:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 17:26:09,444:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 17:26:09,444:INFO:[LightGBM] [Info] Total Bins 632
2024-08-31 17:26:09,444:INFO:[LightGBM] [Info] Number of data points in the train set: 18248, number of used features: 13
2024-08-31 17:26:09,444:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240848 -> initscore=-1.148034
2024-08-31 17:26:09,444:INFO:[LightGBM] [Info] Start training from score -1.148034
2024-08-31 17:26:09,572:INFO:Uploading results into container
2024-08-31 17:26:09,573:INFO:Uploading model into container now
2024-08-31 17:26:09,573:INFO:_master_model_container: 18
2024-08-31 17:26:09,573:INFO:_display_container: 5
2024-08-31 17:26:09,574:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:26:09,574:INFO:create_model() successfully completed......................................
2024-08-31 17:26:09,719:INFO:SubProcess create_model() end ==================================
2024-08-31 17:26:09,719:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8645
2024-08-31 17:26:09,720:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8641
2024-08-31 17:26:09,720:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 17:26:09,721:INFO:choose_better completed
2024-08-31 17:26:09,721:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 17:26:09,730:INFO:_master_model_container: 18
2024-08-31 17:26:09,730:INFO:_display_container: 4
2024-08-31 17:26:09,731:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:26:09,731:INFO:tune_model() successfully completed......................................
2024-08-31 17:26:09,853:INFO:Initializing finalize_model()
2024-08-31 17:26:09,853:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 17:26:09,853:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 17:26:09,860:INFO:Initializing create_model()
2024-08-31 17:26:09,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 17:26:09,860:INFO:Checking exceptions
2024-08-31 17:26:09,861:INFO:Importing libraries
2024-08-31 17:26:09,861:INFO:Copying training dataset
2024-08-31 17:26:09,862:INFO:Defining folds
2024-08-31 17:26:09,862:INFO:Declaring metric variables
2024-08-31 17:26:09,862:INFO:Importing untrained model
2024-08-31 17:26:09,862:INFO:Declaring custom model
2024-08-31 17:26:09,862:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 17:26:09,863:INFO:Cross validation set to False
2024-08-31 17:26:09,863:INFO:Fitting Model
2024-08-31 17:26:09,894:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 17:26:09,894:INFO:[LightGBM] [Info] Number of positive: 6279, number of negative: 19790
2024-08-31 17:26:09,897:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000833 seconds.
2024-08-31 17:26:09,897:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 17:26:09,897:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 17:26:09,897:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 17:26:09,897:INFO:[LightGBM] [Info] Number of data points in the train set: 26069, number of used features: 13
2024-08-31 17:26:09,898:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240861 -> initscore=-1.147966
2024-08-31 17:26:09,898:INFO:[LightGBM] [Info] Start training from score -1.147966
2024-08-31 17:26:10,008:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 17:26:10,008:INFO:create_model() successfully completed......................................
2024-08-31 17:26:10,146:INFO:_master_model_container: 18
2024-08-31 17:26:10,147:INFO:_display_container: 4
2024-08-31 17:26:10,151:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 17:26:10,151:INFO:finalize_model() successfully completed......................................
2024-08-31 17:26:10,268:INFO:Initializing predict_model()
2024-08-31 17:26:10,268:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A2216F3060>)
2024-08-31 17:26:10,268:INFO:Checking exceptions
2024-08-31 17:26:10,268:INFO:Preloading libraries
2024-08-31 17:26:10,270:INFO:Set up data.
2024-08-31 17:26:10,275:INFO:Set up index.
2024-08-31 17:26:10,468:INFO:Initializing evaluate_model()
2024-08-31 17:26:10,468:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-31 17:26:10,487:INFO:Initializing plot_model()
2024-08-31 17:26:10,488:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A218535D10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran', 'no_income',
                                             'fnlwgt', 'Pendidikan',
                                             'Jenjang Pendidikan', 'Status',
                                             'Hubungan', 'Etnis', 'sex',
                                             'pendapatan', 'pengeluaran',
                                             'hours per week', 'Asal Negara',
                                             'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=N...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 17:26:10,488:INFO:Checking exceptions
2024-08-31 17:26:10,493:INFO:Preloading libraries
2024-08-31 17:26:10,499:INFO:Copying training dataset
2024-08-31 17:26:10,499:INFO:Plot type: pipeline
2024-08-31 17:26:10,670:INFO:Visual Rendered Successfully
2024-08-31 17:26:10,785:INFO:plot_model() successfully completed......................................
2024-08-31 17:29:39,998:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\feature_engine\encoding\base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) Asal Negara.
  warnings.warn(

2024-08-31 18:05:43,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 18:05:43,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 18:05:43,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 18:05:43,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 18:05:44,105:INFO:PyCaret ClassificationExperiment
2024-08-31 18:05:44,105:INFO:Logging name: clf-default-name
2024-08-31 18:05:44,105:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:05:44,105:INFO:version 3.3.2
2024-08-31 18:05:44,105:INFO:Initializing setup()
2024-08-31 18:05:44,105:INFO:self.USI: e9b8
2024-08-31 18:05:44,105:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:05:44,105:INFO:Checking environment
2024-08-31 18:05:44,105:INFO:python_version: 3.11.9
2024-08-31 18:05:44,105:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:05:44,105:INFO:machine: AMD64
2024-08-31 18:05:44,105:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:05:44,110:INFO:Memory: svmem(total=16867028992, available=4951461888, percent=70.6, used=11915567104, free=4951461888)
2024-08-31 18:05:44,110:INFO:Physical Core: 6
2024-08-31 18:05:44,110:INFO:Logical Core: 12
2024-08-31 18:05:44,110:INFO:Checking libraries
2024-08-31 18:05:44,110:INFO:System:
2024-08-31 18:05:44,110:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:05:44,110:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:05:44,110:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:05:44,110:INFO:PyCaret required dependencies:
2024-08-31 18:05:44,112:INFO:                 pip: 23.2.1
2024-08-31 18:05:44,112:INFO:          setuptools: 67.8.0
2024-08-31 18:05:44,112:INFO:             pycaret: 3.3.2
2024-08-31 18:05:44,112:INFO:             IPython: 8.14.0
2024-08-31 18:05:44,112:INFO:          ipywidgets: 8.1.5
2024-08-31 18:05:44,112:INFO:                tqdm: 4.66.5
2024-08-31 18:05:44,112:INFO:               numpy: 1.24.3
2024-08-31 18:05:44,112:INFO:              pandas: 2.0.3
2024-08-31 18:05:44,112:INFO:              jinja2: 3.1.4
2024-08-31 18:05:44,112:INFO:               scipy: 1.10.1
2024-08-31 18:05:44,112:INFO:              joblib: 1.2.0
2024-08-31 18:05:44,112:INFO:             sklearn: 1.4.2
2024-08-31 18:05:44,112:INFO:                pyod: 2.0.1
2024-08-31 18:05:44,112:INFO:            imblearn: 0.12.3
2024-08-31 18:05:44,112:INFO:   category_encoders: 2.6.3
2024-08-31 18:05:44,112:INFO:            lightgbm: 4.5.0
2024-08-31 18:05:44,112:INFO:               numba: 0.60.0
2024-08-31 18:05:44,112:INFO:            requests: 2.32.3
2024-08-31 18:05:44,112:INFO:          matplotlib: 3.7.1
2024-08-31 18:05:44,112:INFO:          scikitplot: 0.3.7
2024-08-31 18:05:44,112:INFO:         yellowbrick: 1.5
2024-08-31 18:05:44,112:INFO:              plotly: 5.16.1
2024-08-31 18:05:44,112:INFO:    plotly-resampler: Not installed
2024-08-31 18:05:44,112:INFO:             kaleido: 0.2.1
2024-08-31 18:05:44,113:INFO:           schemdraw: 0.15
2024-08-31 18:05:44,113:INFO:         statsmodels: 0.14.2
2024-08-31 18:05:44,113:INFO:              sktime: 0.26.0
2024-08-31 18:05:44,113:INFO:               tbats: 1.1.3
2024-08-31 18:05:44,113:INFO:            pmdarima: 2.0.4
2024-08-31 18:05:44,113:INFO:              psutil: 5.9.0
2024-08-31 18:05:44,113:INFO:          markupsafe: 2.1.3
2024-08-31 18:05:44,113:INFO:             pickle5: Not installed
2024-08-31 18:05:44,113:INFO:         cloudpickle: 3.0.0
2024-08-31 18:05:44,113:INFO:         deprecation: 2.1.0
2024-08-31 18:05:44,113:INFO:              xxhash: 3.5.0
2024-08-31 18:05:44,113:INFO:           wurlitzer: Not installed
2024-08-31 18:05:44,113:INFO:PyCaret optional dependencies:
2024-08-31 18:05:47,167:INFO:                shap: Not installed
2024-08-31 18:05:47,167:INFO:           interpret: Not installed
2024-08-31 18:05:47,167:INFO:                umap: Not installed
2024-08-31 18:05:47,167:INFO:     ydata_profiling: Not installed
2024-08-31 18:05:47,167:INFO:  explainerdashboard: Not installed
2024-08-31 18:05:47,167:INFO:             autoviz: Not installed
2024-08-31 18:05:47,167:INFO:           fairlearn: Not installed
2024-08-31 18:05:47,167:INFO:          deepchecks: Not installed
2024-08-31 18:05:47,167:INFO:             xgboost: 2.0.2
2024-08-31 18:05:47,167:INFO:            catboost: 1.2.5
2024-08-31 18:05:47,167:INFO:              kmodes: Not installed
2024-08-31 18:05:47,167:INFO:             mlxtend: Not installed
2024-08-31 18:05:47,167:INFO:       statsforecast: Not installed
2024-08-31 18:05:47,167:INFO:        tune_sklearn: Not installed
2024-08-31 18:05:47,167:INFO:                 ray: Not installed
2024-08-31 18:05:47,167:INFO:            hyperopt: Not installed
2024-08-31 18:05:47,167:INFO:              optuna: Not installed
2024-08-31 18:05:47,167:INFO:               skopt: Not installed
2024-08-31 18:05:47,167:INFO:              mlflow: Not installed
2024-08-31 18:05:47,167:INFO:              gradio: 4.41.0
2024-08-31 18:05:47,167:INFO:             fastapi: 0.112.1
2024-08-31 18:05:47,167:INFO:             uvicorn: 0.30.6
2024-08-31 18:05:47,167:INFO:              m2cgen: Not installed
2024-08-31 18:05:47,167:INFO:           evidently: Not installed
2024-08-31 18:05:47,167:INFO:               fugue: Not installed
2024-08-31 18:05:47,167:INFO:           streamlit: Not installed
2024-08-31 18:05:47,167:INFO:             prophet: Not installed
2024-08-31 18:05:47,167:INFO:None
2024-08-31 18:05:47,167:INFO:Set up data.
2024-08-31 18:05:47,179:INFO:Set up folding strategy.
2024-08-31 18:05:47,179:INFO:Set up train/test split.
2024-08-31 18:05:47,192:INFO:Set up index.
2024-08-31 18:05:47,192:INFO:Assigning column types.
2024-08-31 18:05:47,201:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:05:47,237:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,239:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,268:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,270:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,307:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,307:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,336:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,339:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,340:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:05:47,375:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,398:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,401:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,437:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:05:47,460:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,463:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,463:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:05:47,522:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,524:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,582:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,585:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:47,586:INFO:Preparing preprocessing pipeline...
2024-08-31 18:05:47,587:INFO:Set up simple imputation.
2024-08-31 18:05:47,589:INFO:Set up column name cleaning.
2024-08-31 18:05:47,624:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:05:47,629:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income']...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:05:47,629:INFO:Creating final display dataframe.
2024-08-31 18:05:47,734:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 17)
4        Transformed data shape       (26035, 17)
5   Transformed train set shape       (18224, 17)
6    Transformed test set shape        (7811, 17)
7              Numeric features                16
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              e9b8
2024-08-31 18:05:47,804:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:47,806:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:48,007:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:05:48,009:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:05:48,011:INFO:setup() successfully completed in 3.91s...............
2024-08-31 18:06:29,784:INFO:PyCaret ClassificationExperiment
2024-08-31 18:06:29,785:INFO:Logging name: clf-default-name
2024-08-31 18:06:29,785:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:06:29,785:INFO:version 3.3.2
2024-08-31 18:06:29,785:INFO:Initializing setup()
2024-08-31 18:06:29,785:INFO:self.USI: b3e3
2024-08-31 18:06:29,785:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:06:29,785:INFO:Checking environment
2024-08-31 18:06:29,785:INFO:python_version: 3.11.9
2024-08-31 18:06:29,785:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:06:29,785:INFO:machine: AMD64
2024-08-31 18:06:29,785:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:06:29,790:INFO:Memory: svmem(total=16867028992, available=4831666176, percent=71.4, used=12035362816, free=4831666176)
2024-08-31 18:06:29,790:INFO:Physical Core: 6
2024-08-31 18:06:29,790:INFO:Logical Core: 12
2024-08-31 18:06:29,790:INFO:Checking libraries
2024-08-31 18:06:29,790:INFO:System:
2024-08-31 18:06:29,790:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:06:29,790:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:06:29,790:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:06:29,790:INFO:PyCaret required dependencies:
2024-08-31 18:06:29,790:INFO:                 pip: 23.2.1
2024-08-31 18:06:29,790:INFO:          setuptools: 67.8.0
2024-08-31 18:06:29,790:INFO:             pycaret: 3.3.2
2024-08-31 18:06:29,790:INFO:             IPython: 8.14.0
2024-08-31 18:06:29,790:INFO:          ipywidgets: 8.1.5
2024-08-31 18:06:29,790:INFO:                tqdm: 4.66.5
2024-08-31 18:06:29,790:INFO:               numpy: 1.24.3
2024-08-31 18:06:29,790:INFO:              pandas: 2.0.3
2024-08-31 18:06:29,791:INFO:              jinja2: 3.1.4
2024-08-31 18:06:29,791:INFO:               scipy: 1.10.1
2024-08-31 18:06:29,791:INFO:              joblib: 1.2.0
2024-08-31 18:06:29,791:INFO:             sklearn: 1.4.2
2024-08-31 18:06:29,791:INFO:                pyod: 2.0.1
2024-08-31 18:06:29,791:INFO:            imblearn: 0.12.3
2024-08-31 18:06:29,791:INFO:   category_encoders: 2.6.3
2024-08-31 18:06:29,791:INFO:            lightgbm: 4.5.0
2024-08-31 18:06:29,791:INFO:               numba: 0.60.0
2024-08-31 18:06:29,791:INFO:            requests: 2.32.3
2024-08-31 18:06:29,791:INFO:          matplotlib: 3.7.1
2024-08-31 18:06:29,791:INFO:          scikitplot: 0.3.7
2024-08-31 18:06:29,791:INFO:         yellowbrick: 1.5
2024-08-31 18:06:29,791:INFO:              plotly: 5.16.1
2024-08-31 18:06:29,791:INFO:    plotly-resampler: Not installed
2024-08-31 18:06:29,791:INFO:             kaleido: 0.2.1
2024-08-31 18:06:29,791:INFO:           schemdraw: 0.15
2024-08-31 18:06:29,791:INFO:         statsmodels: 0.14.2
2024-08-31 18:06:29,791:INFO:              sktime: 0.26.0
2024-08-31 18:06:29,791:INFO:               tbats: 1.1.3
2024-08-31 18:06:29,791:INFO:            pmdarima: 2.0.4
2024-08-31 18:06:29,791:INFO:              psutil: 5.9.0
2024-08-31 18:06:29,791:INFO:          markupsafe: 2.1.3
2024-08-31 18:06:29,791:INFO:             pickle5: Not installed
2024-08-31 18:06:29,791:INFO:         cloudpickle: 3.0.0
2024-08-31 18:06:29,791:INFO:         deprecation: 2.1.0
2024-08-31 18:06:29,791:INFO:              xxhash: 3.5.0
2024-08-31 18:06:29,791:INFO:           wurlitzer: Not installed
2024-08-31 18:06:29,791:INFO:PyCaret optional dependencies:
2024-08-31 18:06:29,791:INFO:                shap: Not installed
2024-08-31 18:06:29,791:INFO:           interpret: Not installed
2024-08-31 18:06:29,791:INFO:                umap: Not installed
2024-08-31 18:06:29,791:INFO:     ydata_profiling: Not installed
2024-08-31 18:06:29,791:INFO:  explainerdashboard: Not installed
2024-08-31 18:06:29,791:INFO:             autoviz: Not installed
2024-08-31 18:06:29,791:INFO:           fairlearn: Not installed
2024-08-31 18:06:29,791:INFO:          deepchecks: Not installed
2024-08-31 18:06:29,792:INFO:             xgboost: 2.0.2
2024-08-31 18:06:29,792:INFO:            catboost: 1.2.5
2024-08-31 18:06:29,792:INFO:              kmodes: Not installed
2024-08-31 18:06:29,792:INFO:             mlxtend: Not installed
2024-08-31 18:06:29,792:INFO:       statsforecast: Not installed
2024-08-31 18:06:29,792:INFO:        tune_sklearn: Not installed
2024-08-31 18:06:29,792:INFO:                 ray: Not installed
2024-08-31 18:06:29,792:INFO:            hyperopt: Not installed
2024-08-31 18:06:29,792:INFO:              optuna: Not installed
2024-08-31 18:06:29,792:INFO:               skopt: Not installed
2024-08-31 18:06:29,792:INFO:              mlflow: Not installed
2024-08-31 18:06:29,792:INFO:              gradio: 4.41.0
2024-08-31 18:06:29,792:INFO:             fastapi: 0.112.1
2024-08-31 18:06:29,792:INFO:             uvicorn: 0.30.6
2024-08-31 18:06:29,792:INFO:              m2cgen: Not installed
2024-08-31 18:06:29,792:INFO:           evidently: Not installed
2024-08-31 18:06:29,792:INFO:               fugue: Not installed
2024-08-31 18:06:29,792:INFO:           streamlit: Not installed
2024-08-31 18:06:29,792:INFO:             prophet: Not installed
2024-08-31 18:06:29,792:INFO:None
2024-08-31 18:06:29,792:INFO:Set up data.
2024-08-31 18:06:29,807:INFO:Set up folding strategy.
2024-08-31 18:06:29,807:INFO:Set up train/test split.
2024-08-31 18:06:29,822:INFO:Set up index.
2024-08-31 18:06:29,823:INFO:Assigning column types.
2024-08-31 18:06:29,842:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:06:29,899:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:06:29,900:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:06:29,929:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:29,931:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:29,967:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:06:29,968:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:06:29,993:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:29,997:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:29,998:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:06:30,035:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:06:30,060:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,062:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:06:30,121:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,123:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,123:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:06:30,184:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,186:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,243:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,245:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,247:INFO:Preparing preprocessing pipeline...
2024-08-31 18:06:30,249:INFO:Set up simple imputation.
2024-08-31 18:06:30,250:INFO:Set up column name cleaning.
2024-08-31 18:06:30,285:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:06:30,288:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income']...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:06:30,288:INFO:Creating final display dataframe.
2024-08-31 18:06:30,393:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 17)
4        Transformed data shape       (26035, 17)
5   Transformed train set shape       (18224, 17)
6    Transformed test set shape        (7811, 17)
7              Numeric features                16
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              b3e3
2024-08-31 18:06:30,459:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,461:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,519:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:06:30,521:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:06:30,522:INFO:setup() successfully completed in 0.74s...............
2024-08-31 18:06:30,523:INFO:Initializing compare_models()
2024-08-31 18:06:30,523:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 18:06:30,523:INFO:Checking exceptions
2024-08-31 18:06:30,530:INFO:Preparing display monitor
2024-08-31 18:06:30,550:INFO:Initializing Logistic Regression
2024-08-31 18:06:30,550:INFO:Total runtime is 0.0 minutes
2024-08-31 18:06:30,553:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:30,553:INFO:Initializing create_model()
2024-08-31 18:06:30,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:30,554:INFO:Checking exceptions
2024-08-31 18:06:30,554:INFO:Importing libraries
2024-08-31 18:06:30,554:INFO:Copying training dataset
2024-08-31 18:06:30,566:INFO:Defining folds
2024-08-31 18:06:30,566:INFO:Declaring metric variables
2024-08-31 18:06:30,569:INFO:Importing untrained model
2024-08-31 18:06:30,572:INFO:Logistic Regression Imported successfully
2024-08-31 18:06:30,578:INFO:Starting cross validation
2024-08-31 18:06:30,579:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:35,398:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:06:35,406:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:06:35,440:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:06:35,468:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:06:35,468:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:06:35,511:INFO:Calculating mean and std
2024-08-31 18:06:35,512:INFO:Creating metrics dataframe
2024-08-31 18:06:35,514:INFO:Uploading results into container
2024-08-31 18:06:35,515:INFO:Uploading model into container now
2024-08-31 18:06:35,515:INFO:_master_model_container: 1
2024-08-31 18:06:35,515:INFO:_display_container: 2
2024-08-31 18:06:35,515:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 18:06:35,515:INFO:create_model() successfully completed......................................
2024-08-31 18:06:35,651:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:35,651:INFO:Creating metrics dataframe
2024-08-31 18:06:35,657:INFO:Initializing K Neighbors Classifier
2024-08-31 18:06:35,657:INFO:Total runtime is 0.0851235548655192 minutes
2024-08-31 18:06:35,659:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:35,659:INFO:Initializing create_model()
2024-08-31 18:06:35,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:35,659:INFO:Checking exceptions
2024-08-31 18:06:35,659:INFO:Importing libraries
2024-08-31 18:06:35,659:INFO:Copying training dataset
2024-08-31 18:06:35,672:INFO:Defining folds
2024-08-31 18:06:35,672:INFO:Declaring metric variables
2024-08-31 18:06:35,675:INFO:Importing untrained model
2024-08-31 18:06:35,677:INFO:K Neighbors Classifier Imported successfully
2024-08-31 18:06:35,683:INFO:Starting cross validation
2024-08-31 18:06:35,684:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:38,838:INFO:Calculating mean and std
2024-08-31 18:06:38,840:INFO:Creating metrics dataframe
2024-08-31 18:06:38,842:INFO:Uploading results into container
2024-08-31 18:06:38,842:INFO:Uploading model into container now
2024-08-31 18:06:38,843:INFO:_master_model_container: 2
2024-08-31 18:06:38,843:INFO:_display_container: 2
2024-08-31 18:06:38,843:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 18:06:38,844:INFO:create_model() successfully completed......................................
2024-08-31 18:06:38,970:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:38,971:INFO:Creating metrics dataframe
2024-08-31 18:06:38,976:INFO:Initializing Naive Bayes
2024-08-31 18:06:38,977:INFO:Total runtime is 0.1404507040977478 minutes
2024-08-31 18:06:38,979:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:38,979:INFO:Initializing create_model()
2024-08-31 18:06:38,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:38,980:INFO:Checking exceptions
2024-08-31 18:06:38,980:INFO:Importing libraries
2024-08-31 18:06:38,980:INFO:Copying training dataset
2024-08-31 18:06:38,993:INFO:Defining folds
2024-08-31 18:06:38,993:INFO:Declaring metric variables
2024-08-31 18:06:38,997:INFO:Importing untrained model
2024-08-31 18:06:39,000:INFO:Naive Bayes Imported successfully
2024-08-31 18:06:39,005:INFO:Starting cross validation
2024-08-31 18:06:39,005:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:40,896:INFO:Calculating mean and std
2024-08-31 18:06:40,897:INFO:Creating metrics dataframe
2024-08-31 18:06:40,899:INFO:Uploading results into container
2024-08-31 18:06:40,899:INFO:Uploading model into container now
2024-08-31 18:06:40,900:INFO:_master_model_container: 3
2024-08-31 18:06:40,900:INFO:_display_container: 2
2024-08-31 18:06:40,900:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 18:06:40,900:INFO:create_model() successfully completed......................................
2024-08-31 18:06:41,026:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:41,026:INFO:Creating metrics dataframe
2024-08-31 18:06:41,033:INFO:Initializing Decision Tree Classifier
2024-08-31 18:06:41,033:INFO:Total runtime is 0.17471590042114257 minutes
2024-08-31 18:06:41,035:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:41,036:INFO:Initializing create_model()
2024-08-31 18:06:41,036:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:41,036:INFO:Checking exceptions
2024-08-31 18:06:41,036:INFO:Importing libraries
2024-08-31 18:06:41,036:INFO:Copying training dataset
2024-08-31 18:06:41,047:INFO:Defining folds
2024-08-31 18:06:41,047:INFO:Declaring metric variables
2024-08-31 18:06:41,051:INFO:Importing untrained model
2024-08-31 18:06:41,055:INFO:Decision Tree Classifier Imported successfully
2024-08-31 18:06:41,059:INFO:Starting cross validation
2024-08-31 18:06:41,060:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:41,200:INFO:Calculating mean and std
2024-08-31 18:06:41,201:INFO:Creating metrics dataframe
2024-08-31 18:06:41,203:INFO:Uploading results into container
2024-08-31 18:06:41,203:INFO:Uploading model into container now
2024-08-31 18:06:41,203:INFO:_master_model_container: 4
2024-08-31 18:06:41,203:INFO:_display_container: 2
2024-08-31 18:06:41,204:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 18:06:41,204:INFO:create_model() successfully completed......................................
2024-08-31 18:06:41,320:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:41,321:INFO:Creating metrics dataframe
2024-08-31 18:06:41,326:INFO:Initializing SVM - Linear Kernel
2024-08-31 18:06:41,327:INFO:Total runtime is 0.17962247133255005 minutes
2024-08-31 18:06:41,330:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:41,330:INFO:Initializing create_model()
2024-08-31 18:06:41,330:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:41,331:INFO:Checking exceptions
2024-08-31 18:06:41,331:INFO:Importing libraries
2024-08-31 18:06:41,331:INFO:Copying training dataset
2024-08-31 18:06:41,342:INFO:Defining folds
2024-08-31 18:06:41,342:INFO:Declaring metric variables
2024-08-31 18:06:41,347:INFO:Importing untrained model
2024-08-31 18:06:41,350:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 18:06:41,355:INFO:Starting cross validation
2024-08-31 18:06:41,356:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:41,600:INFO:Calculating mean and std
2024-08-31 18:06:41,601:INFO:Creating metrics dataframe
2024-08-31 18:06:41,603:INFO:Uploading results into container
2024-08-31 18:06:41,603:INFO:Uploading model into container now
2024-08-31 18:06:41,604:INFO:_master_model_container: 5
2024-08-31 18:06:41,604:INFO:_display_container: 2
2024-08-31 18:06:41,604:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 18:06:41,604:INFO:create_model() successfully completed......................................
2024-08-31 18:06:41,726:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:41,726:INFO:Creating metrics dataframe
2024-08-31 18:06:41,732:INFO:Initializing Ridge Classifier
2024-08-31 18:06:41,732:INFO:Total runtime is 0.18637292385101317 minutes
2024-08-31 18:06:41,735:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:41,735:INFO:Initializing create_model()
2024-08-31 18:06:41,735:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:41,735:INFO:Checking exceptions
2024-08-31 18:06:41,735:INFO:Importing libraries
2024-08-31 18:06:41,735:INFO:Copying training dataset
2024-08-31 18:06:41,746:INFO:Defining folds
2024-08-31 18:06:41,747:INFO:Declaring metric variables
2024-08-31 18:06:41,750:INFO:Importing untrained model
2024-08-31 18:06:41,752:INFO:Ridge Classifier Imported successfully
2024-08-31 18:06:41,758:INFO:Starting cross validation
2024-08-31 18:06:41,758:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:41,852:INFO:Calculating mean and std
2024-08-31 18:06:41,852:INFO:Creating metrics dataframe
2024-08-31 18:06:41,854:INFO:Uploading results into container
2024-08-31 18:06:41,854:INFO:Uploading model into container now
2024-08-31 18:06:41,854:INFO:_master_model_container: 6
2024-08-31 18:06:41,854:INFO:_display_container: 2
2024-08-31 18:06:41,854:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 18:06:41,854:INFO:create_model() successfully completed......................................
2024-08-31 18:06:41,970:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:41,971:INFO:Creating metrics dataframe
2024-08-31 18:06:41,977:INFO:Initializing Random Forest Classifier
2024-08-31 18:06:41,978:INFO:Total runtime is 0.19047573010126748 minutes
2024-08-31 18:06:41,981:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:41,981:INFO:Initializing create_model()
2024-08-31 18:06:41,981:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:41,981:INFO:Checking exceptions
2024-08-31 18:06:41,981:INFO:Importing libraries
2024-08-31 18:06:41,981:INFO:Copying training dataset
2024-08-31 18:06:41,994:INFO:Defining folds
2024-08-31 18:06:41,994:INFO:Declaring metric variables
2024-08-31 18:06:41,998:INFO:Importing untrained model
2024-08-31 18:06:42,001:INFO:Random Forest Classifier Imported successfully
2024-08-31 18:06:42,006:INFO:Starting cross validation
2024-08-31 18:06:42,007:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:43,384:INFO:Calculating mean and std
2024-08-31 18:06:43,385:INFO:Creating metrics dataframe
2024-08-31 18:06:43,387:INFO:Uploading results into container
2024-08-31 18:06:43,387:INFO:Uploading model into container now
2024-08-31 18:06:43,388:INFO:_master_model_container: 7
2024-08-31 18:06:43,388:INFO:_display_container: 2
2024-08-31 18:06:43,388:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 18:06:43,388:INFO:create_model() successfully completed......................................
2024-08-31 18:06:43,507:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:43,507:INFO:Creating metrics dataframe
2024-08-31 18:06:43,513:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 18:06:43,513:INFO:Total runtime is 0.21606117089589436 minutes
2024-08-31 18:06:43,517:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:43,517:INFO:Initializing create_model()
2024-08-31 18:06:43,517:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:43,517:INFO:Checking exceptions
2024-08-31 18:06:43,518:INFO:Importing libraries
2024-08-31 18:06:43,518:INFO:Copying training dataset
2024-08-31 18:06:43,527:INFO:Defining folds
2024-08-31 18:06:43,529:INFO:Declaring metric variables
2024-08-31 18:06:43,532:INFO:Importing untrained model
2024-08-31 18:06:43,535:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 18:06:43,542:INFO:Starting cross validation
2024-08-31 18:06:43,542:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:43,607:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 18:06:43,607:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 18:06:43,607:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 18:06:43,608:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 18:06:43,611:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,616:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,617:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,617:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,617:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,617:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,620:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,621:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,624:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,624:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-08-31 18:06:43,624:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-08-31 18:06:43,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-08-31 18:06:43,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-08-31 18:06:43,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-08-31 18:06:43,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-08-31 18:06:43,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-08-31 18:06:43,645:INFO:Calculating mean and std
2024-08-31 18:06:43,647:INFO:Creating metrics dataframe
2024-08-31 18:06:43,650:INFO:Uploading results into container
2024-08-31 18:06:43,650:INFO:Uploading model into container now
2024-08-31 18:06:43,651:INFO:_master_model_container: 8
2024-08-31 18:06:43,651:INFO:_display_container: 2
2024-08-31 18:06:43,651:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 18:06:43,651:INFO:create_model() successfully completed......................................
2024-08-31 18:06:43,794:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:43,795:INFO:Creating metrics dataframe
2024-08-31 18:06:43,802:INFO:Initializing Ada Boost Classifier
2024-08-31 18:06:43,802:INFO:Total runtime is 0.22087765932083128 minutes
2024-08-31 18:06:43,804:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:43,805:INFO:Initializing create_model()
2024-08-31 18:06:43,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:43,805:INFO:Checking exceptions
2024-08-31 18:06:43,805:INFO:Importing libraries
2024-08-31 18:06:43,805:INFO:Copying training dataset
2024-08-31 18:06:43,818:INFO:Defining folds
2024-08-31 18:06:43,818:INFO:Declaring metric variables
2024-08-31 18:06:43,822:INFO:Importing untrained model
2024-08-31 18:06:43,827:INFO:Ada Boost Classifier Imported successfully
2024-08-31 18:06:43,833:INFO:Starting cross validation
2024-08-31 18:06:43,833:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:43,876:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:06:43,883:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:06:43,883:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:06:43,887:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:06:43,888:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:06:44,454:INFO:Calculating mean and std
2024-08-31 18:06:44,455:INFO:Creating metrics dataframe
2024-08-31 18:06:44,457:INFO:Uploading results into container
2024-08-31 18:06:44,458:INFO:Uploading model into container now
2024-08-31 18:06:44,458:INFO:_master_model_container: 9
2024-08-31 18:06:44,458:INFO:_display_container: 2
2024-08-31 18:06:44,459:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 18:06:44,459:INFO:create_model() successfully completed......................................
2024-08-31 18:06:44,578:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:44,578:INFO:Creating metrics dataframe
2024-08-31 18:06:44,585:INFO:Initializing Gradient Boosting Classifier
2024-08-31 18:06:44,585:INFO:Total runtime is 0.23393062750498453 minutes
2024-08-31 18:06:44,588:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:44,589:INFO:Initializing create_model()
2024-08-31 18:06:44,589:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:44,589:INFO:Checking exceptions
2024-08-31 18:06:44,589:INFO:Importing libraries
2024-08-31 18:06:44,589:INFO:Copying training dataset
2024-08-31 18:06:44,600:INFO:Defining folds
2024-08-31 18:06:44,600:INFO:Declaring metric variables
2024-08-31 18:06:44,603:INFO:Importing untrained model
2024-08-31 18:06:44,607:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 18:06:44,612:INFO:Starting cross validation
2024-08-31 18:06:44,613:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:46,438:INFO:Calculating mean and std
2024-08-31 18:06:46,439:INFO:Creating metrics dataframe
2024-08-31 18:06:46,440:INFO:Uploading results into container
2024-08-31 18:06:46,441:INFO:Uploading model into container now
2024-08-31 18:06:46,441:INFO:_master_model_container: 10
2024-08-31 18:06:46,441:INFO:_display_container: 2
2024-08-31 18:06:46,442:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 18:06:46,442:INFO:create_model() successfully completed......................................
2024-08-31 18:06:46,561:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:46,561:INFO:Creating metrics dataframe
2024-08-31 18:06:46,569:INFO:Initializing Linear Discriminant Analysis
2024-08-31 18:06:46,569:INFO:Total runtime is 0.26698229710261023 minutes
2024-08-31 18:06:46,572:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:46,572:INFO:Initializing create_model()
2024-08-31 18:06:46,572:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:46,572:INFO:Checking exceptions
2024-08-31 18:06:46,572:INFO:Importing libraries
2024-08-31 18:06:46,572:INFO:Copying training dataset
2024-08-31 18:06:46,583:INFO:Defining folds
2024-08-31 18:06:46,584:INFO:Declaring metric variables
2024-08-31 18:06:46,586:INFO:Importing untrained model
2024-08-31 18:06:46,589:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 18:06:46,594:INFO:Starting cross validation
2024-08-31 18:06:46,595:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:46,723:INFO:Calculating mean and std
2024-08-31 18:06:46,724:INFO:Creating metrics dataframe
2024-08-31 18:06:46,725:INFO:Uploading results into container
2024-08-31 18:06:46,726:INFO:Uploading model into container now
2024-08-31 18:06:46,726:INFO:_master_model_container: 11
2024-08-31 18:06:46,726:INFO:_display_container: 2
2024-08-31 18:06:46,726:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 18:06:46,726:INFO:create_model() successfully completed......................................
2024-08-31 18:06:46,843:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:46,843:INFO:Creating metrics dataframe
2024-08-31 18:06:46,851:INFO:Initializing Extra Trees Classifier
2024-08-31 18:06:46,851:INFO:Total runtime is 0.2716898242632548 minutes
2024-08-31 18:06:46,854:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:46,855:INFO:Initializing create_model()
2024-08-31 18:06:46,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:46,855:INFO:Checking exceptions
2024-08-31 18:06:46,855:INFO:Importing libraries
2024-08-31 18:06:46,855:INFO:Copying training dataset
2024-08-31 18:06:46,867:INFO:Defining folds
2024-08-31 18:06:46,867:INFO:Declaring metric variables
2024-08-31 18:06:46,870:INFO:Importing untrained model
2024-08-31 18:06:46,873:INFO:Extra Trees Classifier Imported successfully
2024-08-31 18:06:46,878:INFO:Starting cross validation
2024-08-31 18:06:46,879:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:48,071:INFO:Calculating mean and std
2024-08-31 18:06:48,072:INFO:Creating metrics dataframe
2024-08-31 18:06:48,074:INFO:Uploading results into container
2024-08-31 18:06:48,074:INFO:Uploading model into container now
2024-08-31 18:06:48,075:INFO:_master_model_container: 12
2024-08-31 18:06:48,075:INFO:_display_container: 2
2024-08-31 18:06:48,075:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 18:06:48,075:INFO:create_model() successfully completed......................................
2024-08-31 18:06:48,203:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:48,203:INFO:Creating metrics dataframe
2024-08-31 18:06:48,212:INFO:Initializing Extreme Gradient Boosting
2024-08-31 18:06:48,212:INFO:Total runtime is 0.2943812847137451 minutes
2024-08-31 18:06:48,215:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:48,215:INFO:Initializing create_model()
2024-08-31 18:06:48,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:48,215:INFO:Checking exceptions
2024-08-31 18:06:48,215:INFO:Importing libraries
2024-08-31 18:06:48,215:INFO:Copying training dataset
2024-08-31 18:06:48,228:INFO:Defining folds
2024-08-31 18:06:48,228:INFO:Declaring metric variables
2024-08-31 18:06:48,231:INFO:Importing untrained model
2024-08-31 18:06:48,234:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 18:06:48,239:INFO:Starting cross validation
2024-08-31 18:06:48,240:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:48,641:INFO:Calculating mean and std
2024-08-31 18:06:48,643:INFO:Creating metrics dataframe
2024-08-31 18:06:48,644:INFO:Uploading results into container
2024-08-31 18:06:48,645:INFO:Uploading model into container now
2024-08-31 18:06:48,645:INFO:_master_model_container: 13
2024-08-31 18:06:48,645:INFO:_display_container: 2
2024-08-31 18:06:48,646:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 18:06:48,646:INFO:create_model() successfully completed......................................
2024-08-31 18:06:48,763:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:48,763:INFO:Creating metrics dataframe
2024-08-31 18:06:48,771:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 18:06:48,772:INFO:Total runtime is 0.30370300610860185 minutes
2024-08-31 18:06:48,774:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:48,775:INFO:Initializing create_model()
2024-08-31 18:06:48,775:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:48,775:INFO:Checking exceptions
2024-08-31 18:06:48,775:INFO:Importing libraries
2024-08-31 18:06:48,775:INFO:Copying training dataset
2024-08-31 18:06:48,787:INFO:Defining folds
2024-08-31 18:06:48,787:INFO:Declaring metric variables
2024-08-31 18:06:48,791:INFO:Importing untrained model
2024-08-31 18:06:48,794:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:06:48,799:INFO:Starting cross validation
2024-08-31 18:06:48,799:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:06:49,415:INFO:Calculating mean and std
2024-08-31 18:06:49,417:INFO:Creating metrics dataframe
2024-08-31 18:06:49,420:INFO:Uploading results into container
2024-08-31 18:06:49,421:INFO:Uploading model into container now
2024-08-31 18:06:49,422:INFO:_master_model_container: 14
2024-08-31 18:06:49,422:INFO:_display_container: 2
2024-08-31 18:06:49,423:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:06:49,423:INFO:create_model() successfully completed......................................
2024-08-31 18:06:49,561:INFO:SubProcess create_model() end ==================================
2024-08-31 18:06:49,561:INFO:Creating metrics dataframe
2024-08-31 18:06:49,569:INFO:Initializing CatBoost Classifier
2024-08-31 18:06:49,569:INFO:Total runtime is 0.31698417663574213 minutes
2024-08-31 18:06:49,571:INFO:SubProcess create_model() called ==================================
2024-08-31 18:06:49,572:INFO:Initializing create_model()
2024-08-31 18:06:49,572:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:06:49,572:INFO:Checking exceptions
2024-08-31 18:06:49,572:INFO:Importing libraries
2024-08-31 18:06:49,572:INFO:Copying training dataset
2024-08-31 18:06:49,583:INFO:Defining folds
2024-08-31 18:06:49,584:INFO:Declaring metric variables
2024-08-31 18:06:49,587:INFO:Importing untrained model
2024-08-31 18:06:49,590:INFO:CatBoost Classifier Imported successfully
2024-08-31 18:06:49,595:INFO:Starting cross validation
2024-08-31 18:06:49,595:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:07:05,918:INFO:Calculating mean and std
2024-08-31 18:07:05,919:INFO:Creating metrics dataframe
2024-08-31 18:07:05,921:INFO:Uploading results into container
2024-08-31 18:07:05,921:INFO:Uploading model into container now
2024-08-31 18:07:05,921:INFO:_master_model_container: 15
2024-08-31 18:07:05,921:INFO:_display_container: 2
2024-08-31 18:07:05,923:INFO:<catboost.core.CatBoostClassifier object at 0x00000266A229C550>
2024-08-31 18:07:05,923:INFO:create_model() successfully completed......................................
2024-08-31 18:07:06,087:INFO:SubProcess create_model() end ==================================
2024-08-31 18:07:06,087:INFO:Creating metrics dataframe
2024-08-31 18:07:06,101:INFO:Initializing Dummy Classifier
2024-08-31 18:07:06,102:INFO:Total runtime is 0.5925478339195251 minutes
2024-08-31 18:07:06,107:INFO:SubProcess create_model() called ==================================
2024-08-31 18:07:06,107:INFO:Initializing create_model()
2024-08-31 18:07:06,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A25DE9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:07:06,107:INFO:Checking exceptions
2024-08-31 18:07:06,107:INFO:Importing libraries
2024-08-31 18:07:06,107:INFO:Copying training dataset
2024-08-31 18:07:06,124:INFO:Defining folds
2024-08-31 18:07:06,124:INFO:Declaring metric variables
2024-08-31 18:07:06,128:INFO:Importing untrained model
2024-08-31 18:07:06,132:INFO:Dummy Classifier Imported successfully
2024-08-31 18:07:06,141:INFO:Starting cross validation
2024-08-31 18:07:06,142:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:07:06,211:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:07:06,215:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:07:06,217:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:07:06,218:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:07:06,223:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:07:06,232:INFO:Calculating mean and std
2024-08-31 18:07:06,233:INFO:Creating metrics dataframe
2024-08-31 18:07:06,235:INFO:Uploading results into container
2024-08-31 18:07:06,235:INFO:Uploading model into container now
2024-08-31 18:07:06,237:INFO:_master_model_container: 16
2024-08-31 18:07:06,237:INFO:_display_container: 2
2024-08-31 18:07:06,237:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 18:07:06,237:INFO:create_model() successfully completed......................................
2024-08-31 18:07:06,377:INFO:SubProcess create_model() end ==================================
2024-08-31 18:07:06,377:INFO:Creating metrics dataframe
2024-08-31 18:07:06,395:INFO:Initializing create_model()
2024-08-31 18:07:06,395:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:07:06,397:INFO:Checking exceptions
2024-08-31 18:07:06,399:INFO:Importing libraries
2024-08-31 18:07:06,399:INFO:Copying training dataset
2024-08-31 18:07:06,412:INFO:Defining folds
2024-08-31 18:07:06,412:INFO:Declaring metric variables
2024-08-31 18:07:06,412:INFO:Importing untrained model
2024-08-31 18:07:06,412:INFO:Declaring custom model
2024-08-31 18:07:06,414:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:07:06,414:INFO:Cross validation set to False
2024-08-31 18:07:06,414:INFO:Fitting Model
2024-08-31 18:07:06,451:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:07:06,451:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:07:06,454:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.
2024-08-31 18:07:06,454:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:07:06,454:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:07:06,454:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:07:06,454:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:07:06,455:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:07:06,455:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:07:06,551:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:06,551:INFO:create_model() successfully completed......................................
2024-08-31 18:07:06,735:INFO:_master_model_container: 16
2024-08-31 18:07:06,735:INFO:_display_container: 2
2024-08-31 18:07:06,735:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:06,736:INFO:compare_models() successfully completed......................................
2024-08-31 18:07:06,736:INFO:Initializing create_model()
2024-08-31 18:07:06,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:07:06,736:INFO:Checking exceptions
2024-08-31 18:07:06,751:INFO:Importing libraries
2024-08-31 18:07:06,752:INFO:Copying training dataset
2024-08-31 18:07:06,766:INFO:Defining folds
2024-08-31 18:07:06,766:INFO:Declaring metric variables
2024-08-31 18:07:06,770:INFO:Importing untrained model
2024-08-31 18:07:06,770:INFO:Declaring custom model
2024-08-31 18:07:06,783:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:07:06,791:INFO:Starting cross validation
2024-08-31 18:07:06,791:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:07:08,629:INFO:Calculating mean and std
2024-08-31 18:07:08,630:INFO:Creating metrics dataframe
2024-08-31 18:07:08,641:INFO:Finalizing model
2024-08-31 18:07:08,684:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:07:08,685:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:07:08,688:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000981 seconds.
2024-08-31 18:07:08,688:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:07:08,688:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:07:08,688:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:07:08,688:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:07:08,688:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:07:08,689:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:07:08,804:INFO:Uploading results into container
2024-08-31 18:07:08,805:INFO:Uploading model into container now
2024-08-31 18:07:08,820:INFO:_master_model_container: 17
2024-08-31 18:07:08,820:INFO:_display_container: 3
2024-08-31 18:07:08,821:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:08,821:INFO:create_model() successfully completed......................................
2024-08-31 18:07:08,985:INFO:Initializing tune_model()
2024-08-31 18:07:08,985:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 18:07:08,985:INFO:Checking exceptions
2024-08-31 18:07:09,002:INFO:Copying training dataset
2024-08-31 18:07:09,012:INFO:Checking base model
2024-08-31 18:07:09,013:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 18:07:09,017:INFO:Declaring metric variables
2024-08-31 18:07:09,020:INFO:Defining Hyperparameters
2024-08-31 18:07:09,163:INFO:Tuning with n_jobs=-1
2024-08-31 18:07:09,164:INFO:Initializing RandomizedSearchCV
2024-08-31 18:07:31,651:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 18:07:31,654:INFO:Hyperparameter search completed
2024-08-31 18:07:31,654:INFO:SubProcess create_model() called ==================================
2024-08-31 18:07:31,655:INFO:Initializing create_model()
2024-08-31 18:07:31,655:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026695676710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 18:07:31,656:INFO:Checking exceptions
2024-08-31 18:07:31,656:INFO:Importing libraries
2024-08-31 18:07:31,656:INFO:Copying training dataset
2024-08-31 18:07:31,680:INFO:Defining folds
2024-08-31 18:07:31,681:INFO:Declaring metric variables
2024-08-31 18:07:31,686:INFO:Importing untrained model
2024-08-31 18:07:31,686:INFO:Declaring custom model
2024-08-31 18:07:31,692:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:07:31,703:INFO:Starting cross validation
2024-08-31 18:07:31,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:07:34,553:INFO:Calculating mean and std
2024-08-31 18:07:34,555:INFO:Creating metrics dataframe
2024-08-31 18:07:34,563:INFO:Finalizing model
2024-08-31 18:07:34,599:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:07:34,599:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:07:34,599:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:07:34,613:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:07:34,613:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:07:34,613:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:07:34,613:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:07:34,614:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:07:34,617:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.
2024-08-31 18:07:34,617:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:07:34,617:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:07:34,617:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:07:34,617:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:07:34,618:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:07:34,618:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:07:34,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,811:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,816:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,827:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,845:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,845:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,879:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,886:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,887:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:07:34,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:07:34,925:INFO:Uploading results into container
2024-08-31 18:07:34,926:INFO:Uploading model into container now
2024-08-31 18:07:34,927:INFO:_master_model_container: 18
2024-08-31 18:07:34,928:INFO:_display_container: 4
2024-08-31 18:07:34,929:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:34,929:INFO:create_model() successfully completed......................................
2024-08-31 18:07:35,090:INFO:SubProcess create_model() end ==================================
2024-08-31 18:07:35,090:INFO:choose_better activated
2024-08-31 18:07:35,093:INFO:SubProcess create_model() called ==================================
2024-08-31 18:07:35,094:INFO:Initializing create_model()
2024-08-31 18:07:35,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:07:35,094:INFO:Checking exceptions
2024-08-31 18:07:35,096:INFO:Importing libraries
2024-08-31 18:07:35,097:INFO:Copying training dataset
2024-08-31 18:07:35,113:INFO:Defining folds
2024-08-31 18:07:35,113:INFO:Declaring metric variables
2024-08-31 18:07:35,114:INFO:Importing untrained model
2024-08-31 18:07:35,114:INFO:Declaring custom model
2024-08-31 18:07:35,115:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:07:35,115:INFO:Starting cross validation
2024-08-31 18:07:35,116:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:07:37,220:INFO:Calculating mean and std
2024-08-31 18:07:37,221:INFO:Creating metrics dataframe
2024-08-31 18:07:37,223:INFO:Finalizing model
2024-08-31 18:07:37,270:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:07:37,271:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:07:37,273:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000931 seconds.
2024-08-31 18:07:37,274:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:07:37,274:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:07:37,274:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:07:37,274:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:07:37,274:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:07:37,274:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:07:37,445:INFO:Uploading results into container
2024-08-31 18:07:37,445:INFO:Uploading model into container now
2024-08-31 18:07:37,447:INFO:_master_model_container: 19
2024-08-31 18:07:37,447:INFO:_display_container: 5
2024-08-31 18:07:37,448:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:37,448:INFO:create_model() successfully completed......................................
2024-08-31 18:07:37,627:INFO:SubProcess create_model() end ==================================
2024-08-31 18:07:37,628:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 18:07:37,628:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-08-31 18:07:37,629:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 18:07:37,629:INFO:choose_better completed
2024-08-31 18:07:37,629:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 18:07:37,639:INFO:_master_model_container: 19
2024-08-31 18:07:37,639:INFO:_display_container: 4
2024-08-31 18:07:37,639:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:37,640:INFO:tune_model() successfully completed......................................
2024-08-31 18:07:37,779:INFO:Initializing finalize_model()
2024-08-31 18:07:37,779:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 18:07:37,780:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:07:37,790:INFO:Initializing create_model()
2024-08-31 18:07:37,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:07:37,790:INFO:Checking exceptions
2024-08-31 18:07:37,791:INFO:Importing libraries
2024-08-31 18:07:37,791:INFO:Copying training dataset
2024-08-31 18:07:37,791:INFO:Defining folds
2024-08-31 18:07:37,791:INFO:Declaring metric variables
2024-08-31 18:07:37,791:INFO:Importing untrained model
2024-08-31 18:07:37,791:INFO:Declaring custom model
2024-08-31 18:07:37,793:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:07:37,793:INFO:Cross validation set to False
2024-08-31 18:07:37,793:INFO:Fitting Model
2024-08-31 18:07:37,834:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:07:37,835:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-08-31 18:07:37,837:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000957 seconds.
2024-08-31 18:07:37,837:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:07:37,837:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:07:37,837:INFO:[LightGBM] [Info] Total Bins 681
2024-08-31 18:07:37,837:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-08-31 18:07:37,837:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-08-31 18:07:37,837:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-08-31 18:07:37,982:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income'],
                                    transformer=SimpleImputer(add_indicator...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:07:37,982:INFO:create_model() successfully completed......................................
2024-08-31 18:07:38,130:INFO:_master_model_container: 19
2024-08-31 18:07:38,130:INFO:_display_container: 4
2024-08-31 18:07:38,134:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income'],
                                    transformer=SimpleImputer(add_indicator...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:07:38,135:INFO:finalize_model() successfully completed......................................
2024-08-31 18:07:38,266:INFO:Initializing predict_model()
2024-08-31 18:07:38,267:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002669562A550>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income'],
                                    transformer=SimpleImputer(add_indicator...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002669ADF79C0>)
2024-08-31 18:07:38,267:INFO:Checking exceptions
2024-08-31 18:07:38,267:INFO:Preloading libraries
2024-08-31 18:07:38,269:INFO:Set up data.
2024-08-31 18:07:38,275:INFO:Set up index.
2024-08-31 18:09:02,891:INFO:PyCaret ClassificationExperiment
2024-08-31 18:09:02,891:INFO:Logging name: clf-default-name
2024-08-31 18:09:02,891:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:09:02,891:INFO:version 3.3.2
2024-08-31 18:09:02,891:INFO:Initializing setup()
2024-08-31 18:09:02,891:INFO:self.USI: 0c1b
2024-08-31 18:09:02,891:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:09:02,891:INFO:Checking environment
2024-08-31 18:09:02,892:INFO:python_version: 3.11.9
2024-08-31 18:09:02,892:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:09:02,892:INFO:machine: AMD64
2024-08-31 18:09:02,892:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:09:02,897:INFO:Memory: svmem(total=16867028992, available=2833678336, percent=83.2, used=14033350656, free=2833678336)
2024-08-31 18:09:02,898:INFO:Physical Core: 6
2024-08-31 18:09:02,898:INFO:Logical Core: 12
2024-08-31 18:09:02,898:INFO:Checking libraries
2024-08-31 18:09:02,898:INFO:System:
2024-08-31 18:09:02,898:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:09:02,898:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:09:02,898:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:09:02,898:INFO:PyCaret required dependencies:
2024-08-31 18:09:02,898:INFO:                 pip: 23.2.1
2024-08-31 18:09:02,898:INFO:          setuptools: 67.8.0
2024-08-31 18:09:02,898:INFO:             pycaret: 3.3.2
2024-08-31 18:09:02,898:INFO:             IPython: 8.14.0
2024-08-31 18:09:02,898:INFO:          ipywidgets: 8.1.5
2024-08-31 18:09:02,899:INFO:                tqdm: 4.66.5
2024-08-31 18:09:02,899:INFO:               numpy: 1.24.3
2024-08-31 18:09:02,899:INFO:              pandas: 2.0.3
2024-08-31 18:09:02,899:INFO:              jinja2: 3.1.4
2024-08-31 18:09:02,899:INFO:               scipy: 1.10.1
2024-08-31 18:09:02,899:INFO:              joblib: 1.2.0
2024-08-31 18:09:02,899:INFO:             sklearn: 1.4.2
2024-08-31 18:09:02,899:INFO:                pyod: 2.0.1
2024-08-31 18:09:02,899:INFO:            imblearn: 0.12.3
2024-08-31 18:09:02,899:INFO:   category_encoders: 2.6.3
2024-08-31 18:09:02,899:INFO:            lightgbm: 4.5.0
2024-08-31 18:09:02,899:INFO:               numba: 0.60.0
2024-08-31 18:09:02,899:INFO:            requests: 2.32.3
2024-08-31 18:09:02,899:INFO:          matplotlib: 3.7.1
2024-08-31 18:09:02,899:INFO:          scikitplot: 0.3.7
2024-08-31 18:09:02,899:INFO:         yellowbrick: 1.5
2024-08-31 18:09:02,899:INFO:              plotly: 5.16.1
2024-08-31 18:09:02,899:INFO:    plotly-resampler: Not installed
2024-08-31 18:09:02,899:INFO:             kaleido: 0.2.1
2024-08-31 18:09:02,899:INFO:           schemdraw: 0.15
2024-08-31 18:09:02,899:INFO:         statsmodels: 0.14.2
2024-08-31 18:09:02,899:INFO:              sktime: 0.26.0
2024-08-31 18:09:02,899:INFO:               tbats: 1.1.3
2024-08-31 18:09:02,899:INFO:            pmdarima: 2.0.4
2024-08-31 18:09:02,899:INFO:              psutil: 5.9.0
2024-08-31 18:09:02,899:INFO:          markupsafe: 2.1.3
2024-08-31 18:09:02,899:INFO:             pickle5: Not installed
2024-08-31 18:09:02,899:INFO:         cloudpickle: 3.0.0
2024-08-31 18:09:02,899:INFO:         deprecation: 2.1.0
2024-08-31 18:09:02,900:INFO:              xxhash: 3.5.0
2024-08-31 18:09:02,900:INFO:           wurlitzer: Not installed
2024-08-31 18:09:02,900:INFO:PyCaret optional dependencies:
2024-08-31 18:09:02,900:INFO:                shap: Not installed
2024-08-31 18:09:02,900:INFO:           interpret: Not installed
2024-08-31 18:09:02,900:INFO:                umap: Not installed
2024-08-31 18:09:02,900:INFO:     ydata_profiling: Not installed
2024-08-31 18:09:02,900:INFO:  explainerdashboard: Not installed
2024-08-31 18:09:02,900:INFO:             autoviz: Not installed
2024-08-31 18:09:02,900:INFO:           fairlearn: Not installed
2024-08-31 18:09:02,900:INFO:          deepchecks: Not installed
2024-08-31 18:09:02,900:INFO:             xgboost: 2.0.2
2024-08-31 18:09:02,900:INFO:            catboost: 1.2.5
2024-08-31 18:09:02,900:INFO:              kmodes: Not installed
2024-08-31 18:09:02,900:INFO:             mlxtend: Not installed
2024-08-31 18:09:02,900:INFO:       statsforecast: Not installed
2024-08-31 18:09:02,900:INFO:        tune_sklearn: Not installed
2024-08-31 18:09:02,900:INFO:                 ray: Not installed
2024-08-31 18:09:02,900:INFO:            hyperopt: Not installed
2024-08-31 18:09:02,900:INFO:              optuna: Not installed
2024-08-31 18:09:02,900:INFO:               skopt: Not installed
2024-08-31 18:09:02,900:INFO:              mlflow: Not installed
2024-08-31 18:09:02,900:INFO:              gradio: 4.41.0
2024-08-31 18:09:02,900:INFO:             fastapi: 0.112.1
2024-08-31 18:09:02,900:INFO:             uvicorn: 0.30.6
2024-08-31 18:09:02,900:INFO:              m2cgen: Not installed
2024-08-31 18:09:02,900:INFO:           evidently: Not installed
2024-08-31 18:09:02,901:INFO:               fugue: Not installed
2024-08-31 18:09:02,901:INFO:           streamlit: Not installed
2024-08-31 18:09:02,901:INFO:             prophet: Not installed
2024-08-31 18:09:02,901:INFO:None
2024-08-31 18:09:02,901:INFO:Set up data.
2024-08-31 18:09:02,915:INFO:Set up folding strategy.
2024-08-31 18:09:02,915:INFO:Set up train/test split.
2024-08-31 18:09:02,930:INFO:Set up index.
2024-08-31 18:09:02,930:INFO:Assigning column types.
2024-08-31 18:09:02,941:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:09:02,982:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:09:02,983:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:09:03,007:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,009:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,045:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:09:03,047:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:09:03,067:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,070:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,070:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:09:03,105:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:09:03,129:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,130:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,168:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:09:03,192:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,193:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,194:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:09:03,254:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,256:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,320:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,323:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,324:INFO:Preparing preprocessing pipeline...
2024-08-31 18:09:03,325:INFO:Set up simple imputation.
2024-08-31 18:09:03,327:INFO:Set up column name cleaning.
2024-08-31 18:09:03,360:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:09:03,362:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:09:03,362:INFO:Creating final display dataframe.
2024-08-31 18:09:03,482:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              0c1b
2024-08-31 18:09:03,546:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,548:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,609:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:09:03,611:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:09:03,612:INFO:setup() successfully completed in 0.72s...............
2024-08-31 18:09:03,612:INFO:Initializing compare_models()
2024-08-31 18:09:03,612:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 18:09:03,612:INFO:Checking exceptions
2024-08-31 18:09:03,619:INFO:Preparing display monitor
2024-08-31 18:09:03,638:INFO:Initializing Logistic Regression
2024-08-31 18:09:03,638:INFO:Total runtime is 0.0 minutes
2024-08-31 18:09:03,641:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:03,641:INFO:Initializing create_model()
2024-08-31 18:09:03,641:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:03,641:INFO:Checking exceptions
2024-08-31 18:09:03,641:INFO:Importing libraries
2024-08-31 18:09:03,641:INFO:Copying training dataset
2024-08-31 18:09:03,653:INFO:Defining folds
2024-08-31 18:09:03,653:INFO:Declaring metric variables
2024-08-31 18:09:03,655:INFO:Importing untrained model
2024-08-31 18:09:03,659:INFO:Logistic Regression Imported successfully
2024-08-31 18:09:03,664:INFO:Starting cross validation
2024-08-31 18:09:03,664:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:05,300:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:09:05,308:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:09:05,313:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:09:05,324:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:09:05,327:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:09:05,354:INFO:Calculating mean and std
2024-08-31 18:09:05,356:INFO:Creating metrics dataframe
2024-08-31 18:09:05,357:INFO:Uploading results into container
2024-08-31 18:09:05,358:INFO:Uploading model into container now
2024-08-31 18:09:05,359:INFO:_master_model_container: 1
2024-08-31 18:09:05,359:INFO:_display_container: 2
2024-08-31 18:09:05,359:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 18:09:05,359:INFO:create_model() successfully completed......................................
2024-08-31 18:09:05,523:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:05,524:INFO:Creating metrics dataframe
2024-08-31 18:09:05,531:INFO:Initializing K Neighbors Classifier
2024-08-31 18:09:05,531:INFO:Total runtime is 0.03155722220738729 minutes
2024-08-31 18:09:05,534:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:05,534:INFO:Initializing create_model()
2024-08-31 18:09:05,535:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:05,535:INFO:Checking exceptions
2024-08-31 18:09:05,535:INFO:Importing libraries
2024-08-31 18:09:05,535:INFO:Copying training dataset
2024-08-31 18:09:05,547:INFO:Defining folds
2024-08-31 18:09:05,547:INFO:Declaring metric variables
2024-08-31 18:09:05,552:INFO:Importing untrained model
2024-08-31 18:09:05,555:INFO:K Neighbors Classifier Imported successfully
2024-08-31 18:09:05,561:INFO:Starting cross validation
2024-08-31 18:09:05,562:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:05,943:INFO:Calculating mean and std
2024-08-31 18:09:05,944:INFO:Creating metrics dataframe
2024-08-31 18:09:05,945:INFO:Uploading results into container
2024-08-31 18:09:05,946:INFO:Uploading model into container now
2024-08-31 18:09:05,946:INFO:_master_model_container: 2
2024-08-31 18:09:05,946:INFO:_display_container: 2
2024-08-31 18:09:05,946:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 18:09:05,947:INFO:create_model() successfully completed......................................
2024-08-31 18:09:06,095:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:06,095:INFO:Creating metrics dataframe
2024-08-31 18:09:06,102:INFO:Initializing Naive Bayes
2024-08-31 18:09:06,102:INFO:Total runtime is 0.041068756580352785 minutes
2024-08-31 18:09:06,104:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:06,105:INFO:Initializing create_model()
2024-08-31 18:09:06,105:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:06,105:INFO:Checking exceptions
2024-08-31 18:09:06,105:INFO:Importing libraries
2024-08-31 18:09:06,105:INFO:Copying training dataset
2024-08-31 18:09:06,116:INFO:Defining folds
2024-08-31 18:09:06,116:INFO:Declaring metric variables
2024-08-31 18:09:06,120:INFO:Importing untrained model
2024-08-31 18:09:06,123:INFO:Naive Bayes Imported successfully
2024-08-31 18:09:06,128:INFO:Starting cross validation
2024-08-31 18:09:06,128:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:06,265:INFO:Calculating mean and std
2024-08-31 18:09:06,265:INFO:Creating metrics dataframe
2024-08-31 18:09:06,268:INFO:Uploading results into container
2024-08-31 18:09:06,268:INFO:Uploading model into container now
2024-08-31 18:09:06,269:INFO:_master_model_container: 3
2024-08-31 18:09:06,269:INFO:_display_container: 2
2024-08-31 18:09:06,269:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 18:09:06,269:INFO:create_model() successfully completed......................................
2024-08-31 18:09:06,435:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:06,435:INFO:Creating metrics dataframe
2024-08-31 18:09:06,445:INFO:Initializing Decision Tree Classifier
2024-08-31 18:09:06,445:INFO:Total runtime is 0.04679390986760457 minutes
2024-08-31 18:09:06,449:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:06,450:INFO:Initializing create_model()
2024-08-31 18:09:06,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:06,450:INFO:Checking exceptions
2024-08-31 18:09:06,450:INFO:Importing libraries
2024-08-31 18:09:06,450:INFO:Copying training dataset
2024-08-31 18:09:06,470:INFO:Defining folds
2024-08-31 18:09:06,470:INFO:Declaring metric variables
2024-08-31 18:09:06,474:INFO:Importing untrained model
2024-08-31 18:09:06,479:INFO:Decision Tree Classifier Imported successfully
2024-08-31 18:09:06,486:INFO:Starting cross validation
2024-08-31 18:09:06,488:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:06,668:INFO:Calculating mean and std
2024-08-31 18:09:06,670:INFO:Creating metrics dataframe
2024-08-31 18:09:06,672:INFO:Uploading results into container
2024-08-31 18:09:06,672:INFO:Uploading model into container now
2024-08-31 18:09:06,673:INFO:_master_model_container: 4
2024-08-31 18:09:06,673:INFO:_display_container: 2
2024-08-31 18:09:06,673:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 18:09:06,674:INFO:create_model() successfully completed......................................
2024-08-31 18:09:06,853:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:06,853:INFO:Creating metrics dataframe
2024-08-31 18:09:06,863:INFO:Initializing SVM - Linear Kernel
2024-08-31 18:09:06,864:INFO:Total runtime is 0.053765436013539634 minutes
2024-08-31 18:09:06,866:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:06,867:INFO:Initializing create_model()
2024-08-31 18:09:06,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:06,867:INFO:Checking exceptions
2024-08-31 18:09:06,867:INFO:Importing libraries
2024-08-31 18:09:06,867:INFO:Copying training dataset
2024-08-31 18:09:06,883:INFO:Defining folds
2024-08-31 18:09:06,883:INFO:Declaring metric variables
2024-08-31 18:09:06,887:INFO:Importing untrained model
2024-08-31 18:09:06,892:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 18:09:06,898:INFO:Starting cross validation
2024-08-31 18:09:06,899:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:07,200:INFO:Calculating mean and std
2024-08-31 18:09:07,202:INFO:Creating metrics dataframe
2024-08-31 18:09:07,203:INFO:Uploading results into container
2024-08-31 18:09:07,204:INFO:Uploading model into container now
2024-08-31 18:09:07,204:INFO:_master_model_container: 5
2024-08-31 18:09:07,204:INFO:_display_container: 2
2024-08-31 18:09:07,205:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 18:09:07,205:INFO:create_model() successfully completed......................................
2024-08-31 18:09:07,370:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:07,371:INFO:Creating metrics dataframe
2024-08-31 18:09:07,378:INFO:Initializing Ridge Classifier
2024-08-31 18:09:07,378:INFO:Total runtime is 0.062333881855010986 minutes
2024-08-31 18:09:07,381:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:07,381:INFO:Initializing create_model()
2024-08-31 18:09:07,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:07,381:INFO:Checking exceptions
2024-08-31 18:09:07,381:INFO:Importing libraries
2024-08-31 18:09:07,381:INFO:Copying training dataset
2024-08-31 18:09:07,394:INFO:Defining folds
2024-08-31 18:09:07,394:INFO:Declaring metric variables
2024-08-31 18:09:07,398:INFO:Importing untrained model
2024-08-31 18:09:07,401:INFO:Ridge Classifier Imported successfully
2024-08-31 18:09:07,407:INFO:Starting cross validation
2024-08-31 18:09:07,409:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:07,524:INFO:Calculating mean and std
2024-08-31 18:09:07,526:INFO:Creating metrics dataframe
2024-08-31 18:09:07,527:INFO:Uploading results into container
2024-08-31 18:09:07,528:INFO:Uploading model into container now
2024-08-31 18:09:07,528:INFO:_master_model_container: 6
2024-08-31 18:09:07,528:INFO:_display_container: 2
2024-08-31 18:09:07,528:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 18:09:07,528:INFO:create_model() successfully completed......................................
2024-08-31 18:09:07,684:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:07,685:INFO:Creating metrics dataframe
2024-08-31 18:09:07,692:INFO:Initializing Random Forest Classifier
2024-08-31 18:09:07,692:INFO:Total runtime is 0.06756743590037027 minutes
2024-08-31 18:09:07,695:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:07,695:INFO:Initializing create_model()
2024-08-31 18:09:07,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:07,695:INFO:Checking exceptions
2024-08-31 18:09:07,695:INFO:Importing libraries
2024-08-31 18:09:07,695:INFO:Copying training dataset
2024-08-31 18:09:07,707:INFO:Defining folds
2024-08-31 18:09:07,708:INFO:Declaring metric variables
2024-08-31 18:09:07,711:INFO:Importing untrained model
2024-08-31 18:09:07,714:INFO:Random Forest Classifier Imported successfully
2024-08-31 18:09:07,719:INFO:Starting cross validation
2024-08-31 18:09:07,719:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:09,048:INFO:Calculating mean and std
2024-08-31 18:09:09,049:INFO:Creating metrics dataframe
2024-08-31 18:09:09,051:INFO:Uploading results into container
2024-08-31 18:09:09,051:INFO:Uploading model into container now
2024-08-31 18:09:09,052:INFO:_master_model_container: 7
2024-08-31 18:09:09,052:INFO:_display_container: 2
2024-08-31 18:09:09,052:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 18:09:09,052:INFO:create_model() successfully completed......................................
2024-08-31 18:09:09,207:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:09,208:INFO:Creating metrics dataframe
2024-08-31 18:09:09,216:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 18:09:09,216:INFO:Total runtime is 0.0929715633392334 minutes
2024-08-31 18:09:09,220:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:09,220:INFO:Initializing create_model()
2024-08-31 18:09:09,220:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:09,221:INFO:Checking exceptions
2024-08-31 18:09:09,221:INFO:Importing libraries
2024-08-31 18:09:09,221:INFO:Copying training dataset
2024-08-31 18:09:09,236:INFO:Defining folds
2024-08-31 18:09:09,236:INFO:Declaring metric variables
2024-08-31 18:09:09,240:INFO:Importing untrained model
2024-08-31 18:09:09,249:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 18:09:09,257:INFO:Starting cross validation
2024-08-31 18:09:09,258:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:09,443:INFO:Calculating mean and std
2024-08-31 18:09:09,445:INFO:Creating metrics dataframe
2024-08-31 18:09:09,447:INFO:Uploading results into container
2024-08-31 18:09:09,448:INFO:Uploading model into container now
2024-08-31 18:09:09,449:INFO:_master_model_container: 8
2024-08-31 18:09:09,449:INFO:_display_container: 2
2024-08-31 18:09:09,450:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 18:09:09,450:INFO:create_model() successfully completed......................................
2024-08-31 18:09:09,638:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:09,638:INFO:Creating metrics dataframe
2024-08-31 18:09:09,645:INFO:Initializing Ada Boost Classifier
2024-08-31 18:09:09,646:INFO:Total runtime is 0.10013149579366049 minutes
2024-08-31 18:09:09,650:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:09,650:INFO:Initializing create_model()
2024-08-31 18:09:09,650:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:09,650:INFO:Checking exceptions
2024-08-31 18:09:09,650:INFO:Importing libraries
2024-08-31 18:09:09,650:INFO:Copying training dataset
2024-08-31 18:09:09,662:INFO:Defining folds
2024-08-31 18:09:09,662:INFO:Declaring metric variables
2024-08-31 18:09:09,665:INFO:Importing untrained model
2024-08-31 18:09:09,668:INFO:Ada Boost Classifier Imported successfully
2024-08-31 18:09:09,674:INFO:Starting cross validation
2024-08-31 18:09:09,675:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:09,717:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:09:09,723:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:09:09,724:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:09:09,732:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:09:09,737:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:09:10,309:INFO:Calculating mean and std
2024-08-31 18:09:10,310:INFO:Creating metrics dataframe
2024-08-31 18:09:10,312:INFO:Uploading results into container
2024-08-31 18:09:10,312:INFO:Uploading model into container now
2024-08-31 18:09:10,313:INFO:_master_model_container: 9
2024-08-31 18:09:10,313:INFO:_display_container: 2
2024-08-31 18:09:10,313:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 18:09:10,313:INFO:create_model() successfully completed......................................
2024-08-31 18:09:10,465:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:10,465:INFO:Creating metrics dataframe
2024-08-31 18:09:10,472:INFO:Initializing Gradient Boosting Classifier
2024-08-31 18:09:10,473:INFO:Total runtime is 0.11392792065938315 minutes
2024-08-31 18:09:10,475:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:10,475:INFO:Initializing create_model()
2024-08-31 18:09:10,475:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:10,475:INFO:Checking exceptions
2024-08-31 18:09:10,475:INFO:Importing libraries
2024-08-31 18:09:10,475:INFO:Copying training dataset
2024-08-31 18:09:10,487:INFO:Defining folds
2024-08-31 18:09:10,487:INFO:Declaring metric variables
2024-08-31 18:09:10,491:INFO:Importing untrained model
2024-08-31 18:09:10,494:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 18:09:10,500:INFO:Starting cross validation
2024-08-31 18:09:10,500:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:12,346:INFO:Calculating mean and std
2024-08-31 18:09:12,347:INFO:Creating metrics dataframe
2024-08-31 18:09:12,349:INFO:Uploading results into container
2024-08-31 18:09:12,350:INFO:Uploading model into container now
2024-08-31 18:09:12,350:INFO:_master_model_container: 10
2024-08-31 18:09:12,350:INFO:_display_container: 2
2024-08-31 18:09:12,350:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 18:09:12,350:INFO:create_model() successfully completed......................................
2024-08-31 18:09:12,500:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:12,500:INFO:Creating metrics dataframe
2024-08-31 18:09:12,510:INFO:Initializing Linear Discriminant Analysis
2024-08-31 18:09:12,510:INFO:Total runtime is 0.14786805311838785 minutes
2024-08-31 18:09:12,513:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:12,513:INFO:Initializing create_model()
2024-08-31 18:09:12,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:12,513:INFO:Checking exceptions
2024-08-31 18:09:12,513:INFO:Importing libraries
2024-08-31 18:09:12,513:INFO:Copying training dataset
2024-08-31 18:09:12,525:INFO:Defining folds
2024-08-31 18:09:12,525:INFO:Declaring metric variables
2024-08-31 18:09:12,528:INFO:Importing untrained model
2024-08-31 18:09:12,531:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 18:09:12,537:INFO:Starting cross validation
2024-08-31 18:09:12,538:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:12,642:INFO:Calculating mean and std
2024-08-31 18:09:12,643:INFO:Creating metrics dataframe
2024-08-31 18:09:12,644:INFO:Uploading results into container
2024-08-31 18:09:12,645:INFO:Uploading model into container now
2024-08-31 18:09:12,645:INFO:_master_model_container: 11
2024-08-31 18:09:12,645:INFO:_display_container: 2
2024-08-31 18:09:12,645:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 18:09:12,645:INFO:create_model() successfully completed......................................
2024-08-31 18:09:12,801:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:12,801:INFO:Creating metrics dataframe
2024-08-31 18:09:12,808:INFO:Initializing Extra Trees Classifier
2024-08-31 18:09:12,808:INFO:Total runtime is 0.15284640391667684 minutes
2024-08-31 18:09:12,810:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:12,811:INFO:Initializing create_model()
2024-08-31 18:09:12,811:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:12,811:INFO:Checking exceptions
2024-08-31 18:09:12,811:INFO:Importing libraries
2024-08-31 18:09:12,811:INFO:Copying training dataset
2024-08-31 18:09:12,823:INFO:Defining folds
2024-08-31 18:09:12,823:INFO:Declaring metric variables
2024-08-31 18:09:12,827:INFO:Importing untrained model
2024-08-31 18:09:12,829:INFO:Extra Trees Classifier Imported successfully
2024-08-31 18:09:12,835:INFO:Starting cross validation
2024-08-31 18:09:12,836:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:14,007:INFO:Calculating mean and std
2024-08-31 18:09:14,008:INFO:Creating metrics dataframe
2024-08-31 18:09:14,009:INFO:Uploading results into container
2024-08-31 18:09:14,010:INFO:Uploading model into container now
2024-08-31 18:09:14,011:INFO:_master_model_container: 12
2024-08-31 18:09:14,011:INFO:_display_container: 2
2024-08-31 18:09:14,011:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 18:09:14,011:INFO:create_model() successfully completed......................................
2024-08-31 18:09:14,179:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:14,180:INFO:Creating metrics dataframe
2024-08-31 18:09:14,187:INFO:Initializing Extreme Gradient Boosting
2024-08-31 18:09:14,187:INFO:Total runtime is 0.17582550446192424 minutes
2024-08-31 18:09:14,190:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:14,190:INFO:Initializing create_model()
2024-08-31 18:09:14,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:14,190:INFO:Checking exceptions
2024-08-31 18:09:14,190:INFO:Importing libraries
2024-08-31 18:09:14,190:INFO:Copying training dataset
2024-08-31 18:09:14,202:INFO:Defining folds
2024-08-31 18:09:14,202:INFO:Declaring metric variables
2024-08-31 18:09:14,204:INFO:Importing untrained model
2024-08-31 18:09:14,208:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 18:09:14,216:INFO:Starting cross validation
2024-08-31 18:09:14,217:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:14,602:INFO:Calculating mean and std
2024-08-31 18:09:14,603:INFO:Creating metrics dataframe
2024-08-31 18:09:14,605:INFO:Uploading results into container
2024-08-31 18:09:14,605:INFO:Uploading model into container now
2024-08-31 18:09:14,606:INFO:_master_model_container: 13
2024-08-31 18:09:14,606:INFO:_display_container: 2
2024-08-31 18:09:14,607:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 18:09:14,607:INFO:create_model() successfully completed......................................
2024-08-31 18:09:14,789:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:14,789:INFO:Creating metrics dataframe
2024-08-31 18:09:14,798:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 18:09:14,798:INFO:Total runtime is 0.18600446780522664 minutes
2024-08-31 18:09:14,801:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:14,801:INFO:Initializing create_model()
2024-08-31 18:09:14,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:14,801:INFO:Checking exceptions
2024-08-31 18:09:14,801:INFO:Importing libraries
2024-08-31 18:09:14,801:INFO:Copying training dataset
2024-08-31 18:09:14,813:INFO:Defining folds
2024-08-31 18:09:14,813:INFO:Declaring metric variables
2024-08-31 18:09:14,816:INFO:Importing untrained model
2024-08-31 18:09:14,819:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:09:14,825:INFO:Starting cross validation
2024-08-31 18:09:14,825:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:15,575:INFO:Calculating mean and std
2024-08-31 18:09:15,577:INFO:Creating metrics dataframe
2024-08-31 18:09:15,581:INFO:Uploading results into container
2024-08-31 18:09:15,581:INFO:Uploading model into container now
2024-08-31 18:09:15,582:INFO:_master_model_container: 14
2024-08-31 18:09:15,582:INFO:_display_container: 2
2024-08-31 18:09:15,582:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:09:15,582:INFO:create_model() successfully completed......................................
2024-08-31 18:09:15,753:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:15,753:INFO:Creating metrics dataframe
2024-08-31 18:09:15,760:INFO:Initializing CatBoost Classifier
2024-08-31 18:09:15,760:INFO:Total runtime is 0.20204058090845745 minutes
2024-08-31 18:09:15,764:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:15,764:INFO:Initializing create_model()
2024-08-31 18:09:15,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:15,765:INFO:Checking exceptions
2024-08-31 18:09:15,765:INFO:Importing libraries
2024-08-31 18:09:15,765:INFO:Copying training dataset
2024-08-31 18:09:15,776:INFO:Defining folds
2024-08-31 18:09:15,776:INFO:Declaring metric variables
2024-08-31 18:09:15,779:INFO:Importing untrained model
2024-08-31 18:09:15,782:INFO:CatBoost Classifier Imported successfully
2024-08-31 18:09:15,790:INFO:Starting cross validation
2024-08-31 18:09:15,791:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:32,409:INFO:Calculating mean and std
2024-08-31 18:09:32,411:INFO:Creating metrics dataframe
2024-08-31 18:09:32,413:INFO:Uploading results into container
2024-08-31 18:09:32,413:INFO:Uploading model into container now
2024-08-31 18:09:32,414:INFO:_master_model_container: 15
2024-08-31 18:09:32,414:INFO:_display_container: 2
2024-08-31 18:09:32,414:INFO:<catboost.core.CatBoostClassifier object at 0x00000266A6582AD0>
2024-08-31 18:09:32,414:INFO:create_model() successfully completed......................................
2024-08-31 18:09:32,599:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:32,599:INFO:Creating metrics dataframe
2024-08-31 18:09:32,609:INFO:Initializing Dummy Classifier
2024-08-31 18:09:32,610:INFO:Total runtime is 0.48287620941797893 minutes
2024-08-31 18:09:32,613:INFO:SubProcess create_model() called ==================================
2024-08-31 18:09:32,613:INFO:Initializing create_model()
2024-08-31 18:09:32,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A6422610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:32,613:INFO:Checking exceptions
2024-08-31 18:09:32,613:INFO:Importing libraries
2024-08-31 18:09:32,613:INFO:Copying training dataset
2024-08-31 18:09:32,628:INFO:Defining folds
2024-08-31 18:09:32,628:INFO:Declaring metric variables
2024-08-31 18:09:32,633:INFO:Importing untrained model
2024-08-31 18:09:32,637:INFO:Dummy Classifier Imported successfully
2024-08-31 18:09:32,644:INFO:Starting cross validation
2024-08-31 18:09:32,645:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:32,716:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:09:32,717:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:09:32,719:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:09:32,728:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:09:32,739:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:09:32,745:INFO:Calculating mean and std
2024-08-31 18:09:32,747:INFO:Creating metrics dataframe
2024-08-31 18:09:32,749:INFO:Uploading results into container
2024-08-31 18:09:32,750:INFO:Uploading model into container now
2024-08-31 18:09:32,750:INFO:_master_model_container: 16
2024-08-31 18:09:32,750:INFO:_display_container: 2
2024-08-31 18:09:32,750:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 18:09:32,750:INFO:create_model() successfully completed......................................
2024-08-31 18:09:32,925:INFO:SubProcess create_model() end ==================================
2024-08-31 18:09:32,925:INFO:Creating metrics dataframe
2024-08-31 18:09:32,943:INFO:Initializing create_model()
2024-08-31 18:09:32,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:32,943:INFO:Checking exceptions
2024-08-31 18:09:32,945:INFO:Importing libraries
2024-08-31 18:09:32,946:INFO:Copying training dataset
2024-08-31 18:09:32,959:INFO:Defining folds
2024-08-31 18:09:32,959:INFO:Declaring metric variables
2024-08-31 18:09:32,959:INFO:Importing untrained model
2024-08-31 18:09:32,959:INFO:Declaring custom model
2024-08-31 18:09:32,960:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:09:32,961:INFO:Cross validation set to False
2024-08-31 18:09:32,961:INFO:Fitting Model
2024-08-31 18:09:32,997:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:09:32,998:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:09:33,000:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.
2024-08-31 18:09:33,000:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:09:33,000:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:09:33,000:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:09:33,000:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:09:33,000:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:09:33,001:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:09:33,170:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:09:33,170:INFO:create_model() successfully completed......................................
2024-08-31 18:09:33,398:INFO:_master_model_container: 16
2024-08-31 18:09:33,399:INFO:_display_container: 2
2024-08-31 18:09:33,399:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:09:33,399:INFO:compare_models() successfully completed......................................
2024-08-31 18:09:33,400:INFO:Initializing create_model()
2024-08-31 18:09:33,400:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:09:33,400:INFO:Checking exceptions
2024-08-31 18:09:33,416:INFO:Importing libraries
2024-08-31 18:09:33,416:INFO:Copying training dataset
2024-08-31 18:09:33,434:INFO:Defining folds
2024-08-31 18:09:33,434:INFO:Declaring metric variables
2024-08-31 18:09:33,438:INFO:Importing untrained model
2024-08-31 18:09:33,438:INFO:Declaring custom model
2024-08-31 18:09:33,443:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:09:33,454:INFO:Starting cross validation
2024-08-31 18:09:33,455:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:09:35,504:INFO:Calculating mean and std
2024-08-31 18:09:35,507:INFO:Creating metrics dataframe
2024-08-31 18:09:35,517:INFO:Finalizing model
2024-08-31 18:09:35,572:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:09:35,573:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:09:35,575:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.
2024-08-31 18:09:35,575:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:09:35,575:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:09:35,575:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:09:35,575:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:09:35,577:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:09:35,577:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:09:35,731:INFO:Uploading results into container
2024-08-31 18:09:35,733:INFO:Uploading model into container now
2024-08-31 18:09:35,749:INFO:_master_model_container: 17
2024-08-31 18:09:35,749:INFO:_display_container: 3
2024-08-31 18:09:35,749:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:09:35,749:INFO:create_model() successfully completed......................................
2024-08-31 18:09:36,012:INFO:Initializing tune_model()
2024-08-31 18:09:36,014:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 18:09:36,014:INFO:Checking exceptions
2024-08-31 18:09:36,037:INFO:Copying training dataset
2024-08-31 18:09:36,053:INFO:Checking base model
2024-08-31 18:09:36,053:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 18:09:36,058:INFO:Declaring metric variables
2024-08-31 18:09:36,063:INFO:Defining Hyperparameters
2024-08-31 18:09:36,258:INFO:Tuning with n_jobs=-1
2024-08-31 18:09:36,258:INFO:Initializing RandomizedSearchCV
2024-08-31 18:10:02,489:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 18:10:02,490:INFO:Hyperparameter search completed
2024-08-31 18:10:02,491:INFO:SubProcess create_model() called ==================================
2024-08-31 18:10:02,492:INFO:Initializing create_model()
2024-08-31 18:10:02,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002669568AC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 18:10:02,492:INFO:Checking exceptions
2024-08-31 18:10:02,492:INFO:Importing libraries
2024-08-31 18:10:02,492:INFO:Copying training dataset
2024-08-31 18:10:02,519:INFO:Defining folds
2024-08-31 18:10:02,519:INFO:Declaring metric variables
2024-08-31 18:10:02,524:INFO:Importing untrained model
2024-08-31 18:10:02,524:INFO:Declaring custom model
2024-08-31 18:10:02,529:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:10:02,539:INFO:Starting cross validation
2024-08-31 18:10:02,540:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:10:04,778:INFO:Calculating mean and std
2024-08-31 18:10:04,779:INFO:Creating metrics dataframe
2024-08-31 18:10:04,788:INFO:Finalizing model
2024-08-31 18:10:04,823:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:10:04,824:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:10:04,824:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:10:04,836:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:10:04,837:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:10:04,837:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:10:04,837:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:10:04,837:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:10:04,840:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.
2024-08-31 18:10:04,840:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:04,840:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:04,840:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:10:04,840:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:10:04,841:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:10:04,841:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:10:04,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:04,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,104:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,182:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,193:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:10:05,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:10:05,235:INFO:Uploading results into container
2024-08-31 18:10:05,237:INFO:Uploading model into container now
2024-08-31 18:10:05,238:INFO:_master_model_container: 18
2024-08-31 18:10:05,238:INFO:_display_container: 4
2024-08-31 18:10:05,239:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:10:05,239:INFO:create_model() successfully completed......................................
2024-08-31 18:10:05,449:INFO:SubProcess create_model() end ==================================
2024-08-31 18:10:05,450:INFO:choose_better activated
2024-08-31 18:10:05,453:INFO:SubProcess create_model() called ==================================
2024-08-31 18:10:05,454:INFO:Initializing create_model()
2024-08-31 18:10:05,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:10:05,454:INFO:Checking exceptions
2024-08-31 18:10:05,456:INFO:Importing libraries
2024-08-31 18:10:05,456:INFO:Copying training dataset
2024-08-31 18:10:05,469:INFO:Defining folds
2024-08-31 18:10:05,470:INFO:Declaring metric variables
2024-08-31 18:10:05,470:INFO:Importing untrained model
2024-08-31 18:10:05,470:INFO:Declaring custom model
2024-08-31 18:10:05,470:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:10:05,471:INFO:Starting cross validation
2024-08-31 18:10:05,472:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:10:06,877:INFO:Calculating mean and std
2024-08-31 18:10:06,877:INFO:Creating metrics dataframe
2024-08-31 18:10:06,880:INFO:Finalizing model
2024-08-31 18:10:06,921:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:10:06,921:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:10:06,924:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000888 seconds.
2024-08-31 18:10:06,925:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:06,925:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:06,925:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:10:06,925:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:10:06,925:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:10:06,925:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:10:07,103:INFO:Uploading results into container
2024-08-31 18:10:07,104:INFO:Uploading model into container now
2024-08-31 18:10:07,105:INFO:_master_model_container: 19
2024-08-31 18:10:07,105:INFO:_display_container: 5
2024-08-31 18:10:07,105:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:10:07,105:INFO:create_model() successfully completed......................................
2024-08-31 18:10:07,313:INFO:SubProcess create_model() end ==================================
2024-08-31 18:10:07,314:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 18:10:07,314:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-08-31 18:10:07,315:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 18:10:07,315:INFO:choose_better completed
2024-08-31 18:10:07,315:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 18:10:07,326:INFO:_master_model_container: 19
2024-08-31 18:10:07,326:INFO:_display_container: 4
2024-08-31 18:10:07,326:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:10:07,326:INFO:tune_model() successfully completed......................................
2024-08-31 18:10:07,517:INFO:Initializing finalize_model()
2024-08-31 18:10:07,517:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 18:10:07,518:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:10:07,525:INFO:Initializing create_model()
2024-08-31 18:10:07,525:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:10:07,525:INFO:Checking exceptions
2024-08-31 18:10:07,527:INFO:Importing libraries
2024-08-31 18:10:07,527:INFO:Copying training dataset
2024-08-31 18:10:07,527:INFO:Defining folds
2024-08-31 18:10:07,527:INFO:Declaring metric variables
2024-08-31 18:10:07,528:INFO:Importing untrained model
2024-08-31 18:10:07,528:INFO:Declaring custom model
2024-08-31 18:10:07,528:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:10:07,529:INFO:Cross validation set to False
2024-08-31 18:10:07,529:INFO:Fitting Model
2024-08-31 18:10:07,560:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:10:07,561:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-08-31 18:10:07,563:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.
2024-08-31 18:10:07,563:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:07,563:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:07,563:INFO:[LightGBM] [Info] Total Bins 681
2024-08-31 18:10:07,563:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-08-31 18:10:07,564:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-08-31 18:10:07,564:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-08-31 18:10:07,672:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:10:07,673:INFO:create_model() successfully completed......................................
2024-08-31 18:10:07,865:INFO:_master_model_container: 19
2024-08-31 18:10:07,865:INFO:_display_container: 4
2024-08-31 18:10:07,869:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:10:07,869:INFO:finalize_model() successfully completed......................................
2024-08-31 18:10:08,031:INFO:Initializing predict_model()
2024-08-31 18:10:08,031:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002669635D440>)
2024-08-31 18:10:08,031:INFO:Checking exceptions
2024-08-31 18:10:08,031:INFO:Preloading libraries
2024-08-31 18:10:08,033:INFO:Set up data.
2024-08-31 18:10:08,039:INFO:Set up index.
2024-08-31 18:10:08,295:INFO:Initializing evaluate_model()
2024-08-31 18:10:08,295:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-31 18:10:08,312:INFO:Initializing plot_model()
2024-08-31 18:10:08,313:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:10:08,313:INFO:Checking exceptions
2024-08-31 18:10:08,318:INFO:Preloading libraries
2024-08-31 18:10:08,322:INFO:Copying training dataset
2024-08-31 18:10:08,323:INFO:Plot type: pipeline
2024-08-31 18:10:08,478:INFO:Visual Rendered Successfully
2024-08-31 18:10:08,634:INFO:plot_model() successfully completed......................................
2024-08-31 18:10:11,172:INFO:Initializing plot_model()
2024-08-31 18:10:11,173:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:10:11,173:INFO:Checking exceptions
2024-08-31 18:10:11,178:INFO:Preloading libraries
2024-08-31 18:10:11,212:INFO:Copying training dataset
2024-08-31 18:10:11,212:INFO:Plot type: parameter
2024-08-31 18:10:11,217:INFO:Visual Rendered Successfully
2024-08-31 18:10:11,384:INFO:plot_model() successfully completed......................................
2024-08-31 18:10:23,767:INFO:Initializing plot_model()
2024-08-31 18:10:23,767:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:10:23,767:INFO:Checking exceptions
2024-08-31 18:10:23,771:INFO:Preloading libraries
2024-08-31 18:10:23,775:INFO:Copying training dataset
2024-08-31 18:10:23,775:INFO:Plot type: pipeline
2024-08-31 18:10:23,882:INFO:Visual Rendered Successfully
2024-08-31 18:10:24,051:INFO:plot_model() successfully completed......................................
2024-08-31 18:10:25,000:INFO:Initializing plot_model()
2024-08-31 18:10:25,000:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:10:25,000:INFO:Checking exceptions
2024-08-31 18:10:25,006:INFO:Preloading libraries
2024-08-31 18:10:25,011:INFO:Copying training dataset
2024-08-31 18:10:25,011:INFO:Plot type: rfe
2024-08-31 18:10:25,165:INFO:Fitting Model
2024-08-31 18:10:25,189:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,190:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2024-08-31 18:10:25,191:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,191:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,191:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:25,191:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:25,191:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,191:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,318:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,320:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:10:25,320:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,320:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,320:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:25,320:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:25,321:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,321:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,489:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,490:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.
2024-08-31 18:10:25,491:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,491:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,491:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:10:25,491:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:25,491:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,491:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,606:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,608:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.
2024-08-31 18:10:25,608:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,608:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,608:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:10:25,608:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:25,608:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,609:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,704:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,705:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.
2024-08-31 18:10:25,705:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,705:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,706:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:10:25,706:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:25,706:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,706:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,797:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,799:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.
2024-08-31 18:10:25,800:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,800:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,800:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:25,800:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:25,800:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,800:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,891:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,891:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:10:25,891:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,891:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,891:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:25,891:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:25,892:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,892:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:25,978:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:25,979:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.
2024-08-31 18:10:25,979:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:25,979:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:25,979:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:25,979:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:25,979:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:25,979:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,066:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,067:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:10:26,067:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,067:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,067:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:26,067:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:26,067:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,067:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,150:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,151:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000083 seconds.
2024-08-31 18:10:26,151:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,151:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,151:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:26,151:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:26,151:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,151:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,234:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,235:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000081 seconds.
2024-08-31 18:10:26,235:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,235:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,235:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:10:26,235:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:26,235:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,235:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,314:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,315:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:10:26,315:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,315:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,315:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:26,315:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:26,315:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,315:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,391:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,392:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:10:26,393:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,393:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,393:INFO:[LightGBM] [Info] Total Bins 412
2024-08-31 18:10:26,393:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:26,393:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,393:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,473:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,474:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.
2024-08-31 18:10:26,474:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,474:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,474:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:26,474:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:26,474:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,474:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.
2024-08-31 18:10:26,535:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,535:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 1
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,535:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,611:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,612:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.
2024-08-31 18:10:26,613:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,613:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,613:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:26,613:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:26,613:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,613:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,719:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,721:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000614 seconds.
2024-08-31 18:10:26,721:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,721:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,721:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:26,722:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:26,722:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,722:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,878:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,879:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.
2024-08-31 18:10:26,879:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,879:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,879:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:26,879:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:26,880:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,880:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:26,977:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:26,978:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.
2024-08-31 18:10:26,978:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:26,978:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:26,978:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:26,978:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:26,978:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:26,978:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,074:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,076:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.
2024-08-31 18:10:27,076:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,076:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,076:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:27,076:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:27,076:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,076:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,171:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,172:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.
2024-08-31 18:10:27,172:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,172:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,174:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:27,174:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:27,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,174:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,278:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,279:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000347 seconds.
2024-08-31 18:10:27,279:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,279:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,279:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:27,279:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:27,280:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,280:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,367:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,368:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.
2024-08-31 18:10:27,368:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,368:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,368:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:27,369:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:27,369:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,369:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,455:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,456:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.
2024-08-31 18:10:27,456:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,456:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,456:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:10:27,456:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:27,457:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,457:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,541:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,542:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.
2024-08-31 18:10:27,542:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,542:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,542:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:27,542:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:27,542:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,543:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,617:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,618:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.
2024-08-31 18:10:27,618:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,618:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,618:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:10:27,618:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:27,618:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,618:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,726:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,726:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000071 seconds.
2024-08-31 18:10:27,727:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,727:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,727:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:27,727:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:27,727:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,727:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,798:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,798:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.
2024-08-31 18:10:27,798:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,798:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,798:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:27,798:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:27,799:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,799:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:27,868:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,868:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.
2024-08-31 18:10:27,925:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,925:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 1
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,925:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:27,994:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:27,995:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000412 seconds.
2024-08-31 18:10:27,995:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:27,995:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:27,995:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:10:27,995:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:27,995:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:27,997:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,091:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,093:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.
2024-08-31 18:10:28,093:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,093:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,093:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:10:28,093:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:28,093:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,093:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,217:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,219:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.
2024-08-31 18:10:28,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,219:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,219:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:10:28,219:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:28,219:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,219:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,317:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,319:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.
2024-08-31 18:10:28,319:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,319:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,319:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:10:28,319:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:28,320:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,320:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,428:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,430:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.
2024-08-31 18:10:28,430:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,430:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,430:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:28,430:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:28,431:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,431:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,531:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,533:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.
2024-08-31 18:10:28,533:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,533:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,533:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:28,533:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:28,533:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,533:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,618:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,619:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000319 seconds.
2024-08-31 18:10:28,619:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,619:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,619:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:28,619:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:28,620:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,620:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,728:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,729:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000136 seconds.
2024-08-31 18:10:28,729:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,729:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,729:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:28,729:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:28,730:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,730:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,852:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.
2024-08-31 18:10:28,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,853:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:10:28,853:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:28,853:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,854:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:28,950:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:28,951:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.
2024-08-31 18:10:28,951:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:28,951:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:28,951:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:28,951:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:28,951:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:28,951:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000075 seconds.
2024-08-31 18:10:29,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,031:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,123:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,124:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000071 seconds.
2024-08-31 18:10:29,124:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,124:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,124:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:10:29,124:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:29,124:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,124:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,193:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,194:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.
2024-08-31 18:10:29,194:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,194:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,194:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:29,194:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:29,195:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,195:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.
2024-08-31 18:10:29,263:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,263:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,263:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,327:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,327:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.
2024-08-31 18:10:29,327:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,327:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,327:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:29,327:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 1
2024-08-31 18:10:29,327:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,328:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,398:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,400:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.
2024-08-31 18:10:29,400:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,400:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,400:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:29,400:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:29,401:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,401:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,492:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,493:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.
2024-08-31 18:10:29,494:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,494:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,494:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:29,494:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:29,494:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,494:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,619:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,621:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000686 seconds.
2024-08-31 18:10:29,621:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,621:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,621:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:10:29,621:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:29,621:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,621:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,713:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,715:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.
2024-08-31 18:10:29,715:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,715:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,715:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:29,715:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:29,715:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,715:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,810:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,811:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.
2024-08-31 18:10:29,811:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,811:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,811:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:29,811:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:29,811:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,811:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:29,924:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:29,926:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.
2024-08-31 18:10:29,926:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:29,926:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:29,926:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:29,926:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:29,926:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:29,926:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,010:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,012:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000313 seconds.
2024-08-31 18:10:30,012:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,012:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,012:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:10:30,012:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:30,012:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,012:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,103:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,104:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:10:30,104:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,104:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,104:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:30,104:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:30,104:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,104:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,185:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,185:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.
2024-08-31 18:10:30,185:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,185:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,185:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:10:30,185:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:30,187:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,187:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,267:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,268:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:10:30,268:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,268:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,268:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:30,268:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:30,268:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,268:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.
2024-08-31 18:10:30,349:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,349:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] Total Bins 487
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,349:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,450:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,450:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.
2024-08-31 18:10:30,450:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,450:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,450:INFO:[LightGBM] [Info] Total Bins 428
2024-08-31 18:10:30,451:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:30,451:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,451:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,553:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,554:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.
2024-08-31 18:10:30,554:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,554:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,554:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:10:30,554:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:30,554:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,554:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,622:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,623:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.
2024-08-31 18:10:30,623:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:30,623:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:30,623:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:30,623:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,623:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.
2024-08-31 18:10:30,684:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,684:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 1
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:30,684:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:30,764:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:30,765:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.
2024-08-31 18:10:30,765:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,765:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,765:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:30,765:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:30,765:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:30,765:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:30,861:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:30,862:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.
2024-08-31 18:10:30,862:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,862:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,862:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:30,862:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:30,863:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:30,863:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:30,952:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:30,953:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.
2024-08-31 18:10:30,954:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:30,954:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:30,954:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:30,954:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:30,954:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:30,954:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,043:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,045:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.
2024-08-31 18:10:31,045:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,045:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,045:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:31,046:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:31,046:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,046:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,143:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,144:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.
2024-08-31 18:10:31,144:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,144:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,144:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:10:31,144:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:31,144:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,144:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,246:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,247:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.
2024-08-31 18:10:31,247:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,247:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,247:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:10:31,247:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:31,248:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,248:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,372:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,374:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.
2024-08-31 18:10:31,374:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,374:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,374:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:31,374:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:31,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,374:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,527:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,528:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.
2024-08-31 18:10:31,528:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,528:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,528:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:10:31,528:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:31,528:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,528:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.
2024-08-31 18:10:31,617:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,617:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,617:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,699:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,699:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.
2024-08-31 18:10:31,699:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,699:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,699:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:10:31,699:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:31,699:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,700:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,775:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,776:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:31,776:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,776:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,776:INFO:[LightGBM] [Info] Total Bins 504
2024-08-31 18:10:31,776:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:31,776:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,776:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,854:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,854:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.
2024-08-31 18:10:31,854:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,854:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,854:INFO:[LightGBM] [Info] Total Bins 489
2024-08-31 18:10:31,854:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:31,855:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,855:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,924:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,925:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000083 seconds.
2024-08-31 18:10:31,925:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,925:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,925:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:10:31,925:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:31,925:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,925:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.
2024-08-31 18:10:31,991:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:31,991:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:31,991:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,073:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,074:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000110 seconds.
2024-08-31 18:10:32,074:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:32,074:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:32,074:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:32,074:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,074:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,176:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,178:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.
2024-08-31 18:10:32,178:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,178:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,178:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:32,178:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:32,178:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,178:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,275:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,277:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.
2024-08-31 18:10:32,277:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,277:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,277:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:32,277:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:32,277:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,277:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,373:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,374:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.
2024-08-31 18:10:32,374:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,374:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,374:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:32,374:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:32,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,375:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,459:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,461:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.
2024-08-31 18:10:32,461:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,461:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,461:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:10:32,461:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:32,461:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,461:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,545:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,546:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.
2024-08-31 18:10:32,546:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,546:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,546:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:32,546:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:32,546:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,546:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,629:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,630:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:10:32,631:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,631:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,631:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:32,631:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:32,631:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,631:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,714:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,715:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.
2024-08-31 18:10:32,715:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,715:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,715:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:32,715:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:32,715:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,715:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,796:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,796:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:10:32,796:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,796:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,796:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:32,796:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:32,797:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,797:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,878:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,879:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.
2024-08-31 18:10:32,879:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,879:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,879:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:10:32,879:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:32,879:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,879:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:32,960:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:32,960:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000089 seconds.
2024-08-31 18:10:32,960:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:32,960:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:32,961:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:32,961:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:32,961:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:32,961:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,036:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,036:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000075 seconds.
2024-08-31 18:10:33,037:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,037:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,037:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:10:33,037:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:33,037:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,037:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000084 seconds.
2024-08-31 18:10:33,113:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,113:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,113:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,185:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,187:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:33,187:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,187:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,187:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:10:33,187:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:33,187:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,188:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,258:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,258:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000156 seconds.
2024-08-31 18:10:33,258:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:33,258:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:33,259:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:33,259:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,259:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000099 seconds.
2024-08-31 18:10:33,320:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,320:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,395:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,396:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.
2024-08-31 18:10:33,396:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,396:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,397:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:33,397:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:33,397:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,397:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,494:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,495:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.
2024-08-31 18:10:33,496:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,496:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,496:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:33,496:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:33,497:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,497:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,591:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,593:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.
2024-08-31 18:10:33,593:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,593:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,593:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:33,593:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:33,594:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,594:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,718:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,719:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.
2024-08-31 18:10:33,719:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,719:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,719:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:33,719:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:33,720:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,720:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,810:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,812:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.
2024-08-31 18:10:33,812:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,812:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,812:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:33,812:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:33,812:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,812:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:33,896:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:33,897:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.
2024-08-31 18:10:33,898:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:33,898:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:33,898:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:33,898:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:33,898:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:33,898:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,000:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,001:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000318 seconds.
2024-08-31 18:10:34,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,002:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:10:34,002:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:34,002:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,002:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,082:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,082:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000088 seconds.
2024-08-31 18:10:34,082:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,082:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,082:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:34,082:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:34,083:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,083:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,194:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,195:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.
2024-08-31 18:10:34,195:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,195:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,195:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:10:34,195:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:34,195:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,195:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,340:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,342:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.
2024-08-31 18:10:34,342:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,342:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,342:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:34,342:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:34,342:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,343:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000087 seconds.
2024-08-31 18:10:34,460:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,460:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,460:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,539:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,540:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.
2024-08-31 18:10:34,540:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,540:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,540:INFO:[LightGBM] [Info] Total Bins 420
2024-08-31 18:10:34,540:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:34,540:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,540:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,626:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,626:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000088 seconds.
2024-08-31 18:10:34,626:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,627:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,627:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:34,627:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:34,627:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,627:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,712:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,712:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000111 seconds.
2024-08-31 18:10:34,712:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:34,713:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:34,713:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:34,713:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,713:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.
2024-08-31 18:10:34,775:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,775:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,775:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,852:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.
2024-08-31 18:10:34,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,853:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:34,853:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:34,854:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,854:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:34,947:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:34,949:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.
2024-08-31 18:10:34,949:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:34,949:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:34,949:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:34,949:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:34,949:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:34,950:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,039:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,040:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.
2024-08-31 18:10:35,041:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,041:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,041:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:10:35,041:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:35,041:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,041:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,129:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,131:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.
2024-08-31 18:10:35,131:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,131:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,131:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:10:35,131:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:35,131:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,131:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,221:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,224:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000798 seconds.
2024-08-31 18:10:35,224:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,224:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,224:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:10:35,224:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:35,224:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,224:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,332:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,334:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000493 seconds.
2024-08-31 18:10:35,334:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,334:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,334:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:35,334:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:35,334:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,334:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,422:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,423:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000400 seconds.
2024-08-31 18:10:35,423:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,423:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,423:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:10:35,423:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:35,423:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,423:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,507:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,507:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
2024-08-31 18:10:35,508:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,508:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,508:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:35,508:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:35,508:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,508:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,589:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,590:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.
2024-08-31 18:10:35,590:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,590:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,590:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:35,590:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:35,590:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,590:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,669:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,670:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000073 seconds.
2024-08-31 18:10:35,670:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,670:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,670:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:35,670:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:35,670:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,670:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,744:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,744:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:10:35,744:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,744:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,745:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:10:35,745:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:35,745:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,745:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,818:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,818:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000079 seconds.
2024-08-31 18:10:35,818:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,818:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,818:INFO:[LightGBM] [Info] Total Bins 491
2024-08-31 18:10:35,818:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:35,819:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,819:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,890:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,891:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000083 seconds.
2024-08-31 18:10:35,891:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,891:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,892:INFO:[LightGBM] [Info] Total Bins 411
2024-08-31 18:10:35,892:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:35,892:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,892:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000070 seconds.
2024-08-31 18:10:35,955:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:35,955:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:35,955:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,015:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,015:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.
2024-08-31 18:10:36,015:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,015:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,015:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:36,015:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:36,017:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,017:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,088:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,089:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:10:36,089:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,089:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,089:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:10:36,089:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:36,090:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,090:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,185:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,186:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.
2024-08-31 18:10:36,186:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,187:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,187:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:36,187:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:36,187:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,187:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,283:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,284:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.
2024-08-31 18:10:36,285:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,285:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:36,285:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:36,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,285:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,378:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,379:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000390 seconds.
2024-08-31 18:10:36,379:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,379:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,379:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:10:36,379:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:36,380:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,380:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,464:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,465:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.
2024-08-31 18:10:36,467:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,467:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,467:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:36,467:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:36,467:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,467:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,568:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,569:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.
2024-08-31 18:10:36,569:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,569:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,569:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:36,569:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:36,570:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,570:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,659:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,661:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000283 seconds.
2024-08-31 18:10:36,661:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,662:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,662:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:36,662:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:36,662:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,662:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,755:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,755:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.
2024-08-31 18:10:36,755:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,755:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,755:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:36,755:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:36,756:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,756:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,853:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,854:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.
2024-08-31 18:10:36,854:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:36,854:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:10:36,854:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:36,854:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,854:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:36,976:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:36,977:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000079 seconds.
2024-08-31 18:10:36,977:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:36,977:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:36,977:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:36,977:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:36,977:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:36,977:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,053:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,053:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
2024-08-31 18:10:37,053:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,053:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,053:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:10:37,053:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:37,054:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,054:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,125:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,125:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:10:37,125:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,125:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,125:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:10:37,125:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:37,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,126:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,196:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,197:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:10:37,197:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,197:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,198:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:10:37,198:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:37,198:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,198:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,264:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,265:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.
2024-08-31 18:10:37,265:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:37,265:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:37,265:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:37,265:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,265:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,324:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,324:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.
2024-08-31 18:10:37,324:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,324:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,324:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:37,324:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:37,325:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,325:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,397:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,398:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.
2024-08-31 18:10:37,398:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,398:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,398:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:37,398:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:37,399:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,399:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,495:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,497:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.
2024-08-31 18:10:37,497:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,497:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,497:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:37,497:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:37,497:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,497:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,602:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,604:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.
2024-08-31 18:10:37,604:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,604:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,604:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:37,605:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:37,605:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,605:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,695:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,697:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000534 seconds.
2024-08-31 18:10:37,697:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,697:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,697:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:10:37,697:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:37,698:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,698:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,789:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,790:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.
2024-08-31 18:10:37,790:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,790:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,791:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:10:37,791:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:37,791:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,791:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,877:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,877:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.
2024-08-31 18:10:37,878:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,878:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,878:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:37,878:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:37,878:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,878:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:37,962:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:37,963:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000366 seconds.
2024-08-31 18:10:37,963:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:37,963:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:37,963:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:10:37,964:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:37,964:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:37,964:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,047:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,047:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.
2024-08-31 18:10:38,047:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,048:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,048:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:38,048:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:38,048:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,048:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,130:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,131:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000081 seconds.
2024-08-31 18:10:38,131:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,131:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,131:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:38,131:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:38,131:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,131:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000090 seconds.
2024-08-31 18:10:38,211:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,211:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,211:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,291:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,292:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:10:38,292:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,292:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:10:38,292:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:38,292:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,292:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,380:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,381:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:10:38,381:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,381:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,381:INFO:[LightGBM] [Info] Total Bins 421
2024-08-31 18:10:38,381:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:38,382:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,382:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,501:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,501:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.
2024-08-31 18:10:38,501:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:38,501:INFO:[LightGBM] [Info] Total Bins 406
2024-08-31 18:10:38,501:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:38,502:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,502:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,591:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,591:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000112 seconds.
2024-08-31 18:10:38,591:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:38,591:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:38,592:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:38,592:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,592:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.
2024-08-31 18:10:38,655:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,655:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] Total Bins 255
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 1
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:38,655:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:38,767:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:38,769:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000809 seconds.
2024-08-31 18:10:38,769:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,769:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,769:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:38,769:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:38,770:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:38,770:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:38,901:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:38,903:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000402 seconds.
2024-08-31 18:10:38,903:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,903:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,903:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:38,903:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:38,903:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:38,903:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:38,996:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:38,997:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.
2024-08-31 18:10:38,997:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:38,997:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:38,998:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:10:38,998:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:38,998:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:38,998:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,105:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,107:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:10:39,107:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,108:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,108:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:10:39,108:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:39,108:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,108:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,215:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,217:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.
2024-08-31 18:10:39,217:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,217:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,217:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:10:39,217:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:39,217:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,217:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,322:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,323:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.
2024-08-31 18:10:39,323:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,323:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,323:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:39,324:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:39,324:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,324:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,415:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,417:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000218 seconds.
2024-08-31 18:10:39,417:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,417:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,417:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:39,418:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:39,418:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,418:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,503:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.
2024-08-31 18:10:39,505:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,505:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,505:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:39,505:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:39,505:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,505:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,594:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,594:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:39,594:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,594:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,594:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:39,594:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:39,595:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,595:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,675:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,675:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.
2024-08-31 18:10:39,675:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,675:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,675:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:39,675:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:39,676:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,676:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,751:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,752:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.
2024-08-31 18:10:39,752:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:39,752:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:10:39,752:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:39,752:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,752:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,821:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,822:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000072 seconds.
2024-08-31 18:10:39,822:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,822:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,822:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:39,822:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:39,822:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,822:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,905:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,906:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.
2024-08-31 18:10:39,906:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,906:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,906:INFO:[LightGBM] [Info] Total Bins 412
2024-08-31 18:10:39,906:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:39,906:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,906:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.
2024-08-31 18:10:39,991:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:39,991:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:39,991:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,087:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,090:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.
2024-08-31 18:10:40,091:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,091:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,091:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:40,091:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:40,091:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,091:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,233:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,235:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.
2024-08-31 18:10:40,235:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,235:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,235:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:40,235:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:40,235:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,235:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,342:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,343:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.
2024-08-31 18:10:40,343:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,343:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,344:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:40,344:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:40,344:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,344:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,452:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,454:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000422 seconds.
2024-08-31 18:10:40,454:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,454:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,454:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:40,454:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:40,454:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,454:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,579:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,580:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.
2024-08-31 18:10:40,580:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,580:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,580:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:40,580:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:40,581:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,581:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,693:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,694:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.
2024-08-31 18:10:40,694:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,694:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,695:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:40,695:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:40,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,695:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,780:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,782:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.
2024-08-31 18:10:40,782:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,782:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,782:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:40,783:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:40,783:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,783:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,869:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,869:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000136 seconds.
2024-08-31 18:10:40,869:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,869:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,869:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:40,869:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:40,869:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,870:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:40,955:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:40,956:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000083 seconds.
2024-08-31 18:10:40,956:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:40,956:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:40,956:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:10:40,956:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:40,956:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:40,956:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,064:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,064:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:41,064:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,064:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,064:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:41,065:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:41,065:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,065:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,142:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,142:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.
2024-08-31 18:10:41,143:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,143:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,143:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:10:41,143:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:41,143:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,143:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,230:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,231:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.
2024-08-31 18:10:41,231:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,231:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,231:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:41,231:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:41,231:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,231:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,347:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,348:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000230 seconds.
2024-08-31 18:10:41,348:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,349:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,349:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:41,349:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:41,349:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,349:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,453:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,454:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:10:41,454:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:41,454:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:41,454:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:41,454:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,454:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,539:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,541:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.
2024-08-31 18:10:41,541:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,541:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,541:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:10:41,542:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:41,542:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,542:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,730:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,733:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.
2024-08-31 18:10:41,733:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,734:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,734:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:10:41,734:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:41,736:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,736:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:41,884:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:41,885:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:10:41,885:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:41,885:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:41,885:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:10:41,885:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:41,887:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:41,887:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,007:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,009:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.
2024-08-31 18:10:42,009:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,009:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,009:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:10:42,009:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:42,010:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,010:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,125:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,127:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000653 seconds.
2024-08-31 18:10:42,128:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,128:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,128:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:42,128:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:42,128:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,128:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,224:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,226:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.
2024-08-31 18:10:42,226:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,226:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,226:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:42,226:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:42,226:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,227:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,333:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,334:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.
2024-08-31 18:10:42,334:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,334:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,334:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:42,334:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:42,334:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,334:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,429:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,430:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:10:42,430:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,430:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,430:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:42,430:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:42,430:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,430:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,519:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,519:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.
2024-08-31 18:10:42,520:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,520:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,520:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:10:42,520:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:42,520:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,520:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,608:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,609:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.
2024-08-31 18:10:42,609:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,609:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,609:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:42,609:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:42,609:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,610:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,695:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,696:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.
2024-08-31 18:10:42,696:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,696:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,696:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:10:42,696:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:42,696:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,696:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,772:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,772:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000091 seconds.
2024-08-31 18:10:42,772:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,773:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,773:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:10:42,773:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:42,773:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,773:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,853:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.
2024-08-31 18:10:42,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,853:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:42,853:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:42,854:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,854:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:42,929:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:42,929:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000055 seconds.
2024-08-31 18:10:42,929:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:42,929:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:42,929:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:42,930:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:42,930:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:42,930:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,019:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,021:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.
2024-08-31 18:10:43,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,022:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,022:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:43,022:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:43,022:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,022:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,230:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,232:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000763 seconds.
2024-08-31 18:10:43,232:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,232:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,233:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:43,233:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:43,233:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,233:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,357:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,358:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.
2024-08-31 18:10:43,358:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,358:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,359:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:10:43,359:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:43,359:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,360:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,499:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,501:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.
2024-08-31 18:10:43,501:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,501:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,501:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:43,501:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:43,501:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,501:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,602:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,603:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.
2024-08-31 18:10:43,603:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,603:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,604:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:43,604:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:43,604:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,604:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,722:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,724:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.
2024-08-31 18:10:43,724:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,724:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,724:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:43,724:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:43,724:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,724:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:43,856:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:43,857:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000681 seconds.
2024-08-31 18:10:43,858:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:43,858:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:43,858:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:10:43,858:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:43,858:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:43,858:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,012:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,014:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000152 seconds.
2024-08-31 18:10:44,014:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,014:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,014:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:44,014:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:44,014:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,014:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,148:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,149:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000944 seconds.
2024-08-31 18:10:44,149:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:44,149:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:10:44,149:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:44,150:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,150:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,292:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,294:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000501 seconds.
2024-08-31 18:10:44,294:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:44,294:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:44,294:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:44,295:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,295:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,420:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,420:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:10:44,420:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,420:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,420:INFO:[LightGBM] [Info] Total Bins 487
2024-08-31 18:10:44,421:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:44,421:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,421:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,510:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,511:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:10:44,511:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,511:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,511:INFO:[LightGBM] [Info] Total Bins 428
2024-08-31 18:10:44,511:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:44,511:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,511:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,588:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,589:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.
2024-08-31 18:10:44,589:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,589:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,589:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:10:44,589:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:44,589:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,589:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,666:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:44,666:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000164 seconds.
2024-08-31 18:10:44,666:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:44,666:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:44,666:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 2
2024-08-31 18:10:44,666:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:44,667:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:44,741:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:44,743:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.
2024-08-31 18:10:44,743:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,743:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,743:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:44,743:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:44,743:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:44,743:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:44,847:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:44,849:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.
2024-08-31 18:10:44,849:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,849:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,849:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:44,849:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:44,850:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:44,850:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:44,947:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:44,949:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.
2024-08-31 18:10:44,949:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:44,949:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:44,949:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:44,949:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:44,949:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:44,950:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,085:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,087:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000667 seconds.
2024-08-31 18:10:45,087:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,087:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,087:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:45,088:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:45,088:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,088:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,222:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,225:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.
2024-08-31 18:10:45,225:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,225:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,225:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:10:45,225:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:45,225:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,225:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,361:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,363:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:10:45,363:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,363:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,363:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:10:45,363:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:45,363:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,363:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,497:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,499:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.
2024-08-31 18:10:45,499:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,499:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,499:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:45,499:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:45,499:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,499:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,597:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,597:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.
2024-08-31 18:10:45,597:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,597:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,597:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:10:45,597:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:45,597:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,598:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,696:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,696:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:10:45,696:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,696:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,696:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:10:45,696:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:45,697:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,697:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,788:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,788:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.
2024-08-31 18:10:45,788:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,788:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,788:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:10:45,789:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:45,789:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,789:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,879:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,880:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:10:45,880:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,880:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,880:INFO:[LightGBM] [Info] Total Bins 504
2024-08-31 18:10:45,880:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:45,881:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,881:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:45,984:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:45,984:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.
2024-08-31 18:10:45,984:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:45,984:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:45,984:INFO:[LightGBM] [Info] Total Bins 489
2024-08-31 18:10:45,984:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:45,985:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:45,985:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,061:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,062:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.
2024-08-31 18:10:46,062:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,062:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,062:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:10:46,062:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:46,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,062:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.
2024-08-31 18:10:46,132:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,132:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,132:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,212:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,215:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000802 seconds.
2024-08-31 18:10:46,215:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,216:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,216:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:46,216:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:46,216:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,216:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,318:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,320:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.
2024-08-31 18:10:46,320:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,320:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,320:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:46,320:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:46,320:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,320:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,423:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,425:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.
2024-08-31 18:10:46,425:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,425:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,425:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:46,425:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:46,425:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,425:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,525:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,527:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.
2024-08-31 18:10:46,528:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,528:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,528:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:10:46,528:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:46,528:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,528:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,621:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,623:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.
2024-08-31 18:10:46,623:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,623:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,623:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:46,623:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:46,623:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,623:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,747:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,749:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.
2024-08-31 18:10:46,749:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,749:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,749:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:46,749:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:46,750:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,750:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,848:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,849:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000186 seconds.
2024-08-31 18:10:46,849:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,849:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,849:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:46,849:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:46,849:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,849:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:46,938:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:46,938:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:10:46,939:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:46,939:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:46,939:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:46,939:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:46,939:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:46,939:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,028:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,029:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.
2024-08-31 18:10:47,029:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,029:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,029:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:10:47,029:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:47,029:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,029:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,118:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,118:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:10:47,119:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,119:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,119:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:47,119:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:47,119:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,119:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,202:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,202:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:10:47,202:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,202:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,202:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:10:47,202:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:47,203:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,203:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:10:47,285:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,285:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,366:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,367:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:10:47,367:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,367:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,367:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:10:47,367:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:47,367:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,367:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,445:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,446:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.
2024-08-31 18:10:47,446:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,446:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,446:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:47,446:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:47,446:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,446:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,527:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,528:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.
2024-08-31 18:10:47,528:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,528:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,528:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:47,528:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:47,529:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,529:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,683:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,685:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.
2024-08-31 18:10:47,685:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,685:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,685:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:47,685:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:47,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,685:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,836:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,837:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.
2024-08-31 18:10:47,838:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,838:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,838:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:47,838:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:47,838:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,838:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:47,944:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:47,945:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:10:47,945:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:47,945:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:47,947:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:47,947:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:47,947:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:47,947:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,041:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,043:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.
2024-08-31 18:10:48,043:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,043:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,043:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:48,043:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:48,044:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,044:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,135:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,137:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.
2024-08-31 18:10:48,137:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,137:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,137:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:48,137:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:48,137:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,137:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,242:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,244:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.
2024-08-31 18:10:48,244:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,244:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,244:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:10:48,244:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:48,245:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,245:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.
2024-08-31 18:10:48,367:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,367:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,367:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,487:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:10:48,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,488:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:10:48,488:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:48,488:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,488:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,585:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,586:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:10:48,586:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,586:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,586:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:48,586:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:48,586:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,586:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,668:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,669:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.
2024-08-31 18:10:48,669:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,669:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,669:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:10:48,669:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:48,669:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,669:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,751:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,752:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.
2024-08-31 18:10:48,752:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,752:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,752:INFO:[LightGBM] [Info] Total Bins 420
2024-08-31 18:10:48,752:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:48,752:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,752:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,832:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,833:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000074 seconds.
2024-08-31 18:10:48,833:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,833:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,833:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:48,833:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:48,833:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,833:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.
2024-08-31 18:10:48,927:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:48,927:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:48,927:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,014:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,016:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:10:49,016:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,016:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,016:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:49,016:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:49,016:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,016:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,132:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,134:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.
2024-08-31 18:10:49,135:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,135:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,135:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:49,135:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:49,135:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,135:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,235:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,238:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.
2024-08-31 18:10:49,238:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,238:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,238:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:10:49,238:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:49,238:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,238:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,357:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,359:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.
2024-08-31 18:10:49,359:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,359:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,359:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:10:49,359:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:49,359:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,359:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,466:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,468:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:10:49,468:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,468:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,469:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:10:49,469:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:49,469:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,469:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,573:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,574:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.
2024-08-31 18:10:49,574:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,575:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,575:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:49,575:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:49,575:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,575:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,677:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,679:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.
2024-08-31 18:10:49,679:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,679:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,679:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:10:49,679:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:49,680:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,680:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.
2024-08-31 18:10:49,785:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:49,785:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,785:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:49,903:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:49,903:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000328 seconds.
2024-08-31 18:10:49,903:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:49,904:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:49,904:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:49,904:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:49,904:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,010:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,010:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000121 seconds.
2024-08-31 18:10:50,010:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,010:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,010:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:50,010:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:50,011:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,011:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,106:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,107:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
2024-08-31 18:10:50,107:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,107:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,107:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:10:50,107:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:50,107:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,107:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,206:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,206:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.
2024-08-31 18:10:50,206:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,206:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,206:INFO:[LightGBM] [Info] Total Bins 491
2024-08-31 18:10:50,206:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:50,207:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,207:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,306:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,308:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000732 seconds.
2024-08-31 18:10:50,308:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:50,308:INFO:[LightGBM] [Info] Total Bins 411
2024-08-31 18:10:50,308:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:50,308:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,308:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,398:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,399:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.
2024-08-31 18:10:50,399:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:50,399:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:50,399:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:50,399:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,399:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,502:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,504:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.
2024-08-31 18:10:50,504:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,504:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,504:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:10:50,505:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:50,505:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,505:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,633:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,636:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.
2024-08-31 18:10:50,636:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,636:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,636:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:10:50,636:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:50,636:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,637:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,768:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,770:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:10:50,770:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,770:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,770:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:50,770:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:50,770:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,770:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,885:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,888:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000659 seconds.
2024-08-31 18:10:50,888:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,888:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,888:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:10:50,888:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:50,888:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,888:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:50,997:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:50,999:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.
2024-08-31 18:10:50,999:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:50,999:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:50,999:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:50,999:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:50,999:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:50,999:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,107:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,109:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.
2024-08-31 18:10:51,109:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,109:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,109:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:51,109:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:51,109:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,110:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,212:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,214:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.
2024-08-31 18:10:51,214:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,214:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,214:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:51,214:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:51,214:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,214:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000400 seconds.
2024-08-31 18:10:51,317:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,317:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,421:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,421:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000303 seconds.
2024-08-31 18:10:51,422:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:51,422:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:10:51,422:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:51,422:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,422:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,501:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,501:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
2024-08-31 18:10:51,501:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,502:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,502:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:51,502:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:51,502:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,502:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,605:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,605:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.
2024-08-31 18:10:51,605:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,605:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,605:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:10:51,605:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:51,606:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,606:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:10:51,701:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,701:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,701:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,780:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,780:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000084 seconds.
2024-08-31 18:10:51,780:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,780:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,780:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:10:51,781:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:51,781:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,781:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000054 seconds.
2024-08-31 18:10:51,852:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,852:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] Total Bins 325
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,852:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:51,930:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:51,932:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.
2024-08-31 18:10:51,932:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:51,932:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:51,932:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:51,932:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:51,932:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:51,932:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,033:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,035:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.
2024-08-31 18:10:52,035:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,035:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,035:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:52,035:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:52,035:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,035:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,162:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,164:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.
2024-08-31 18:10:52,164:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,164:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,164:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:52,164:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:52,164:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,164:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,259:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,261:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000676 seconds.
2024-08-31 18:10:52,261:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,261:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,261:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:10:52,261:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:52,262:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,262:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,360:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,361:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.
2024-08-31 18:10:52,362:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,362:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,362:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:10:52,362:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:52,362:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,363:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,454:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,455:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.
2024-08-31 18:10:52,455:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,455:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,455:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:52,455:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:52,457:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,457:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,548:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,550:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000543 seconds.
2024-08-31 18:10:52,550:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,550:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,550:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:10:52,550:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:52,550:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,551:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,640:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,640:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.
2024-08-31 18:10:52,640:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,640:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,641:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:52,641:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:10:52,641:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,641:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,727:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,728:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:10:52,728:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,728:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,728:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:52,728:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:10:52,728:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,728:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,814:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,814:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.
2024-08-31 18:10:52,815:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,815:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,815:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:52,815:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:10:52,815:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,815:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,896:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,896:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.
2024-08-31 18:10:52,897:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,897:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,897:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:10:52,897:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:10:52,897:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,897:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:52,978:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:52,979:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
2024-08-31 18:10:52,979:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:52,979:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:52,979:INFO:[LightGBM] [Info] Total Bins 421
2024-08-31 18:10:52,979:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:10:52,979:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:52,979:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:53,055:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:53,057:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000072 seconds.
2024-08-31 18:10:53,057:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,057:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,057:INFO:[LightGBM] [Info] Total Bins 406
2024-08-31 18:10:53,057:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:10:53,057:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:53,057:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:53,127:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:53,127:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.
2024-08-31 18:10:53,127:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,127:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,127:INFO:[LightGBM] [Info] Total Bins 326
2024-08-31 18:10:53,127:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 2
2024-08-31 18:10:53,127:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:53,128:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:53,217:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,219:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.
2024-08-31 18:10:53,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,219:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,219:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:10:53,219:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:53,220:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,220:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:53,338:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.
2024-08-31 18:10:53,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,340:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:10:53,340:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:53,340:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,340:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:53,468:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,470:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.
2024-08-31 18:10:53,470:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,470:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,470:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:10:53,470:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:53,470:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,470:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:53,601:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,604:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000669 seconds.
2024-08-31 18:10:53,604:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,604:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,604:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:10:53,604:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:53,604:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,604:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:53,737:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,739:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.
2024-08-31 18:10:53,739:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,739:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,739:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:10:53,739:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:53,739:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,739:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:53,864:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:53,867:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.
2024-08-31 18:10:53,867:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:53,867:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:53,867:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:10:53,867:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:53,867:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:53,867:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,000:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,000:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000260 seconds.
2024-08-31 18:10:54,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,001:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:54,001:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:54,001:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,001:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,139:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,140:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000199 seconds.
2024-08-31 18:10:54,140:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,140:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,140:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:10:54,141:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:54,141:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,141:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,267:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,268:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.
2024-08-31 18:10:54,268:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:54,268:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:10:54,268:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:54,268:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,268:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,395:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,397:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000309 seconds.
2024-08-31 18:10:54,397:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:54,397:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:10:54,397:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:54,397:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,397:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,504:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,504:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:10:54,504:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,504:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,504:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:10:54,504:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:54,504:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,505:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,613:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,613:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.
2024-08-31 18:10:54,613:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,613:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,613:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:54,614:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:54,614:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,614:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,717:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,718:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.
2024-08-31 18:10:54,718:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,718:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,718:INFO:[LightGBM] [Info] Total Bins 412
2024-08-31 18:10:54,718:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:54,718:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,719:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,833:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,835:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.
2024-08-31 18:10:54,835:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,836:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,836:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:54,836:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:54,836:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,836:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:54,979:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:54,981:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.
2024-08-31 18:10:54,981:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:54,981:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:54,981:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:54,982:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:54,982:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:54,982:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,114:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,116:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.
2024-08-31 18:10:55,116:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,116:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,117:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:55,117:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:55,117:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,117:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,246:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,248:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.
2024-08-31 18:10:55,248:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,248:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,248:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:55,248:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:55,248:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,248:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,386:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,388:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.
2024-08-31 18:10:55,388:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,388:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,388:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:55,388:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:55,388:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,388:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,534:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,536:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.
2024-08-31 18:10:55,536:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,536:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,536:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:55,536:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:55,536:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,537:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,672:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,674:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.
2024-08-31 18:10:55,674:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,674:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,674:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:55,674:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:55,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,675:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,802:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,803:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000540 seconds.
2024-08-31 18:10:55,803:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:55,803:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:55,803:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:55,803:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,804:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:55,918:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:55,919:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.
2024-08-31 18:10:55,919:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:55,919:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:55,919:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:10:55,919:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:55,919:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:55,919:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,048:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,049:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:10:56,049:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,049:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,049:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:56,049:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:56,049:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,050:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:10:56,170:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,170:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,170:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,280:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,281:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000314 seconds.
2024-08-31 18:10:56,281:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:56,281:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:10:56,281:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:56,281:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,281:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,382:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,382:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000085 seconds.
2024-08-31 18:10:56,382:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,382:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,382:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:56,382:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:56,383:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,383:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,513:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,515:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001527 seconds.
2024-08-31 18:10:56,515:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,515:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,515:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:10:56,515:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:56,515:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,517:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,673:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,675:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.
2024-08-31 18:10:56,675:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,675:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,675:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:10:56,675:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:56,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,676:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,810:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,812:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.
2024-08-31 18:10:56,812:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,813:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,813:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:10:56,813:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:56,813:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,813:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:56,943:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:56,944:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000670 seconds.
2024-08-31 18:10:56,945:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:56,945:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:56,945:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:10:56,945:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:56,945:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:56,945:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,072:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,074:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.
2024-08-31 18:10:57,074:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,074:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,074:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:10:57,074:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:57,075:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,075:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,199:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.
2024-08-31 18:10:57,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,201:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,201:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:10:57,201:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:57,202:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,202:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,304:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,305:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.
2024-08-31 18:10:57,305:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,305:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,305:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:10:57,306:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:57,306:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,306:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,398:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,398:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:10:57,398:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,399:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,399:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:10:57,399:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:57,399:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,399:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,488:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.
2024-08-31 18:10:57,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,488:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:10:57,488:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:57,489:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,489:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,572:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,573:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:10:57,573:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,573:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,573:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:10:57,573:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:57,573:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,573:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,659:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,659:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:10:57,659:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,660:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,660:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:10:57,660:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:57,660:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,660:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,740:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,740:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.
2024-08-31 18:10:57,740:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,740:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,741:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:10:57,741:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:57,741:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,741:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,816:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,816:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000082 seconds.
2024-08-31 18:10:57,816:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,816:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,816:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:10:57,817:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:57,817:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,817:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:57,905:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:57,907:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.
2024-08-31 18:10:57,907:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:57,907:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:57,907:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:10:57,907:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:10:57,907:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:57,907:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,007:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,008:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.
2024-08-31 18:10:58,009:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,009:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,009:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:10:58,009:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:10:58,009:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,009:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,119:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,121:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.
2024-08-31 18:10:58,121:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,121:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,121:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:10:58,121:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:10:58,121:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,122:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,254:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,255:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.
2024-08-31 18:10:58,255:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,255:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,255:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:10:58,255:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:10:58,255:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,255:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,355:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,358:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.
2024-08-31 18:10:58,358:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,358:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,358:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:10:58,358:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:10:58,358:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,359:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,452:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,453:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000455 seconds.
2024-08-31 18:10:58,453:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,453:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,455:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:10:58,455:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:10:58,455:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,455:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,547:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,549:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001148 seconds.
2024-08-31 18:10:58,549:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:58,549:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:10:58,550:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:10:58,550:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,550:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,670:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,671:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000377 seconds.
2024-08-31 18:10:58,671:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:58,671:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:10:58,671:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:10:58,671:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,672:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,795:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,795:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.
2024-08-31 18:10:58,795:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:10:58,795:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:10:58,795:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:10:58,797:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,797:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,890:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,890:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
2024-08-31 18:10:58,890:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,890:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,890:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:10:58,890:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:10:58,891:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,891:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:58,988:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:58,989:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:10:58,989:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:58,989:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:58,989:INFO:[LightGBM] [Info] Total Bins 487
2024-08-31 18:10:58,989:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:10:58,989:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:58,989:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:59,068:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:59,068:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:10:59,069:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,069:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,069:INFO:[LightGBM] [Info] Total Bins 428
2024-08-31 18:10:59,069:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:10:59,069:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:59,069:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:59,145:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:10:59,145:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.
2024-08-31 18:10:59,145:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,145:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,145:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:10:59,145:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 3
2024-08-31 18:10:59,147:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:10:59,147:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:10:59,250:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,251:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.
2024-08-31 18:10:59,252:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,252:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,252:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:10:59,252:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:10:59,252:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,252:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,377:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,378:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000664 seconds.
2024-08-31 18:10:59,378:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,379:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,379:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:10:59,379:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:10:59,379:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,379:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,481:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,482:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.
2024-08-31 18:10:59,482:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,482:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,482:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:10:59,482:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:10:59,483:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,483:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,583:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000671 seconds.
2024-08-31 18:10:59,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,585:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:10:59,585:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:10:59,585:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,585:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,697:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,698:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.
2024-08-31 18:10:59,698:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,698:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,699:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:10:59,699:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:10:59,699:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,699:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,804:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,805:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.
2024-08-31 18:10:59,805:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,805:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,805:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:10:59,805:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:10:59,805:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,807:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:10:59,912:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:10:59,913:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.
2024-08-31 18:10:59,914:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:10:59,914:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:10:59,914:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:10:59,914:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:10:59,914:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:10:59,914:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,018:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,019:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.
2024-08-31 18:11:00,019:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,019:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,019:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:11:00,019:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:00,019:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,019:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,111:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,112:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:11:00,112:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,112:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,112:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:11:00,112:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:00,112:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,112:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,223:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,224:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:11:00,224:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,224:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,224:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:11:00,224:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:00,225:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,225:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,319:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,319:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:11:00,319:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,319:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,319:INFO:[LightGBM] [Info] Total Bins 504
2024-08-31 18:11:00,320:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:00,320:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,320:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:11:00,411:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,411:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] Total Bins 489
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,411:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,499:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,500:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.
2024-08-31 18:11:00,500:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,500:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,500:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:11:00,500:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:00,501:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,501:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,586:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,587:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:11:00,587:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,587:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,587:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:00,587:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:00,588:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,588:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,695:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,697:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.
2024-08-31 18:11:00,697:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,697:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,697:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:00,697:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:00,698:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,698:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,840:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,842:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000676 seconds.
2024-08-31 18:11:00,842:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,842:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,842:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:00,842:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:00,842:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,842:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:00,939:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:00,940:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.
2024-08-31 18:11:00,940:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:00,940:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:00,940:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:11:00,940:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:00,941:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:00,941:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,060:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,061:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.
2024-08-31 18:11:01,062:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,062:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,062:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:01,062:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:01,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,062:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,155:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,157:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000478 seconds.
2024-08-31 18:11:01,157:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,157:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,157:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:01,157:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:01,157:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,157:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,280:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,281:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.
2024-08-31 18:11:01,281:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,281:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,281:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:01,281:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:01,281:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,281:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,380:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,381:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.
2024-08-31 18:11:01,381:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,381:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,381:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:01,381:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:01,381:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,381:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,499:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,500:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.
2024-08-31 18:11:01,500:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,500:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,500:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:01,500:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:01,501:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,501:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,587:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,588:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.
2024-08-31 18:11:01,588:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:01,588:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:01,588:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:01,589:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,589:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,683:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,683:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.
2024-08-31 18:11:01,683:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,683:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,683:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:11:01,683:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:01,683:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,684:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,772:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,772:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:01,772:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,772:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,772:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:11:01,773:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:01,773:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,773:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,850:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,851:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000080 seconds.
2024-08-31 18:11:01,851:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:01,851:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:01,851:INFO:[LightGBM] [Info] Total Bins 413
2024-08-31 18:11:01,851:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:01,851:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,851:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:01,932:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:01,935:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001226 seconds.
2024-08-31 18:11:01,935:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:01,935:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:01,935:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:01,935:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:01,935:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,064:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,065:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.
2024-08-31 18:11:02,065:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,065:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,065:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:02,066:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:02,066:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,066:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,175:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,176:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:11:02,176:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,176:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,176:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:02,176:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:02,177:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,177:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,277:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,279:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.
2024-08-31 18:11:02,279:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,279:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,279:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:02,279:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:02,280:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,280:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,377:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,379:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.
2024-08-31 18:11:02,379:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,379:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,379:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:02,379:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:02,379:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,379:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,470:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,471:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.
2024-08-31 18:11:02,471:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,471:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,471:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:02,471:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:02,472:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,472:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,597:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,598:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.
2024-08-31 18:11:02,598:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,598:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,598:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:02,598:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:02,599:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,599:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,699:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,700:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:11:02,700:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,700:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,700:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:02,700:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:02,700:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,700:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,792:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,793:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.
2024-08-31 18:11:02,793:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,793:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,793:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:11:02,793:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:02,794:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,794:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,895:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,896:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
2024-08-31 18:11:02,896:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,896:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,896:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:02,896:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:02,896:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,896:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:02,988:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:02,989:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:11:02,989:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:02,989:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:02,989:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:02,989:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:02,989:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:02,989:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,092:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,093:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:11:03,093:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,093:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,093:INFO:[LightGBM] [Info] Total Bins 420
2024-08-31 18:11:03,093:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:03,093:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,093:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,198:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,199:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000086 seconds.
2024-08-31 18:11:03,199:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,199:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,199:INFO:[LightGBM] [Info] Total Bins 405
2024-08-31 18:11:03,199:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:03,199:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,199:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,297:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,299:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.
2024-08-31 18:11:03,299:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,299:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,299:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:03,299:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:03,299:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,299:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,406:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,408:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.
2024-08-31 18:11:03,408:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,408:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,408:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:03,408:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:03,409:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,409:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,510:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,512:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.
2024-08-31 18:11:03,512:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,512:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,512:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:11:03,512:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:03,513:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,513:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,633:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,635:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.
2024-08-31 18:11:03,637:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,637:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,637:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:03,637:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:03,637:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,637:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,751:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,752:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.
2024-08-31 18:11:03,753:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,753:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,753:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:03,753:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:03,753:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,753:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,853:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,854:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.
2024-08-31 18:11:03,854:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,854:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,854:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:03,854:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:03,854:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,854:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:03,979:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:03,980:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000478 seconds.
2024-08-31 18:11:03,981:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:03,981:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:03,981:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:03,981:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:03,981:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:03,981:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,074:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,075:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
2024-08-31 18:11:04,075:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,075:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,075:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:04,075:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:04,075:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,075:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,181:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,181:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.
2024-08-31 18:11:04,181:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:04,181:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:04,181:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:04,182:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,182:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,284:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,284:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.
2024-08-31 18:11:04,284:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,284:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,284:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:04,285:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:04,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,285:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,428:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,429:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
2024-08-31 18:11:04,429:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,429:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,429:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:11:04,429:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:04,429:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,429:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,519:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,519:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.
2024-08-31 18:11:04,519:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,519:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,519:INFO:[LightGBM] [Info] Total Bins 491
2024-08-31 18:11:04,519:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:04,520:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,520:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,631:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,632:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.
2024-08-31 18:11:04,632:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,632:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,632:INFO:[LightGBM] [Info] Total Bins 411
2024-08-31 18:11:04,633:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:04,633:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,633:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,775:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,778:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:11:04,778:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,778:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,778:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:04,778:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:04,778:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,778:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:04,902:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:04,904:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.
2024-08-31 18:11:04,905:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:04,905:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:04,905:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:04,905:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:04,905:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:04,905:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,046:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,048:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.
2024-08-31 18:11:05,048:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,048:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,048:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:05,048:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:05,049:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,049:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,151:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,154:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.
2024-08-31 18:11:05,154:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,154:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,154:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:05,154:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:05,155:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,155:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,296:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,298:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.
2024-08-31 18:11:05,299:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,299:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,299:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:05,299:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:05,299:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,299:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,423:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,426:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.
2024-08-31 18:11:05,426:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,426:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,426:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:05,426:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:05,426:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,427:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,569:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,571:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000614 seconds.
2024-08-31 18:11:05,571:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,571:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,571:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:05,571:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:05,572:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,572:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,689:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,689:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.
2024-08-31 18:11:05,689:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,689:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,690:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:05,690:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:05,690:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,690:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,779:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,779:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.
2024-08-31 18:11:05,779:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,779:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,779:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:05,780:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:05,780:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,780:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:05,908:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:05,908:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.
2024-08-31 18:11:05,908:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:05,908:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:05,908:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:05,909:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:05,909:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:05,909:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,051:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.
2024-08-31 18:11:06,051:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,051:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,051:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:11:06,051:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:06,052:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,052:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,182:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,182:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.
2024-08-31 18:11:06,182:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,182:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,182:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:11:06,182:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:06,183:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,183:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,284:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,285:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.
2024-08-31 18:11:06,285:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,287:INFO:[LightGBM] [Info] Total Bins 410
2024-08-31 18:11:06,287:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:06,287:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,287:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,409:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,411:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:11:06,411:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,411:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,411:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:06,411:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:06,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,411:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,518:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,520:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.
2024-08-31 18:11:06,520:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,520:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,520:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:06,521:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:06,521:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,521:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,631:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,633:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.
2024-08-31 18:11:06,633:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,633:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,633:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:06,633:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:06,633:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,633:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,730:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,732:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:11:06,732:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,732:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,732:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:06,733:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:06,733:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,733:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,845:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,848:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.
2024-08-31 18:11:06,848:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,848:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,849:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:11:06,849:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:06,849:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,849:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:06,958:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:06,960:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.
2024-08-31 18:11:06,960:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:06,960:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:06,960:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:06,960:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:06,960:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:06,960:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,053:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.
2024-08-31 18:11:07,053:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,053:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,053:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:07,053:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:07,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,053:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,144:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,145:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.
2024-08-31 18:11:07,145:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,145:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,145:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:07,145:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:07,145:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,145:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,232:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,233:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000139 seconds.
2024-08-31 18:11:07,233:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,233:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,233:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:07,233:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:07,233:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,233:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,322:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,323:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:11:07,323:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,323:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,323:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:07,323:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:07,323:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,323:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,405:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,407:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.
2024-08-31 18:11:07,407:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,407:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,407:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:07,407:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:07,407:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,407:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,492:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,493:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000121 seconds.
2024-08-31 18:11:07,493:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,493:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,493:INFO:[LightGBM] [Info] Total Bins 421
2024-08-31 18:11:07,493:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:07,493:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,493:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,573:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:07,573:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000074 seconds.
2024-08-31 18:11:07,573:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,573:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,573:INFO:[LightGBM] [Info] Total Bins 406
2024-08-31 18:11:07,573:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 3
2024-08-31 18:11:07,574:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:07,574:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:07,693:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:07,694:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.
2024-08-31 18:11:07,695:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,695:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,695:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:07,695:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:07,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:07,695:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:07,794:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:07,797:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000655 seconds.
2024-08-31 18:11:07,797:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,797:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,797:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:07,797:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:07,797:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:07,797:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:07,904:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:07,906:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.
2024-08-31 18:11:07,906:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:07,906:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:07,906:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:11:07,906:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:07,906:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:07,906:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,032:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,034:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.
2024-08-31 18:11:08,034:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,034:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,034:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:11:08,035:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:08,035:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,035:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,135:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,136:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.
2024-08-31 18:11:08,136:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,136:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,136:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:08,136:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:08,136:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,136:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,232:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,234:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.
2024-08-31 18:11:08,234:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,234:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,234:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:08,234:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:08,234:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,234:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,333:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,334:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.
2024-08-31 18:11:08,334:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,334:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,334:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:08,334:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:08,334:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,334:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,430:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,430:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.
2024-08-31 18:11:08,431:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,431:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,431:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:08,431:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:08,431:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,431:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,521:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,522:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.
2024-08-31 18:11:08,522:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,522:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,522:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:08,522:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:08,522:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,522:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,609:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,610:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.
2024-08-31 18:11:08,610:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,610:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,610:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:08,610:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:08,610:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,610:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,701:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,701:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:08,702:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,702:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,702:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:11:08,702:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:08,702:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,702:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,819:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,820:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.
2024-08-31 18:11:08,820:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,820:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,820:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:11:08,820:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:11:08,820:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,820:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:08,947:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:08,948:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:11:08,948:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:08,948:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:08,948:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:08,948:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:08,949:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:08,949:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,068:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,070:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.
2024-08-31 18:11:09,070:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,070:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,070:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:09,070:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:09,070:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,070:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,171:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,173:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:11:09,173:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,173:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,173:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:09,173:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:09,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,174:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,302:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,304:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.
2024-08-31 18:11:09,304:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,304:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,304:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:09,304:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:09,304:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,304:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,425:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,427:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.
2024-08-31 18:11:09,427:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,427:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,427:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:09,427:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:09,427:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,427:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,529:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,531:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.
2024-08-31 18:11:09,531:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,531:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,531:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:09,531:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:09,532:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,532:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,653:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,654:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.
2024-08-31 18:11:09,654:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,654:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,654:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:09,654:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:09,655:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,655:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,751:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,752:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.
2024-08-31 18:11:09,752:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,752:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,752:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:09,752:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:09,752:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,753:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,876:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,876:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.
2024-08-31 18:11:09,876:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,876:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,876:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:11:09,876:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:09,877:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,877:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:09,967:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:09,968:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:11:09,968:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:09,968:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:09,968:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:09,968:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:09,968:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:09,968:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,055:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,056:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.
2024-08-31 18:11:10,056:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,056:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,056:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:11:10,056:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:10,056:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,056:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,140:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,140:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.
2024-08-31 18:11:10,140:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,140:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,140:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:11:10,140:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:11:10,141:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,141:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,227:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,229:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.
2024-08-31 18:11:10,229:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,229:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,229:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:10,229:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:10,229:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,230:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,347:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,349:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.
2024-08-31 18:11:10,349:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,349:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,349:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:11:10,349:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:10,350:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,350:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,499:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,501:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:11:10,501:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,501:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,501:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:11:10,501:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:10,502:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,502:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,615:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,618:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.
2024-08-31 18:11:10,618:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,618:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,618:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:10,618:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:10,618:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,618:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,720:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,723:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.
2024-08-31 18:11:10,723:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,723:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,723:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:10,723:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:10,723:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,723:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,838:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,840:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.
2024-08-31 18:11:10,840:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,840:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,840:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:10,840:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:10,840:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,841:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:10,961:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:10,962:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.
2024-08-31 18:11:10,962:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:10,962:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:10,962:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:10,963:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:10,963:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:10,963:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,052:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.
2024-08-31 18:11:11,052:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,052:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,052:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:11,052:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:11,052:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,052:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:11:11,147:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,147:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,147:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,257:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,257:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.
2024-08-31 18:11:11,257:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,257:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,257:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:11,258:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:11,258:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,258:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,368:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,369:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.
2024-08-31 18:11:11,369:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,369:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,369:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:11:11,369:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:11,369:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,369:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,469:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,470:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:11:11,470:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,470:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,470:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:11:11,470:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:11:11,470:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,470:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,567:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,569:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.
2024-08-31 18:11:11,569:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,569:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,569:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:11,569:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:11,569:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,569:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,680:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,682:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.
2024-08-31 18:11:11,682:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,682:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,682:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:11,682:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:11,682:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,682:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,793:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,795:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000672 seconds.
2024-08-31 18:11:11,795:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,795:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,796:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:11:11,796:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:11,796:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,796:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:11,920:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:11,922:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000556 seconds.
2024-08-31 18:11:11,922:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:11,922:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:11,922:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:11,922:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:11,922:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:11,922:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,048:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,050:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.
2024-08-31 18:11:12,051:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,051:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,051:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:12,051:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:12,051:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,051:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,164:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,165:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:11:12,165:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,167:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,167:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:12,167:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:12,167:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,167:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,270:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,271:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000440 seconds.
2024-08-31 18:11:12,271:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,271:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,271:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:12,271:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:12,272:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,272:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,368:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,368:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.
2024-08-31 18:11:12,369:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,369:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,369:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:12,369:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:12,369:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,369:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,457:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,457:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:11:12,457:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,457:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,457:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:12,457:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:12,458:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,458:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,544:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,545:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:11:12,545:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,545:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,545:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:12,545:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:12,545:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,545:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000092 seconds.
2024-08-31 18:11:12,630:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,630:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] Total Bins 487
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,630:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,713:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:12,713:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.
2024-08-31 18:11:12,714:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,714:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,714:INFO:[LightGBM] [Info] Total Bins 428
2024-08-31 18:11:12,714:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 4
2024-08-31 18:11:12,714:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:12,714:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:12,802:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:12,804:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.
2024-08-31 18:11:12,804:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,804:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,804:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:12,804:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:12,805:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:12,805:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:12,934:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:12,936:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.
2024-08-31 18:11:12,937:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:12,937:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:12,937:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:12,937:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:12,937:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:12,937:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,035:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,038:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.
2024-08-31 18:11:13,038:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,038:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,038:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:13,038:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:13,038:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,039:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,134:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,136:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.
2024-08-31 18:11:13,136:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,136:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,136:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:13,136:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:13,136:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,136:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,227:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,228:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:11:13,228:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,229:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,229:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:11:13,229:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:13,229:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,229:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,322:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,324:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.
2024-08-31 18:11:13,324:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,324:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,324:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:11:13,324:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:13,324:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,324:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,415:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,416:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.
2024-08-31 18:11:13,416:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,416:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,417:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:13,417:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:13,417:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,417:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,505:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,506:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:11:13,506:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,506:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,506:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:11:13,506:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:13,506:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,506:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,592:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,592:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:11:13,592:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,592:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,592:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:11:13,592:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:13,593:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,593:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,681:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,682:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:11:13,682:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,682:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,682:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:11:13,682:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:13,682:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,682:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,762:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,762:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:11:13,762:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,762:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,762:INFO:[LightGBM] [Info] Total Bins 504
2024-08-31 18:11:13,762:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:13,763:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,763:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,846:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,846:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.
2024-08-31 18:11:13,846:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,846:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,846:INFO:[LightGBM] [Info] Total Bins 489
2024-08-31 18:11:13,846:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:13,847:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,847:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:13,934:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:13,935:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000556 seconds.
2024-08-31 18:11:13,935:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:13,936:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:13,936:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:13,936:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:13,936:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:13,936:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,040:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,041:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.
2024-08-31 18:11:14,041:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,042:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,042:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:14,042:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:14,042:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,042:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,142:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,143:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.
2024-08-31 18:11:14,143:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,143:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,144:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:14,144:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:14,144:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,144:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,241:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,243:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000895 seconds.
2024-08-31 18:11:14,243:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,243:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,243:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:11:14,244:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:14,244:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,244:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,355:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,357:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.
2024-08-31 18:11:14,358:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,358:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,358:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:14,358:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:14,358:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,358:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,454:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,455:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.
2024-08-31 18:11:14,455:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,455:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,456:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:14,456:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:14,456:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,456:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,572:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,573:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.
2024-08-31 18:11:14,573:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,573:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,573:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:14,574:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:14,574:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,574:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,666:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,666:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.
2024-08-31 18:11:14,666:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:14,666:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:14,666:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:14,667:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,667:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.
2024-08-31 18:11:14,747:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,747:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,828:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,829:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:14,829:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,829:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,829:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:14,829:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:14,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,829:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,909:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,910:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.
2024-08-31 18:11:14,910:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,911:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,911:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:11:14,911:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:14,911:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,911:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:14,991:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:14,992:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.
2024-08-31 18:11:14,992:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:14,992:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:14,992:INFO:[LightGBM] [Info] Total Bins 492
2024-08-31 18:11:14,992:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:14,992:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:14,992:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,080:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,082:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.
2024-08-31 18:11:15,083:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,083:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,083:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:15,083:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:15,083:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,083:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,183:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,185:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.
2024-08-31 18:11:15,185:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,185:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,185:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:15,185:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:15,185:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,185:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,287:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,288:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.
2024-08-31 18:11:15,288:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,288:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,288:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:15,288:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:15,289:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,289:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,390:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,392:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.
2024-08-31 18:11:15,392:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,392:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,392:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:15,392:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:15,392:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,392:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,487:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.
2024-08-31 18:11:15,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,488:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:15,488:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:15,489:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,489:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,590:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,591:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.
2024-08-31 18:11:15,591:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,591:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,592:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:15,592:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:15,592:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,592:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,692:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,693:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.
2024-08-31 18:11:15,693:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,693:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,694:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:15,694:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:15,694:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,694:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,791:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,791:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000143 seconds.
2024-08-31 18:11:15,792:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,792:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,792:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:15,792:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:15,792:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,792:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,888:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,888:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.
2024-08-31 18:11:15,888:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:15,888:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:15,888:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:11:15,888:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:15,889:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,889:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:15,985:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:15,985:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.
2024-08-31 18:11:15,985:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:15,985:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:15,985:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:15,986:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:15,986:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,069:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,069:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:11:16,069:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,069:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,069:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:16,069:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:16,070:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,070:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,180:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,180:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.
2024-08-31 18:11:16,181:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,181:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,181:INFO:[LightGBM] [Info] Total Bins 420
2024-08-31 18:11:16,181:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:16,181:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,181:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,277:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,279:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.
2024-08-31 18:11:16,279:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,279:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,279:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:16,279:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:16,279:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,279:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,390:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,392:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.
2024-08-31 18:11:16,392:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,392:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,393:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:16,393:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:16,393:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,393:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,533:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,534:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.
2024-08-31 18:11:16,534:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,534:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,534:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:11:16,535:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:16,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,535:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,654:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,656:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.
2024-08-31 18:11:16,656:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,656:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,656:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:16,656:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:16,656:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,656:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,754:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,755:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.
2024-08-31 18:11:16,756:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,756:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,756:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:16,756:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:16,756:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,756:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,849:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.
2024-08-31 18:11:16,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,853:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:16,853:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:16,853:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,853:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:16,944:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:16,945:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.
2024-08-31 18:11:16,945:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:16,945:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:16,945:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:16,945:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:16,946:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:16,946:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,034:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,034:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.
2024-08-31 18:11:17,034:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,034:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,034:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:17,035:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:17,035:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,035:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,125:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,126:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:17,126:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,126:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,126:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:17,126:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:17,126:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,126:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,210:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,211:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:17,211:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,211:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,211:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:17,211:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:17,211:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,211:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,292:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,292:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:11:17,292:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,292:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:11:17,292:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:17,293:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,293:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,375:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,375:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:11:17,375:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,375:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,375:INFO:[LightGBM] [Info] Total Bins 491
2024-08-31 18:11:17,375:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:17,375:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,376:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,463:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,465:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.
2024-08-31 18:11:17,465:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,465:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,465:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:17,465:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:17,465:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,465:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,565:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,567:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000675 seconds.
2024-08-31 18:11:17,567:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,567:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,567:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:17,567:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:17,567:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,567:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,667:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,668:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.
2024-08-31 18:11:17,668:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,668:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,668:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:17,669:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:17,669:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,669:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,783:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,785:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.
2024-08-31 18:11:17,785:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,785:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,785:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:17,785:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:17,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,786:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,888:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,890:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.
2024-08-31 18:11:17,890:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,890:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,890:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:17,890:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:17,890:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,890:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:17,983:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:17,984:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.
2024-08-31 18:11:17,984:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:17,984:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:17,984:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:17,984:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:17,984:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:17,984:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,078:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,079:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.
2024-08-31 18:11:18,079:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,079:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,079:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:18,079:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:18,080:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,080:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,168:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,168:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000141 seconds.
2024-08-31 18:11:18,168:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,168:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,168:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:18,169:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:18,169:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,169:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,255:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,256:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
2024-08-31 18:11:18,256:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,256:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,256:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:18,256:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:18,256:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,256:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,346:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,347:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.
2024-08-31 18:11:18,347:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,347:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,347:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:18,347:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:18,347:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,347:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,429:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,429:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:11:18,430:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,430:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,430:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:11:18,430:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:18,430:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,430:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,505:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.
2024-08-31 18:11:18,505:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,505:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,505:INFO:[LightGBM] [Info] Total Bins 490
2024-08-31 18:11:18,505:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:18,507:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,507:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,594:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,596:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.
2024-08-31 18:11:18,596:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,596:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,596:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:18,596:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:18,596:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,596:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,715:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,718:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000668 seconds.
2024-08-31 18:11:18,718:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,718:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,718:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:18,718:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:18,718:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,718:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,820:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,822:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.
2024-08-31 18:11:18,822:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,822:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,822:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:18,822:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:18,822:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,822:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:18,920:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:18,922:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.
2024-08-31 18:11:18,922:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:18,922:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:18,922:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:18,922:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:18,923:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:18,923:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,015:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,017:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000560 seconds.
2024-08-31 18:11:19,017:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,017:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,017:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:11:19,017:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:19,017:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,017:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,113:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,115:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000559 seconds.
2024-08-31 18:11:19,115:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,115:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,115:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:19,115:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:19,116:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,116:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,223:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,224:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.
2024-08-31 18:11:19,224:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,224:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,224:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:19,224:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:19,224:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,224:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,313:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,314:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.
2024-08-31 18:11:19,314:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,314:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,314:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:19,314:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:19,315:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,315:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,428:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,429:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.
2024-08-31 18:11:19,429:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,429:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,429:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:19,429:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:19,430:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,430:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,525:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,526:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:11:19,526:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,526:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,526:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:19,526:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:19,526:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,526:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,607:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,608:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000375 seconds.
2024-08-31 18:11:19,608:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:19,608:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:19,608:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:19,608:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,608:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:11:19,685:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,685:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] Total Bins 421
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 4
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:19,685:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:19,777:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:19,778:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.
2024-08-31 18:11:19,779:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,779:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,779:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:19,779:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:19,779:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:19,779:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:19,879:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:19,881:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.
2024-08-31 18:11:19,881:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,882:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,882:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:19,882:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:19,882:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:19,882:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:19,982:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:19,984:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.
2024-08-31 18:11:19,984:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:19,984:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:19,985:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:11:19,985:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:19,985:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:19,985:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,138:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,139:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.
2024-08-31 18:11:20,139:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,139:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,140:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:11:20,140:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:20,140:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,140:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,238:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,240:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.
2024-08-31 18:11:20,240:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,240:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,240:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:20,240:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:20,240:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,240:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,342:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,343:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.
2024-08-31 18:11:20,343:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,343:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,343:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:20,343:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:20,344:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,344:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,468:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,468:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000179 seconds.
2024-08-31 18:11:20,469:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,469:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,469:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:20,469:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:20,469:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,469:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,671:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,672:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:11:20,672:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,672:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,672:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:20,673:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:20,673:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,673:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,767:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,767:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.
2024-08-31 18:11:20,767:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,767:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,767:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:20,768:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:20,768:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,768:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:20,949:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:20,950:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.
2024-08-31 18:11:20,950:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:20,950:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:20,950:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:20,950:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:20,952:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:20,952:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,106:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,106:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:11:21,107:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,107:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,107:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:11:21,107:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:21,107:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,107:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,255:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,257:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.
2024-08-31 18:11:21,257:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,257:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,257:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:21,257:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:21,258:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,258:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,376:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,377:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.
2024-08-31 18:11:21,377:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,377:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,377:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:21,377:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:21,377:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,377:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,526:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,528:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.
2024-08-31 18:11:21,528:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,528:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,528:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:21,528:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:21,529:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,529:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,673:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,675:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.
2024-08-31 18:11:21,675:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,675:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,676:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:21,676:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:21,676:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,676:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,826:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,829:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.
2024-08-31 18:11:21,829:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,829:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,829:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:21,829:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:21,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,829:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:21,932:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:21,933:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.
2024-08-31 18:11:21,933:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:21,933:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:21,934:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:21,934:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:21,934:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:21,934:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,111:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,112:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.
2024-08-31 18:11:22,112:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,113:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,113:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:22,113:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:22,113:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,113:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,243:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,244:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000171 seconds.
2024-08-31 18:11:22,244:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,244:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,244:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:22,244:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:22,244:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,244:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,440:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,441:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000159 seconds.
2024-08-31 18:11:22,441:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,441:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,441:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:11:22,441:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:22,442:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,442:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,633:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,634:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:11:22,634:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,634:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,634:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:22,634:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:22,635:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,635:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,802:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,803:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.
2024-08-31 18:11:22,803:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,803:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,803:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:11:22,803:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:22,803:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,803:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:22,909:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:22,911:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.
2024-08-31 18:11:22,911:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:22,911:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:22,911:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:22,911:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:22,911:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:22,911:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,027:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,029:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.
2024-08-31 18:11:23,029:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,029:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,029:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:11:23,029:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:23,029:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,030:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,144:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,147:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000682 seconds.
2024-08-31 18:11:23,147:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,147:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,147:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:11:23,147:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:23,147:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,147:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,261:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,263:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:11:23,263:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,263:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,263:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:23,263:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:23,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,263:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,363:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,365:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.
2024-08-31 18:11:23,365:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,365:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,365:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:23,365:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:23,365:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,365:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,459:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,461:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.
2024-08-31 18:11:23,461:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,462:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,462:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:23,462:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:23,462:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,462:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,601:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,603:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.
2024-08-31 18:11:23,603:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,603:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,603:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:23,603:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:23,603:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,603:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,734:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,735:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.
2024-08-31 18:11:23,735:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,735:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,735:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:23,735:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:23,736:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,736:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,833:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,834:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.
2024-08-31 18:11:23,834:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:23,834:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:23,834:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:23,834:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,834:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:23,917:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:23,917:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:11:23,918:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:23,918:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:23,918:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:23,918:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:23,918:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:23,918:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,004:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,005:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.
2024-08-31 18:11:24,005:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,005:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,005:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:11:24,005:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:24,005:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,005:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,098:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,100:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000670 seconds.
2024-08-31 18:11:24,100:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,100:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,100:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:24,101:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:24,101:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,101:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,207:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,209:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.
2024-08-31 18:11:24,209:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,209:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,209:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:24,209:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:24,209:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,210:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,331:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,333:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.
2024-08-31 18:11:24,333:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,333:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,333:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:11:24,333:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:24,333:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,333:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,445:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,448:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.
2024-08-31 18:11:24,448:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,448:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,448:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:24,448:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:24,449:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,449:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,567:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,568:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.
2024-08-31 18:11:24,568:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,568:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,568:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:24,568:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:24,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,568:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,671:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,672:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.
2024-08-31 18:11:24,673:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,673:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,673:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:24,673:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:24,673:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,673:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,801:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,802:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.
2024-08-31 18:11:24,802:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,802:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,802:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:24,802:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:24,802:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,802:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:24,931:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:24,932:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:24,932:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:24,932:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:24,932:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:24,932:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:24,932:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:24,932:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:25,022:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:25,022:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000088 seconds.
2024-08-31 18:11:25,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,022:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,024:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:25,024:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:25,024:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:25,024:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:25,113:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:25,114:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.
2024-08-31 18:11:25,115:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,115:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,115:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:25,115:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:25,115:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:25,115:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000124 seconds.
2024-08-31 18:11:25,225:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,225:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] Total Bins 487
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 5
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:25,225:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:25,334:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:25,336:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.
2024-08-31 18:11:25,336:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,336:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,336:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:25,336:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:25,337:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:25,337:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:25,508:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:25,511:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.
2024-08-31 18:11:25,511:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,511:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,511:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:25,511:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:25,511:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:25,511:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:25,655:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:25,657:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.
2024-08-31 18:11:25,658:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,658:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,658:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:25,658:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:25,658:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:25,658:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:25,835:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:25,837:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.
2024-08-31 18:11:25,837:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:25,837:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:25,837:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:25,838:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:25,838:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:25,838:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:25,997:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,000:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.
2024-08-31 18:11:26,000:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,000:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,000:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:11:26,000:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:26,000:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,001:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,145:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,146:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.
2024-08-31 18:11:26,147:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,147:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,147:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:11:26,148:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:26,148:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,148:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,302:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,303:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:11:26,304:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,304:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,304:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:26,304:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:26,304:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,304:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,460:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,461:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.
2024-08-31 18:11:26,461:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,461:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,461:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:11:26,461:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:26,461:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,461:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,568:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,569:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.
2024-08-31 18:11:26,569:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:26,569:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:11:26,569:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:26,569:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,569:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,689:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,689:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.
2024-08-31 18:11:26,689:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,689:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,689:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:11:26,690:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:26,690:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,690:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:11:26,775:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,775:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] Total Bins 504
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,775:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:26,917:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:26,920:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.
2024-08-31 18:11:26,920:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:26,920:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:26,920:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:26,920:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:26,920:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:26,920:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,052:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,054:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.
2024-08-31 18:11:27,054:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,054:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,054:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:27,055:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:27,055:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,055:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,212:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,214:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.
2024-08-31 18:11:27,214:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,215:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,215:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:27,215:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:27,215:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,215:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,354:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,356:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.
2024-08-31 18:11:27,356:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,356:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,356:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:11:27,356:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:27,356:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,356:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,503:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.
2024-08-31 18:11:27,506:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,506:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,506:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:27,506:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:27,506:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,507:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,693:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,695:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:11:27,695:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,695:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,695:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:27,696:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:27,696:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,696:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:27,860:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:27,862:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000206 seconds.
2024-08-31 18:11:27,862:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:27,862:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:27,862:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:27,862:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:27,863:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:27,863:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,047:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,048:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000160 seconds.
2024-08-31 18:11:28,048:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,048:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,048:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:28,048:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:28,048:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,048:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000401 seconds.
2024-08-31 18:11:28,170:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,170:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,284:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,284:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:11:28,284:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,285:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:28,285:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:28,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,285:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,440:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,440:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.
2024-08-31 18:11:28,440:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,440:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,441:INFO:[LightGBM] [Info] Total Bins 507
2024-08-31 18:11:28,441:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:28,441:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,441:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,584:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000676 seconds.
2024-08-31 18:11:28,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,585:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:28,587:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:28,587:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,587:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,734:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,737:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.
2024-08-31 18:11:28,737:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,737:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,737:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:28,737:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:28,737:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,738:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:28,885:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:28,888:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000859 seconds.
2024-08-31 18:11:28,888:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:28,888:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:28,888:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:28,888:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:28,888:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:28,889:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,068:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,070:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.
2024-08-31 18:11:29,070:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,070:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,070:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:29,070:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:29,070:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,071:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,245:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,247:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000672 seconds.
2024-08-31 18:11:29,247:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,248:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,248:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:29,248:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:29,248:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,248:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,391:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,393:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.
2024-08-31 18:11:29,393:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,393:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,393:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:29,393:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:29,394:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,394:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,562:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,563:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.
2024-08-31 18:11:29,563:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,563:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,563:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:29,564:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:29,564:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,564:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.
2024-08-31 18:11:29,715:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,715:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,715:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:29,919:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:29,920:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.
2024-08-31 18:11:29,920:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:29,920:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:29,920:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:11:29,920:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:29,921:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:29,921:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,044:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,044:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.
2024-08-31 18:11:30,044:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,044:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,044:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:30,044:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:30,045:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,045:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,158:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,159:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:11:30,159:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,159:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,159:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:30,159:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:30,159:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,159:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,352:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,354:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.
2024-08-31 18:11:30,355:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,355:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,355:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:30,355:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:30,355:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,355:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,537:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,539:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.
2024-08-31 18:11:30,539:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,539:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,539:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:30,540:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:30,540:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,540:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,684:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,685:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.
2024-08-31 18:11:30,686:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,686:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,686:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:11:30,687:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:30,687:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,687:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,824:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,826:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.
2024-08-31 18:11:30,826:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,826:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,826:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:30,826:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:30,827:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,827:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:30,970:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:30,972:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.
2024-08-31 18:11:30,972:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:30,972:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:30,972:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:30,972:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:30,972:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:30,972:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,233:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,235:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.
2024-08-31 18:11:31,235:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,235:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,235:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:31,235:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:31,235:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,235:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,387:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,388:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.
2024-08-31 18:11:31,388:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,388:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,388:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:31,388:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:31,388:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,388:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,515:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,516:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.
2024-08-31 18:11:31,516:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,516:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,516:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:31,516:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:31,517:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,517:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,616:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,617:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:11:31,617:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,617:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,617:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:31,617:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:31,617:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,617:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,749:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,749:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.
2024-08-31 18:11:31,750:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,750:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,750:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:31,750:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:31,750:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,751:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,847:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,847:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.
2024-08-31 18:11:31,847:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,847:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,847:INFO:[LightGBM] [Info] Total Bins 551
2024-08-31 18:11:31,847:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:31,848:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,848:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:31,938:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:31,939:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.
2024-08-31 18:11:31,939:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:31,939:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:31,940:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:31,940:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:31,940:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:31,940:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,062:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,064:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.
2024-08-31 18:11:32,064:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,064:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,064:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:32,065:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:32,065:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,065:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,199:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.
2024-08-31 18:11:32,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,201:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,201:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:32,201:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:32,201:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,201:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,304:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,305:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.
2024-08-31 18:11:32,305:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,305:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,307:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:32,307:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:32,307:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,307:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,407:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,410:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.
2024-08-31 18:11:32,410:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,410:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,410:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:32,410:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:32,410:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,410:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,556:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,558:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.
2024-08-31 18:11:32,559:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,559:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,559:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:32,559:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:32,559:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,559:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,676:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,677:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.
2024-08-31 18:11:32,677:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,677:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,678:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:32,678:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:32,678:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,678:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,812:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,812:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.
2024-08-31 18:11:32,812:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,812:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,813:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:32,813:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:32,813:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,813:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:32,933:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:32,934:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.
2024-08-31 18:11:32,934:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:32,934:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:32,934:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:32,934:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:32,935:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:32,935:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,027:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,028:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.
2024-08-31 18:11:33,028:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,028:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,028:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:33,028:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:33,028:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,028:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,121:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,121:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000091 seconds.
2024-08-31 18:11:33,121:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,121:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,122:INFO:[LightGBM] [Info] Total Bins 550
2024-08-31 18:11:33,122:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:33,122:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,122:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,243:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,246:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.
2024-08-31 18:11:33,246:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,246:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,246:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:33,246:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:33,246:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,246:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,361:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,363:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.
2024-08-31 18:11:33,363:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,363:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,364:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:33,364:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:33,364:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,364:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,467:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,469:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000684 seconds.
2024-08-31 18:11:33,469:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,469:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,469:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:33,469:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:33,469:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,470:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,607:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,608:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000492 seconds.
2024-08-31 18:11:33,609:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,609:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,609:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:33,609:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:33,609:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,609:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,715:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,717:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.
2024-08-31 18:11:33,717:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,717:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,717:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:11:33,717:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:33,717:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,717:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,813:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,815:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.
2024-08-31 18:11:33,815:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,815:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,815:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:33,816:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:33,816:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,816:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:33,934:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:33,935:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.
2024-08-31 18:11:33,935:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:33,935:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:33,936:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:33,936:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:33,936:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:33,936:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:34,057:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:34,058:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.
2024-08-31 18:11:34,058:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,058:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,058:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:34,058:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:34,058:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:34,058:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:34,181:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:34,182:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000322 seconds.
2024-08-31 18:11:34,182:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:34,182:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:34,182:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:34,182:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:34,182:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:34,300:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:34,301:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.
2024-08-31 18:11:34,301:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,301:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,301:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:34,301:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:34,301:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:34,302:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:34,386:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:34,386:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:11:34,386:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,386:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,386:INFO:[LightGBM] [Info] Total Bins 508
2024-08-31 18:11:34,387:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 5
2024-08-31 18:11:34,387:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:34,387:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:34,487:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:34,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.
2024-08-31 18:11:34,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,489:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:34,489:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:34,489:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:34,489:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:34,588:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:34,590:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000786 seconds.
2024-08-31 18:11:34,590:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,590:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,590:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:34,590:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:34,590:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:34,590:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:34,692:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:34,694:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.
2024-08-31 18:11:34,694:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,694:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,694:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:11:34,694:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:34,694:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:34,694:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:34,794:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:34,796:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.
2024-08-31 18:11:34,796:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,796:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,796:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:11:34,796:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:34,797:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:34,797:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:34,938:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:34,939:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.
2024-08-31 18:11:34,940:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:34,940:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:34,940:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:34,940:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:34,940:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:34,940:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,088:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,090:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.
2024-08-31 18:11:35,090:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,090:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,090:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:35,090:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:35,090:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,090:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,293:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,294:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.
2024-08-31 18:11:35,294:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,294:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,294:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:35,294:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:35,295:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,295:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,440:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,440:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.
2024-08-31 18:11:35,440:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,440:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,440:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:35,441:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:35,441:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,441:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,548:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,549:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000091 seconds.
2024-08-31 18:11:35,549:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,549:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,549:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:35,549:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:35,549:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,549:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,648:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,648:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000091 seconds.
2024-08-31 18:11:35,648:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,648:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,648:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:35,648:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:35,649:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,649:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:35,873:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:35,875:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.
2024-08-31 18:11:35,875:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:35,875:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:35,875:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:35,876:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:35,876:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:35,876:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,034:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,036:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.
2024-08-31 18:11:36,036:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,037:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,037:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:36,037:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:36,037:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,037:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,190:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,192:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000736 seconds.
2024-08-31 18:11:36,192:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,192:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,192:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:36,192:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:36,193:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,193:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,308:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,311:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.
2024-08-31 18:11:36,311:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,311:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,311:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:36,311:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:36,312:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,312:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,470:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,472:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:11:36,472:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,472:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,472:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:36,472:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:36,472:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,472:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,591:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,593:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000480 seconds.
2024-08-31 18:11:36,593:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,593:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,593:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:36,593:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:36,593:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,593:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,708:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,710:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.
2024-08-31 18:11:36,710:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,710:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,710:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:36,710:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:36,710:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,710:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,845:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,847:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.
2024-08-31 18:11:36,847:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:36,848:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:36,848:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:36,848:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,848:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:36,953:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:36,954:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.
2024-08-31 18:11:36,954:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:36,954:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:36,954:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:11:36,954:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:36,954:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:36,954:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,042:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,042:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.
2024-08-31 18:11:37,042:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,043:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,043:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:37,043:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:37,043:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,043:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,177:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,178:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.
2024-08-31 18:11:37,178:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,179:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,179:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:37,179:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:37,179:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,179:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,290:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,291:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.
2024-08-31 18:11:37,292:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,292:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:11:37,292:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:37,292:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,292:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,436:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,438:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000841 seconds.
2024-08-31 18:11:37,438:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,438:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,438:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:11:37,438:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:37,440:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,440:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,565:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,567:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:11:37,567:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,567:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,567:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:37,567:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:37,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,568:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,671:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,673:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.
2024-08-31 18:11:37,673:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,673:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,673:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:37,673:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:37,673:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,674:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,793:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,795:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.
2024-08-31 18:11:37,795:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,795:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,795:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:37,795:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:37,795:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,795:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:37,902:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:37,903:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.
2024-08-31 18:11:37,903:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:37,903:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:37,904:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:37,904:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:37,904:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:37,904:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,067:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,068:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.
2024-08-31 18:11:38,068:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,068:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,068:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:38,068:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:38,069:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,069:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,190:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,191:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:11:38,191:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,191:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,191:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:38,191:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:38,191:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,191:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:11:38,339:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,339:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,339:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,484:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,487:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.
2024-08-31 18:11:38,487:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,487:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,487:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:38,487:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:38,488:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,488:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,676:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,678:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.
2024-08-31 18:11:38,678:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,678:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,678:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:38,678:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:38,679:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,679:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,848:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,850:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.
2024-08-31 18:11:38,850:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,850:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,851:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:11:38,851:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:38,851:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,851:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:38,981:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:38,982:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2024-08-31 18:11:38,982:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:38,982:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:38,983:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:38,983:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:38,983:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:38,983:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,176:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,179:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.
2024-08-31 18:11:39,179:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,179:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,179:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:39,179:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:39,179:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,180:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,297:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,298:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.
2024-08-31 18:11:39,298:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,298:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,298:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:39,298:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:39,298:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,298:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,402:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,403:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.
2024-08-31 18:11:39,403:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,403:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,403:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:39,404:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:39,404:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,404:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,537:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,537:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000466 seconds.
2024-08-31 18:11:39,537:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:39,537:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:39,537:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:39,538:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,538:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,646:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,646:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.
2024-08-31 18:11:39,647:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,647:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,647:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:39,647:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:39,647:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,647:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,744:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:39,744:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.
2024-08-31 18:11:39,744:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:39,744:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:39,744:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 6
2024-08-31 18:11:39,745:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:39,745:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:39,844:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:39,846:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.
2024-08-31 18:11:39,846:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,846:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,846:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:39,846:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:39,847:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:39,847:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:39,964:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:39,965:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.
2024-08-31 18:11:39,966:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:39,966:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:39,966:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:39,966:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:39,966:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:39,966:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,069:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,071:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.
2024-08-31 18:11:40,071:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,071:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,071:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:40,071:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:40,071:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,072:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,172:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,174:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:11:40,174:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,174:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,174:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:40,174:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:40,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,174:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,271:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,272:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.
2024-08-31 18:11:40,272:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,272:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,272:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:11:40,272:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:40,273:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,273:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,372:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,373:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2024-08-31 18:11:40,373:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,373:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,373:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:11:40,373:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:40,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,374:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,475:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,477:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.
2024-08-31 18:11:40,477:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,477:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,477:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:40,477:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:40,477:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,477:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,575:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,576:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000215 seconds.
2024-08-31 18:11:40,576:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,576:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,576:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:11:40,576:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:40,576:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,576:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,685:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,685:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.
2024-08-31 18:11:40,685:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:40,685:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:11:40,685:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:40,686:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,686:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,781:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,782:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.
2024-08-31 18:11:40,782:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,782:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,782:INFO:[LightGBM] [Info] Total Bins 564
2024-08-31 18:11:40,782:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:40,782:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,782:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:40,894:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:40,897:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.
2024-08-31 18:11:40,897:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:40,897:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:40,897:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:40,898:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:40,898:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:40,898:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,053:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,054:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.
2024-08-31 18:11:41,054:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,055:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,055:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:41,055:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:41,055:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,055:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,176:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001957 seconds.
2024-08-31 18:11:41,179:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:41,179:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:41,179:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:41,180:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,180:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,425:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,427:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.
2024-08-31 18:11:41,427:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,427:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,427:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:11:41,428:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:41,428:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,428:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,529:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,530:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.
2024-08-31 18:11:41,530:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,530:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,530:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:41,530:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:41,530:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,531:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,705:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,707:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.
2024-08-31 18:11:41,707:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,707:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,707:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:41,707:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:41,708:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,708:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,813:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,813:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.
2024-08-31 18:11:41,813:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,813:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,813:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:41,814:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:41,814:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,814:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:41,912:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:41,913:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.
2024-08-31 18:11:41,913:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:41,913:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:41,913:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:41,913:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:41,913:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:41,913:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,002:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,003:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.
2024-08-31 18:11:42,003:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:42,003:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:42,004:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:42,004:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,004:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,102:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,103:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
2024-08-31 18:11:42,103:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,103:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,103:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:42,103:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:42,103:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,104:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,196:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,198:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.
2024-08-31 18:11:42,198:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,198:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,198:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:42,198:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:42,198:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,198:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,333:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,335:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.
2024-08-31 18:11:42,335:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,335:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,335:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:42,335:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:42,335:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,337:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,472:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,474:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.
2024-08-31 18:11:42,474:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,474:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,474:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:42,474:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:42,474:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,474:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,582:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,584:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.
2024-08-31 18:11:42,584:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,584:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,584:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:42,584:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:42,584:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,584:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,715:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,717:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.
2024-08-31 18:11:42,718:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,718:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,718:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:42,718:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:42,718:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,718:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,813:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,815:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.
2024-08-31 18:11:42,815:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,815:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,815:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:42,815:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:42,815:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,815:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:42,906:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:42,908:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.
2024-08-31 18:11:42,908:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:42,908:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:42,908:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:42,908:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:42,908:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:42,908:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,005:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,006:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.
2024-08-31 18:11:43,007:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,007:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,007:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:43,007:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:43,007:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,007:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.
2024-08-31 18:11:43,095:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,095:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,095:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,211:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,211:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000094 seconds.
2024-08-31 18:11:43,211:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,211:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,211:INFO:[LightGBM] [Info] Total Bins 567
2024-08-31 18:11:43,211:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:43,211:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,212:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,305:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,308:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.
2024-08-31 18:11:43,309:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,309:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,309:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:43,309:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:43,309:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,309:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,421:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,422:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.
2024-08-31 18:11:43,423:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,423:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,423:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:43,423:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:43,424:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,424:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,533:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,534:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.
2024-08-31 18:11:43,534:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,534:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,534:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:11:43,535:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:43,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,535:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,645:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,646:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.
2024-08-31 18:11:43,646:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,646:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,647:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:43,647:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:43,648:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,648:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,746:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,747:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.
2024-08-31 18:11:43,747:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,748:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,748:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:43,748:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:43,748:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,748:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,847:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,849:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.
2024-08-31 18:11:43,849:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,849:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,849:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:43,849:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:43,849:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,849:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:43,984:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:43,985:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.
2024-08-31 18:11:43,985:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:43,985:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:43,985:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:43,985:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:43,985:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:43,985:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,083:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,083:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.
2024-08-31 18:11:44,083:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,083:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,084:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:44,084:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:44,084:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,084:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,173:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,173:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000121 seconds.
2024-08-31 18:11:44,174:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,174:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,174:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:44,174:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:44,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,174:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,270:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,270:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.
2024-08-31 18:11:44,270:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,270:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,270:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:44,270:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:44,271:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,271:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,387:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,389:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.
2024-08-31 18:11:44,389:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,389:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,389:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:44,389:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:44,390:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,390:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,516:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,517:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.
2024-08-31 18:11:44,518:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,518:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,518:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:44,518:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:44,518:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,518:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,638:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,640:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.
2024-08-31 18:11:44,640:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,640:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,640:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:44,640:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:44,640:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,640:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,743:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,744:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.
2024-08-31 18:11:44,744:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,746:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,746:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:44,746:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:44,746:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,746:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:44,891:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:44,893:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.
2024-08-31 18:11:44,893:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:44,893:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:44,893:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:44,893:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:44,893:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:44,893:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,000:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,001:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.
2024-08-31 18:11:45,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,001:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:45,001:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:45,002:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,002:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,138:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,139:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001134 seconds.
2024-08-31 18:11:45,140:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:45,140:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:45,140:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:45,140:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,140:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,298:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,299:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.
2024-08-31 18:11:45,299:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,299:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,299:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:45,299:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:45,299:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,300:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,512:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,512:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000136 seconds.
2024-08-31 18:11:45,512:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,512:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,512:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:45,513:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:45,513:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,513:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,649:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,650:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000113 seconds.
2024-08-31 18:11:45,650:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,650:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,650:INFO:[LightGBM] [Info] Total Bins 565
2024-08-31 18:11:45,650:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:45,650:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,650:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,744:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,746:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000869 seconds.
2024-08-31 18:11:45,746:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,746:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,746:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:45,746:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:45,747:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,747:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:45,896:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:45,897:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.
2024-08-31 18:11:45,897:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:45,897:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:45,897:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:45,897:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:45,897:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:45,898:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,000:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,002:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.
2024-08-31 18:11:46,002:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,002:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,002:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:46,002:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:46,002:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,002:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,118:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,120:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.
2024-08-31 18:11:46,120:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,120:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,120:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:46,120:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:46,121:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,121:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,218:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,219:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.
2024-08-31 18:11:46,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,219:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,219:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:11:46,220:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:46,220:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,220:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,311:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,313:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.
2024-08-31 18:11:46,313:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,313:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,313:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:46,313:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:46,314:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,314:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,406:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,407:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000437 seconds.
2024-08-31 18:11:46,407:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,407:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,407:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:46,407:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:46,407:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,407:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,500:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,500:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.
2024-08-31 18:11:46,501:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,501:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,501:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:46,501:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:46,501:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,501:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,591:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,591:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.
2024-08-31 18:11:46,592:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,592:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,592:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:46,592:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:46,592:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,592:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,712:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:46,713:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000141 seconds.
2024-08-31 18:11:46,713:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,713:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,713:INFO:[LightGBM] [Info] Total Bins 566
2024-08-31 18:11:46,713:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 6
2024-08-31 18:11:46,713:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:46,713:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:46,843:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:46,846:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.
2024-08-31 18:11:46,846:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,846:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,846:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:46,846:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:46,846:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:46,846:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:46,954:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:46,956:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.
2024-08-31 18:11:46,956:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:46,956:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:46,956:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:46,956:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:46,956:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:46,956:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,054:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,055:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000560 seconds.
2024-08-31 18:11:47,055:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,055:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,056:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:11:47,056:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:47,056:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,056:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,154:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,156:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.
2024-08-31 18:11:47,156:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,157:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,157:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:11:47,157:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:47,157:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,157:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,273:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,275:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.
2024-08-31 18:11:47,275:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,275:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,275:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:47,275:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:47,275:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,275:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,372:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,374:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.
2024-08-31 18:11:47,374:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,374:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,375:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:47,375:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:47,375:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,375:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,477:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,478:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.
2024-08-31 18:11:47,478:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:47,478:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:47,478:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:47,479:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,479:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,605:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,606:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.
2024-08-31 18:11:47,606:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,606:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,606:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:47,606:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:47,607:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,607:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,712:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,713:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.
2024-08-31 18:11:47,713:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,713:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,713:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:47,713:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:47,713:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,713:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:47,887:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:47,890:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001178 seconds.
2024-08-31 18:11:47,890:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:47,890:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:47,890:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:47,890:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:47,891:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:47,891:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,054:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,056:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000871 seconds.
2024-08-31 18:11:48,056:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,056:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,057:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:48,057:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:48,057:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,057:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,185:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,187:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.
2024-08-31 18:11:48,187:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,187:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,187:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:48,187:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:48,187:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,187:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,343:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,346:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.
2024-08-31 18:11:48,346:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,346:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,347:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:48,347:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:48,347:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,347:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,492:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,494:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.
2024-08-31 18:11:48,494:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,494:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,494:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:48,494:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:48,495:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,495:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,692:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,694:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000829 seconds.
2024-08-31 18:11:48,694:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,694:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,694:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:48,694:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:48,694:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,695:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:48,903:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:48,905:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.
2024-08-31 18:11:48,905:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:48,905:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:48,905:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:48,905:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:48,905:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:48,905:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.
2024-08-31 18:11:49,036:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,036:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,036:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,137:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,138:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
2024-08-31 18:11:49,138:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,138:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,138:INFO:[LightGBM] [Info] Total Bins 571
2024-08-31 18:11:49,138:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:49,139:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,139:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,254:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,255:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.
2024-08-31 18:11:49,256:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,256:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,256:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:49,256:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:49,256:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,256:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,417:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,419:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.
2024-08-31 18:11:49,419:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,419:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,419:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:11:49,419:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:49,420:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,420:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,571:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,573:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000701 seconds.
2024-08-31 18:11:49,573:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,573:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,573:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:11:49,573:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:49,574:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,574:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,683:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,684:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.
2024-08-31 18:11:49,684:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,684:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,684:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:49,684:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:49,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,685:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,785:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,787:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.
2024-08-31 18:11:49,787:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,787:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,787:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:49,787:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:49,788:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,788:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:49,914:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:49,915:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000510 seconds.
2024-08-31 18:11:49,915:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:49,915:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:49,916:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:49,916:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:49,916:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:49,916:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,025:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,028:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000492 seconds.
2024-08-31 18:11:50,028:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,028:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,028:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:50,028:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:50,028:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,028:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,129:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,129:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.
2024-08-31 18:11:50,129:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,129:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,129:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:50,129:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:50,130:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,130:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,218:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,218:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000137 seconds.
2024-08-31 18:11:50,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,219:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,219:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:50,219:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:50,219:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,219:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,318:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,320:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.
2024-08-31 18:11:50,321:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,321:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,321:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:50,321:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:50,322:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,322:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,435:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,437:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.
2024-08-31 18:11:50,437:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,437:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,437:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:50,438:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:50,438:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,438:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,605:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,609:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001396 seconds.
2024-08-31 18:11:50,609:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:50,609:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:50,609:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:11:50,609:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:50,609:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,609:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:50,801:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:50,803:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001546 seconds.
2024-08-31 18:11:50,803:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:50,803:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:50,804:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:50,804:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:50,804:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,102:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:51,104:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.
2024-08-31 18:11:51,104:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,104:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,104:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:51,105:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:51,105:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:51,105:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,290:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:51,291:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000473 seconds.
2024-08-31 18:11:51,291:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,292:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:51,292:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:51,292:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:51,292:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,441:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:51,443:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.
2024-08-31 18:11:51,443:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,443:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,443:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:51,443:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:51,444:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:51,444:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,575:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:51,576:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.
2024-08-31 18:11:51,576:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,576:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,576:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:51,576:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:51,576:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:51,576:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,676:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:51,677:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000489 seconds.
2024-08-31 18:11:51,677:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:51,677:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:51,677:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 7
2024-08-31 18:11:51,677:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:51,677:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:51,808:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:51,811:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000801 seconds.
2024-08-31 18:11:51,811:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,811:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,811:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:11:51,811:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:51,811:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:51,811:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:51,915:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:51,917:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000513 seconds.
2024-08-31 18:11:51,917:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:51,917:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:51,917:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:51,917:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:51,918:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:51,918:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,067:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,069:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.
2024-08-31 18:11:52,069:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,069:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,069:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:11:52,070:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:52,070:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,070:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,189:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,190:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.
2024-08-31 18:11:52,190:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,190:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,191:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:11:52,191:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:52,191:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,191:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,316:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,317:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.
2024-08-31 18:11:52,317:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,317:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,317:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:11:52,317:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:52,318:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,318:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,412:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,414:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.
2024-08-31 18:11:52,414:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,414:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,414:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:11:52,414:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:52,415:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,415:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,530:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,532:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000423 seconds.
2024-08-31 18:11:52,532:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,532:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,532:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:52,532:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:52,532:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,533:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,624:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,625:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.
2024-08-31 18:11:52,625:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,625:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,625:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:11:52,625:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:52,625:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,625:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.
2024-08-31 18:11:52,721:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,721:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] Total Bins 576
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,721:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,864:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,866:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.
2024-08-31 18:11:52,866:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,866:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,866:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:52,866:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:52,866:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,866:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:52,968:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:52,970:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:11:52,970:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:52,970:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:52,970:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:52,970:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:52,970:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:52,970:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,097:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,098:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.
2024-08-31 18:11:53,098:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,098:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,099:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:53,099:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:53,099:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,099:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,195:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,198:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.
2024-08-31 18:11:53,198:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,198:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,198:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:11:53,198:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:53,199:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,199:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,304:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,305:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000659 seconds.
2024-08-31 18:11:53,305:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,305:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,305:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:53,307:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:53,307:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,307:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,417:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,419:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.
2024-08-31 18:11:53,419:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,419:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,419:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:53,419:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:53,419:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,419:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,544:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,545:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000184 seconds.
2024-08-31 18:11:53,545:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,545:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,545:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:53,545:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:53,545:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,545:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,652:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,652:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.
2024-08-31 18:11:53,652:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,653:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,653:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:53,653:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:53,653:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,653:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,795:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,796:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.
2024-08-31 18:11:53,796:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,796:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,796:INFO:[LightGBM] [Info] Total Bins 579
2024-08-31 18:11:53,796:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:53,796:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,796:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:53,917:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:53,918:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.
2024-08-31 18:11:53,918:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:53,918:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:53,918:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:11:53,918:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:53,919:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:53,919:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,052:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.
2024-08-31 18:11:54,052:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,052:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,053:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:11:54,053:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:54,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,053:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,159:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,162:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.
2024-08-31 18:11:54,162:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,162:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,162:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:54,162:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:54,162:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,163:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,322:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,324:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000513 seconds.
2024-08-31 18:11:54,324:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,324:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,325:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:54,325:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:54,325:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,325:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,439:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,440:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.
2024-08-31 18:11:54,440:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,440:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,440:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:11:54,440:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:54,440:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,440:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,558:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,560:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:11:54,560:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,560:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,560:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:11:54,560:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:54,560:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,561:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,681:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,683:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.
2024-08-31 18:11:54,683:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:54,684:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:54,684:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:11:54,684:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:54,684:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,684:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:54,817:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:54,817:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000515 seconds.
2024-08-31 18:11:54,817:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:54,817:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:11:54,817:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:54,818:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:54,818:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,000:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,002:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.
2024-08-31 18:11:55,002:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,003:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,003:INFO:[LightGBM] [Info] Total Bins 573
2024-08-31 18:11:55,003:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:55,003:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,003:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,214:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,217:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.
2024-08-31 18:11:55,217:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,217:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,217:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:55,217:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:55,217:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,217:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,409:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,411:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.
2024-08-31 18:11:55,411:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,411:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,412:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:55,412:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:55,412:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,412:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,584:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.
2024-08-31 18:11:55,587:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,587:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,587:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:11:55,587:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:55,587:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,587:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,743:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,746:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000862 seconds.
2024-08-31 18:11:55,746:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,746:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,746:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:55,746:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:55,746:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,746:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,891:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,892:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.
2024-08-31 18:11:55,892:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,892:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,893:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:55,893:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:55,893:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,893:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:55,989:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:55,991:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.
2024-08-31 18:11:55,991:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:55,991:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:55,991:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:55,991:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:55,991:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:55,991:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,101:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,103:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.
2024-08-31 18:11:56,103:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,103:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,103:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:56,103:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:56,103:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,103:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,208:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,208:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
2024-08-31 18:11:56,208:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,208:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,208:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:56,209:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:56,209:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,209:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,309:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,310:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000517 seconds.
2024-08-31 18:11:56,310:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:56,310:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:56,311:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:56,311:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,311:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,413:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,415:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.
2024-08-31 18:11:56,415:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,415:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,416:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:11:56,416:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:56,416:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,416:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,550:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,552:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.
2024-08-31 18:11:56,552:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,552:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,552:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:11:56,553:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:56,553:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,553:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,665:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,668:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.
2024-08-31 18:11:56,668:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,668:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,668:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:11:56,668:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:56,670:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,670:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:56,870:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:56,873:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.
2024-08-31 18:11:56,873:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:56,873:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:56,873:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:11:56,874:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:56,874:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:56,874:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,128:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,130:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.
2024-08-31 18:11:57,130:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,130:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,131:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:11:57,131:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:57,131:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,131:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,313:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,315:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000497 seconds.
2024-08-31 18:11:57,315:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,316:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,316:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:11:57,316:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:57,316:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,316:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,429:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,431:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.
2024-08-31 18:11:57,431:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,431:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,431:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:11:57,431:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:57,431:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,431:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,550:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,550:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.
2024-08-31 18:11:57,550:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,550:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,551:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:11:57,551:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:57,551:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,551:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,650:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,650:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.
2024-08-31 18:11:57,650:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,650:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,650:INFO:[LightGBM] [Info] Total Bins 577
2024-08-31 18:11:57,651:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:57,651:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,651:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,755:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,758:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.
2024-08-31 18:11:57,758:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,758:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,758:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:57,758:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:11:57,758:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,759:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:57,884:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:57,887:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000853 seconds.
2024-08-31 18:11:57,887:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:57,887:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:57,887:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:57,887:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:11:57,887:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:57,887:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,045:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,047:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000870 seconds.
2024-08-31 18:11:58,048:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,048:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,048:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:11:58,048:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:11:58,048:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,048:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,192:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,194:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.
2024-08-31 18:11:58,194:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,194:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,194:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:11:58,194:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:11:58,194:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,194:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,311:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,314:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.
2024-08-31 18:11:58,314:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,314:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,314:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:11:58,314:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:11:58,314:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,314:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,520:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,522:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.
2024-08-31 18:11:58,522:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,522:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,522:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:58,523:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:11:58,523:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,523:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,679:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,681:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.
2024-08-31 18:11:58,681:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,681:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,682:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:11:58,682:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:11:58,682:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,683:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,804:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,804:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.
2024-08-31 18:11:58,804:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,804:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,805:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:58,805:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:11:58,805:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,805:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:58,901:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:11:58,902:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.
2024-08-31 18:11:58,902:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:58,902:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:58,902:INFO:[LightGBM] [Info] Total Bins 578
2024-08-31 18:11:58,902:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 7
2024-08-31 18:11:58,902:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:11:58,902:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:11:59,050:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,052:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001096 seconds.
2024-08-31 18:11:59,052:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,052:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,052:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:11:59,053:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:11:59,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,053:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,246:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,248:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.
2024-08-31 18:11:59,248:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,248:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,248:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:11:59,248:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:11:59,249:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,249:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,403:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,405:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001941 seconds.
2024-08-31 18:11:59,405:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:11:59,405:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:11:59,405:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:11:59,405:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,407:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,535:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,538:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:11:59,538:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,538:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,538:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:11:59,538:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:11:59,538:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,538:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,637:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,638:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.
2024-08-31 18:11:59,638:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,638:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,638:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:11:59,638:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:11:59,638:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,638:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,740:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,741:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.
2024-08-31 18:11:59,741:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,741:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,741:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:11:59,741:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:11:59,742:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,742:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,838:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,839:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000152 seconds.
2024-08-31 18:11:59,839:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,839:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,839:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:11:59,839:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:11:59,839:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,839:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:11:59,942:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:11:59,943:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.
2024-08-31 18:11:59,943:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:11:59,943:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:11:59,943:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:11:59,943:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:11:59,943:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:11:59,943:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,053:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000656 seconds.
2024-08-31 18:12:00,053:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,053:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,053:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:00,053:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:00,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,053:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,169:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,171:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000768 seconds.
2024-08-31 18:12:00,171:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,171:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,171:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:00,171:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:00,172:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,172:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,315:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,317:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000675 seconds.
2024-08-31 18:12:00,317:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,317:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,317:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:00,317:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:00,317:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,317:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,447:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,449:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.
2024-08-31 18:12:00,449:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,449:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,449:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:00,449:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:00,450:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,450:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,553:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,554:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000482 seconds.
2024-08-31 18:12:00,554:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,554:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,554:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:00,555:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:00,555:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,555:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,701:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,703:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.
2024-08-31 18:12:00,703:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,703:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,703:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:00,703:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:00,704:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,704:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:00,871:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:00,872:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.
2024-08-31 18:12:00,873:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:00,873:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:00,873:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:00,873:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:00,873:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:00,873:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,047:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,047:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000177 seconds.
2024-08-31 18:12:01,047:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,047:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,047:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:12:01,047:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:12:01,048:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,048:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,184:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,185:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.
2024-08-31 18:12:01,185:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,185:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,187:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:01,187:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:01,187:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,187:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,290:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,292:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000681 seconds.
2024-08-31 18:12:01,292:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,292:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:01,292:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:01,293:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,293:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,434:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,438:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.
2024-08-31 18:12:01,438:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,439:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,439:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:01,439:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:01,439:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,439:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,673:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,675:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001562 seconds.
2024-08-31 18:12:01,675:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:01,675:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:01,675:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:01,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,675:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,833:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,834:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.
2024-08-31 18:12:01,834:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,835:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,835:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:01,835:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:01,835:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,835:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:01,982:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:01,984:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.
2024-08-31 18:12:01,984:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:01,984:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:01,984:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:01,984:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:01,984:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:01,984:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,115:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,117:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.
2024-08-31 18:12:02,117:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,117:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,117:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:02,117:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:02,117:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,118:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,276:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,277:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.
2024-08-31 18:12:02,277:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,277:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,277:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:12:02,277:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:12:02,277:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,277:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,480:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,482:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.
2024-08-31 18:12:02,483:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,483:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,483:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:02,483:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:02,484:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,484:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,642:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,644:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.
2024-08-31 18:12:02,644:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,644:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,644:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:02,644:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:02,644:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,645:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,784:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,786:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.
2024-08-31 18:12:02,787:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,787:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,787:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:02,787:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:02,788:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,788:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:02,915:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:02,918:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:12:02,918:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:02,918:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:02,918:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:02,918:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:02,918:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:02,918:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:03,050:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:03,052:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.
2024-08-31 18:12:03,052:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:03,052:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:03,052:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:03,052:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:03,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:03,053:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:03,201:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:03,203:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:12:03,203:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:03,203:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:03,203:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:03,203:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:03,203:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:03,203:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:03,379:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:03,381:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.
2024-08-31 18:12:03,382:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:03,382:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:03,382:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:12:03,382:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:03,382:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:03,382:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:03,544:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:03,545:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.
2024-08-31 18:12:03,545:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:03,545:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:12:03,545:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 8
2024-08-31 18:12:03,545:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:03,545:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:03,681:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:03,683:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001495 seconds.
2024-08-31 18:12:03,683:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:03,684:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:03,684:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:03,684:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:03,684:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:03,851:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:03,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.
2024-08-31 18:12:03,854:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:03,854:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:03,854:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:03,854:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:03,854:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:03,854:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,012:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,015:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.
2024-08-31 18:12:04,015:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,015:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,015:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:04,015:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:04,015:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,015:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,156:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,158:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.
2024-08-31 18:12:04,158:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,159:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,159:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:04,159:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:04,159:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,159:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,294:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,295:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.
2024-08-31 18:12:04,297:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,297:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,297:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:12:04,297:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:04,297:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,297:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,431:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,433:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.
2024-08-31 18:12:04,433:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,433:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,433:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:12:04,433:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:04,434:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,434:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,554:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,556:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.
2024-08-31 18:12:04,556:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,556:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,556:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:12:04,556:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:04,556:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,557:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,694:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,695:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000182 seconds.
2024-08-31 18:12:04,695:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,695:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,695:INFO:[LightGBM] [Info] Total Bins 582
2024-08-31 18:12:04,695:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:04,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,695:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,840:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,842:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.
2024-08-31 18:12:04,842:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,842:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,842:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:04,842:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:04,842:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,842:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:04,992:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:04,994:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.
2024-08-31 18:12:04,994:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:04,994:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:04,994:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:04,994:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:04,994:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:04,994:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,131:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,133:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.
2024-08-31 18:12:05,133:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,134:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,134:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:05,134:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:05,134:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,134:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,280:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,282:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.
2024-08-31 18:12:05,282:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,282:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,282:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:12:05,283:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:05,283:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,283:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,444:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,447:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000664 seconds.
2024-08-31 18:12:05,447:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,447:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,447:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:05,447:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:05,447:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,447:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,583:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.
2024-08-31 18:12:05,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,585:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:05,585:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:05,585:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,585:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,715:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,717:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000180 seconds.
2024-08-31 18:12:05,717:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,717:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,717:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:05,717:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:05,717:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,717:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,854:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,855:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000407 seconds.
2024-08-31 18:12:05,855:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:05,855:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:12:05,856:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:05,856:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,856:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:05,972:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:05,975:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.
2024-08-31 18:12:05,975:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:05,975:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:05,975:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:05,975:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:05,976:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:05,976:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,101:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,102:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.
2024-08-31 18:12:06,102:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,102:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,102:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:06,104:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:06,104:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,104:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,217:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,219:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.
2024-08-31 18:12:06,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,220:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,220:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:06,220:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:06,220:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,220:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,359:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,360:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.
2024-08-31 18:12:06,360:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,360:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,361:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:06,361:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:06,361:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,361:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,469:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,471:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.
2024-08-31 18:12:06,471:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,471:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,471:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:06,471:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:06,471:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,471:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,567:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,568:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000407 seconds.
2024-08-31 18:12:06,568:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,568:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,568:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:06,568:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:06,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,569:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,709:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,711:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.
2024-08-31 18:12:06,711:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:06,711:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:12:06,711:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:06,711:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,712:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,846:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,847:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.
2024-08-31 18:12:06,847:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:06,847:INFO:[LightGBM] [Info] Total Bins 585
2024-08-31 18:12:06,847:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:06,847:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,847:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:06,955:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:06,957:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.
2024-08-31 18:12:06,957:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:06,957:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:06,957:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:06,957:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:06,958:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:06,958:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,072:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,073:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.
2024-08-31 18:12:07,074:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,074:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,074:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:07,074:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:07,074:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,074:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,210:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,212:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000798 seconds.
2024-08-31 18:12:07,212:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,212:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,212:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:12:07,212:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:07,212:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,213:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,328:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,330:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.
2024-08-31 18:12:07,330:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,330:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,330:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:07,330:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:07,331:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,331:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,477:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,480:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000838 seconds.
2024-08-31 18:12:07,480:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,481:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,481:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:07,481:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:07,481:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,481:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,682:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,684:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.
2024-08-31 18:12:07,684:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,684:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,685:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:07,685:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:07,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,685:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,788:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,789:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000489 seconds.
2024-08-31 18:12:07,789:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,789:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,789:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:12:07,790:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:07,790:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,790:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:07,891:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:07,891:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000147 seconds.
2024-08-31 18:12:07,892:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:07,892:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:07,892:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:12:07,892:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:07,892:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:07,892:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,021:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,023:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.
2024-08-31 18:12:08,023:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,023:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,023:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:08,024:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:08,024:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,024:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,153:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,154:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.
2024-08-31 18:12:08,154:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,155:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,155:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:08,155:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:08,155:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,155:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,277:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,279:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.
2024-08-31 18:12:08,279:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,280:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,280:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:08,280:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:08,280:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,280:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,468:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,470:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000881 seconds.
2024-08-31 18:12:08,470:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,470:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,470:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:08,471:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:08,471:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,471:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,657:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,659:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.
2024-08-31 18:12:08,659:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,659:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,659:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:08,660:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:08,660:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,660:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,795:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,798:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.
2024-08-31 18:12:08,798:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,798:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,798:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:08,798:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:08,798:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,798:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:08,978:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:08,980:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000664 seconds.
2024-08-31 18:12:08,980:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:08,980:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:08,980:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:08,980:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:08,981:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:08,981:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,134:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,134:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.
2024-08-31 18:12:09,134:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:09,134:INFO:[LightGBM] [Info] Total Bins 583
2024-08-31 18:12:09,134:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:09,135:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,135:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,263:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,265:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.
2024-08-31 18:12:09,265:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:09,265:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:09,265:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:09,265:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:09,266:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,266:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,409:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,411:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.
2024-08-31 18:12:09,411:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:09,411:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:09,411:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:09,411:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:09,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,411:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,599:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,601:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000956 seconds.
2024-08-31 18:12:09,601:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:09,601:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:09,601:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:09,601:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:09,602:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,602:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,757:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,759:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.
2024-08-31 18:12:09,759:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:09,759:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:09,759:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:09,759:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:09,759:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,759:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:09,902:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:09,904:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.
2024-08-31 18:12:09,904:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:09,904:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:09,904:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:12:09,904:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:09,904:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:09,904:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:10,086:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:10,088:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.
2024-08-31 18:12:10,088:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:10,088:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:10,088:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:10,088:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:10,089:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:10,089:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:10,241:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:10,242:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.
2024-08-31 18:12:10,243:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:10,243:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:10,243:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:12:10,243:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:10,244:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:10,244:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:10,374:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:10,375:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.
2024-08-31 18:12:10,375:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:10,375:INFO:[LightGBM] [Info] Total Bins 584
2024-08-31 18:12:10,375:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 8
2024-08-31 18:12:10,375:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:10,375:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:10,538:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:10,540:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000839 seconds.
2024-08-31 18:12:10,540:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:10,541:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:10,541:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:10,541:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:10,541:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:10,541:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:10,762:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:10,764:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000630 seconds.
2024-08-31 18:12:10,764:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:10,764:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:10,764:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:10,764:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:10,764:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:10,765:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:10,928:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:10,930:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.
2024-08-31 18:12:10,930:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:10,930:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:10,930:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:12:10,930:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:10,931:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:10,931:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,097:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,099:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2024-08-31 18:12:11,099:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,099:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,099:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:12:11,099:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:11,100:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,100:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,198:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.
2024-08-31 18:12:11,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,201:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,201:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:11,201:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:11,201:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,201:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,342:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,345:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.
2024-08-31 18:12:11,345:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,345:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,345:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:11,345:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:11,346:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,346:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,507:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,507:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.
2024-08-31 18:12:11,507:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,507:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,507:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:12:11,507:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:11,508:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,508:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,650:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,652:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000849 seconds.
2024-08-31 18:12:11,652:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,652:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,652:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:11,652:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:11,653:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,653:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,836:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,838:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.
2024-08-31 18:12:11,838:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,838:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,838:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:11,838:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:11,839:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,839:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:11,956:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:11,958:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.
2024-08-31 18:12:11,958:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:11,958:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:11,958:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:11,959:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:11,959:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:11,959:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,084:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,086:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.
2024-08-31 18:12:12,086:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,086:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,086:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:12,086:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:12,087:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,087:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,263:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,265:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000681 seconds.
2024-08-31 18:12:12,265:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,265:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,267:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:12,267:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:12,267:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,267:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,384:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,385:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.
2024-08-31 18:12:12,386:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,386:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,386:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:12,386:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:12,386:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,386:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,481:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,483:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.
2024-08-31 18:12:12,483:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,483:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,483:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:12,483:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:12,484:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,484:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,610:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,612:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.
2024-08-31 18:12:12,612:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,612:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,613:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:12,613:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:12,613:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,613:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,780:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,782:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.
2024-08-31 18:12:12,782:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,782:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,782:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:12,782:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:12,783:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,783:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,887:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,889:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.
2024-08-31 18:12:12,889:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,889:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,889:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:12,889:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:12,889:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,889:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:12,991:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:12,992:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.
2024-08-31 18:12:12,992:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:12,993:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:12,993:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:12,993:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:12,993:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:12,993:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,093:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,094:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000480 seconds.
2024-08-31 18:12:13,094:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,095:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,095:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:13,095:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:13,095:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,095:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,207:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,209:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.
2024-08-31 18:12:13,209:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,209:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,209:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:13,209:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:13,210:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,210:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,309:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,312:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.
2024-08-31 18:12:13,312:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,312:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,312:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:13,312:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:13,312:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,312:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,442:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,444:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.
2024-08-31 18:12:13,444:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,444:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,445:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:13,445:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:13,445:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,445:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,541:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,543:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.
2024-08-31 18:12:13,543:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,543:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,543:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:13,543:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:13,543:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,543:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,679:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,682:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.
2024-08-31 18:12:13,682:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,682:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,682:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:13,682:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:13,682:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,683:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:13,874:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:13,877:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.
2024-08-31 18:12:13,877:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:13,877:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:13,878:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:13,878:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:13,878:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:13,878:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:14,061:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:14,063:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000682 seconds.
2024-08-31 18:12:14,063:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,063:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,063:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:14,063:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:14,063:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:14,063:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:14,169:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:14,171:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.
2024-08-31 18:12:14,171:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,171:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,171:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:14,171:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:14,171:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:14,171:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:14,345:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:14,346:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001114 seconds.
2024-08-31 18:12:14,346:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:14,346:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:12:14,347:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 9
2024-08-31 18:12:14,347:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:14,347:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:14,484:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:14,486:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.
2024-08-31 18:12:14,486:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,486:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,486:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:14,486:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:14,487:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:14,487:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:14,592:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:14,593:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:12:14,593:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,594:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,594:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:14,594:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:14,594:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:14,594:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:14,695:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:14,697:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.
2024-08-31 18:12:14,697:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,697:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,698:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:14,698:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:14,698:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:14,698:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:14,796:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:14,798:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.
2024-08-31 18:12:14,798:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,798:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,798:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:14,798:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:14,798:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:14,799:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:14,894:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:14,895:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.
2024-08-31 18:12:14,895:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:14,895:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:14,895:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:12:14,896:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:14,896:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:14,896:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,014:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,016:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.
2024-08-31 18:12:15,016:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,016:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,016:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:12:15,016:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:15,017:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,017:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,121:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,122:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.
2024-08-31 18:12:15,122:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,122:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,122:INFO:[LightGBM] [Info] Total Bins 588
2024-08-31 18:12:15,123:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:15,123:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,123:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,227:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,228:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000655 seconds.
2024-08-31 18:12:15,228:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,229:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,229:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:15,229:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:15,229:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,229:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,339:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,341:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.
2024-08-31 18:12:15,341:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,341:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,341:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:15,341:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:15,342:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,342:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,444:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,446:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.
2024-08-31 18:12:15,446:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,446:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,446:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:15,446:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:15,446:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,447:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,543:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,545:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.
2024-08-31 18:12:15,545:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,545:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,545:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:12:15,545:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:15,546:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,546:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,640:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,641:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.
2024-08-31 18:12:15,641:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,641:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,641:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:15,641:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:15,642:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,642:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,737:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,738:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.
2024-08-31 18:12:15,739:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,739:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,739:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:15,739:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:15,739:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,739:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,837:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,838:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000228 seconds.
2024-08-31 18:12:15,838:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,838:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,839:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:15,839:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:15,839:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,839:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:15,939:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:15,942:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.
2024-08-31 18:12:15,942:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:15,942:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:15,942:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:15,942:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:15,943:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:15,943:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,046:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,048:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.
2024-08-31 18:12:16,048:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,048:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,048:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:16,048:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:16,048:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,049:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,148:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,150:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.
2024-08-31 18:12:16,150:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,150:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,150:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:16,150:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:16,150:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,151:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,250:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,251:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.
2024-08-31 18:12:16,251:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,251:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,251:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:16,252:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:16,252:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,252:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,345:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,348:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:12:16,348:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,348:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,348:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:16,348:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:16,349:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,349:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,438:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,440:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.
2024-08-31 18:12:16,440:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,440:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,440:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:16,440:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:16,440:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,440:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,555:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,557:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000429 seconds.
2024-08-31 18:12:16,557:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,557:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,557:INFO:[LightGBM] [Info] Total Bins 591
2024-08-31 18:12:16,557:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:16,557:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,557:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,683:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,685:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.
2024-08-31 18:12:16,685:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,685:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,685:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:16,685:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:16,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,685:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,797:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,799:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.
2024-08-31 18:12:16,799:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,799:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,799:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:16,799:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:16,800:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,800:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:16,908:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:16,909:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.
2024-08-31 18:12:16,909:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:16,909:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:16,909:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:12:16,910:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:16,910:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:16,910:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,010:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,011:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.
2024-08-31 18:12:17,011:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,011:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,011:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:17,011:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:17,013:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,013:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,122:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,124:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.
2024-08-31 18:12:17,124:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,124:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,124:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:17,125:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:17,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,125:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,245:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,247:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.
2024-08-31 18:12:17,247:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,247:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,247:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:17,248:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:17,248:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,248:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,348:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,350:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001326 seconds.
2024-08-31 18:12:17,350:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:17,350:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:12:17,350:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:17,350:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,350:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,473:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,475:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.
2024-08-31 18:12:17,475:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,475:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,475:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:17,475:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:17,476:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,476:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,607:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,610:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.
2024-08-31 18:12:17,610:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,610:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,610:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:17,610:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:17,610:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,610:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,718:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,720:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.
2024-08-31 18:12:17,720:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,720:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,720:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:17,720:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:17,721:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,721:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,825:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,827:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.
2024-08-31 18:12:17,827:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,827:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,828:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:17,828:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:17,828:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,828:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:17,947:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:17,949:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.
2024-08-31 18:12:17,949:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:17,949:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:17,950:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:17,950:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:17,950:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:17,950:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,040:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,042:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.
2024-08-31 18:12:18,042:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,042:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,042:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:18,042:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:18,042:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,042:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,131:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,133:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000513 seconds.
2024-08-31 18:12:18,133:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,133:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,133:INFO:[LightGBM] [Info] Total Bins 589
2024-08-31 18:12:18,133:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:18,133:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,133:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,249:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,251:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.
2024-08-31 18:12:18,251:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,251:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,251:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:18,252:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:18,252:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,252:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,425:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,429:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.
2024-08-31 18:12:18,429:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,429:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,429:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:18,429:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:18,429:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,430:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,565:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,567:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.
2024-08-31 18:12:18,567:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,567:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,567:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:18,567:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:18,567:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,567:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,730:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,735:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002833 seconds.
2024-08-31 18:12:18,736:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,736:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,736:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:18,736:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:18,736:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,736:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:18,929:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:18,931:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000743 seconds.
2024-08-31 18:12:18,931:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:18,932:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:18,932:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:12:18,932:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:18,932:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:18,933:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:19,096:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:19,098:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.
2024-08-31 18:12:19,098:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,098:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,098:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:19,099:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:19,099:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:19,099:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:19,270:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:19,272:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.
2024-08-31 18:12:19,272:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,272:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,272:INFO:[LightGBM] [Info] Total Bins 590
2024-08-31 18:12:19,272:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 9
2024-08-31 18:12:19,272:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:19,273:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:19,417:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:19,420:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.
2024-08-31 18:12:19,420:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,420:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,420:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:19,420:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:19,420:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:19,420:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:19,555:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:19,557:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000712 seconds.
2024-08-31 18:12:19,557:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,557:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,557:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:19,557:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:19,557:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:19,557:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:19,672:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:19,674:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.
2024-08-31 18:12:19,674:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,674:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,674:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:12:19,674:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:19,674:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:19,674:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:19,800:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:19,802:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.
2024-08-31 18:12:19,802:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,803:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,803:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:12:19,803:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:19,803:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:19,803:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:19,942:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:19,944:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000658 seconds.
2024-08-31 18:12:19,944:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:19,944:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:19,944:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:19,944:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:19,944:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:19,944:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,059:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,060:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.
2024-08-31 18:12:20,060:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,060:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,060:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:20,060:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:20,061:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,061:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,174:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,175:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.
2024-08-31 18:12:20,175:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,175:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,176:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:20,176:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:20,176:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,176:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,287:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,289:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.
2024-08-31 18:12:20,289:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,289:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,290:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:20,290:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:20,290:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,290:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,413:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,415:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.
2024-08-31 18:12:20,415:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,415:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,415:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:20,415:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:20,415:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,415:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,531:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,533:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.
2024-08-31 18:12:20,533:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,533:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,533:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:20,533:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:20,534:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,534:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,646:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,647:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.
2024-08-31 18:12:20,647:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,647:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,647:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:20,647:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:20,648:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,648:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,764:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,765:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.
2024-08-31 18:12:20,765:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,766:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,766:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:20,766:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:20,766:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,766:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:20,874:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:20,876:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.
2024-08-31 18:12:20,876:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:20,876:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:20,876:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:20,876:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:20,876:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:20,876:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,006:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,008:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.
2024-08-31 18:12:21,008:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,008:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,008:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:21,009:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:21,009:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,009:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,190:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,192:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.
2024-08-31 18:12:21,192:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,192:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,192:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:21,193:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:21,193:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,193:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,357:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,359:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.
2024-08-31 18:12:21,359:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,359:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,359:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:21,359:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:21,359:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,360:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,497:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,499:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.
2024-08-31 18:12:21,499:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,499:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,499:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:21,499:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:21,499:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,499:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,629:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,631:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.
2024-08-31 18:12:21,631:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,631:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,631:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:21,632:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:21,632:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,632:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,782:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,784:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000660 seconds.
2024-08-31 18:12:21,784:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,784:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,784:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:21,785:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:21,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,785:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:21,887:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:21,889:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.
2024-08-31 18:12:21,889:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:21,889:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:21,889:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:21,889:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:21,889:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:21,889:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:22,007:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:22,009:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2024-08-31 18:12:22,009:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,009:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,009:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:22,009:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:22,009:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:22,010:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:22,110:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:22,112:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.
2024-08-31 18:12:22,112:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,112:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,113:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:22,113:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:22,113:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:22,113:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:22,212:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:22,214:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.
2024-08-31 18:12:22,214:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,214:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,215:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:22,215:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:22,215:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:22,215:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:22,339:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:22,341:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000415 seconds.
2024-08-31 18:12:22,341:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,341:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,341:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:22,341:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 10
2024-08-31 18:12:22,341:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:22,341:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:22,446:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:22,448:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.
2024-08-31 18:12:22,448:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,448:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,449:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:22,449:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:22,449:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:22,449:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:22,617:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:22,620:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000803 seconds.
2024-08-31 18:12:22,620:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,620:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,620:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:22,620:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:22,621:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:22,621:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:22,755:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:22,757:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.
2024-08-31 18:12:22,757:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,757:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,758:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:22,758:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:22,758:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:22,758:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:22,887:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:22,889:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.
2024-08-31 18:12:22,889:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:22,889:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:22,889:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:22,889:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:22,889:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:22,890:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:22,999:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,001:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.
2024-08-31 18:12:23,001:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,001:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,001:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:12:23,001:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:23,001:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,001:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,095:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,098:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.
2024-08-31 18:12:23,098:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,098:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,098:INFO:[LightGBM] [Info] Total Bins 592
2024-08-31 18:12:23,098:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:23,098:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,098:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,260:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,263:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000653 seconds.
2024-08-31 18:12:23,263:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,263:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,263:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:23,263:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:23,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,263:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,384:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,385:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.
2024-08-31 18:12:23,385:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,385:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,387:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:23,387:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:23,387:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,387:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,492:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,494:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.
2024-08-31 18:12:23,494:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,494:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,494:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:23,494:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:23,495:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,495:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,612:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,614:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.
2024-08-31 18:12:23,614:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,614:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,614:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:12:23,614:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:23,614:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,614:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,723:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,724:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.
2024-08-31 18:12:23,725:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,725:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,725:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:23,725:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:23,725:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,725:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,848:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:23,850:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.
2024-08-31 18:12:23,850:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:23,851:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:23,851:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:23,851:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:23,851:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:23,851:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:23,998:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,000:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.
2024-08-31 18:12:24,000:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,000:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,000:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:24,000:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:24,000:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,000:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,110:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,112:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.
2024-08-31 18:12:24,112:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,112:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,113:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:24,113:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:24,113:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,113:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,222:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,224:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.
2024-08-31 18:12:24,224:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,224:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,224:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:24,224:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:24,225:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,225:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,365:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,368:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2024-08-31 18:12:24,368:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,368:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,368:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:24,368:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:24,368:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,368:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,470:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,472:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.
2024-08-31 18:12:24,472:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,472:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,472:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:24,472:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:24,473:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,473:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,569:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,572:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.
2024-08-31 18:12:24,572:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,572:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,572:INFO:[LightGBM] [Info] Total Bins 595
2024-08-31 18:12:24,572:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:24,572:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,572:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,675:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,676:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.
2024-08-31 18:12:24,676:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,676:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,676:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:24,676:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:24,677:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,677:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,806:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,809:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000914 seconds.
2024-08-31 18:12:24,809:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,809:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,809:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:24,809:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:24,810:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,810:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:24,945:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:24,948:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.
2024-08-31 18:12:24,948:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:24,948:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:24,948:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:12:24,948:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:24,949:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:24,949:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,057:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,059:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.
2024-08-31 18:12:25,059:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,060:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,060:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:25,060:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:25,060:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,060:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,159:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,161:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.
2024-08-31 18:12:25,161:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,161:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,161:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:25,161:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:25,161:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,161:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,252:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,253:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000479 seconds.
2024-08-31 18:12:25,254:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,254:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,254:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:25,254:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:25,254:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,255:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,362:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,365:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.
2024-08-31 18:12:25,365:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,365:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,365:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:25,365:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:25,365:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,365:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,503:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.
2024-08-31 18:12:25,505:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,505:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,505:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:25,505:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:25,505:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,505:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,615:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,616:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.
2024-08-31 18:12:25,616:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,616:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,616:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:25,617:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:25,617:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,617:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,723:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,725:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.
2024-08-31 18:12:25,725:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,725:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,725:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:25,725:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:25,725:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,725:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,829:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,831:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.
2024-08-31 18:12:25,831:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,831:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,831:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:25,831:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:25,831:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,831:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:25,925:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:25,927:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.
2024-08-31 18:12:25,927:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:25,927:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:25,927:INFO:[LightGBM] [Info] Total Bins 593
2024-08-31 18:12:25,927:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:25,928:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:25,928:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,035:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,038:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.
2024-08-31 18:12:26,038:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,038:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,038:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:26,038:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:26,038:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,038:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,172:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,174:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.
2024-08-31 18:12:26,174:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,174:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,174:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:26,174:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:26,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,174:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,282:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,284:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.
2024-08-31 18:12:26,284:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,284:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,284:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:26,284:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:26,284:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,284:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,391:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,393:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2024-08-31 18:12:26,393:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,393:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,393:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:26,393:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:26,393:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,394:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,521:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,523:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.
2024-08-31 18:12:26,523:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,523:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,523:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:12:26,523:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:26,523:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,524:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,621:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:26,622:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000483 seconds.
2024-08-31 18:12:26,622:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,623:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,623:INFO:[LightGBM] [Info] Total Bins 594
2024-08-31 18:12:26,623:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 10
2024-08-31 18:12:26,623:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:26,623:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:26,738:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:26,740:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.
2024-08-31 18:12:26,740:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,740:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,740:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:26,740:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:26,740:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:26,741:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:26,849:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:26,851:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.
2024-08-31 18:12:26,851:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,851:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,851:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:26,851:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:26,852:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:26,852:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:26,948:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:26,950:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.
2024-08-31 18:12:26,950:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:26,950:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:26,951:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:12:26,951:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:26,951:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:26,951:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,051:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,052:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.
2024-08-31 18:12:27,052:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,052:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,052:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:12:27,052:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:27,052:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,052:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,155:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,157:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.
2024-08-31 18:12:27,157:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,157:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,157:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:27,157:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:27,157:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,157:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,261:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,263:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000739 seconds.
2024-08-31 18:12:27,263:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,263:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,263:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:27,264:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:27,264:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,264:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,377:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,379:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.
2024-08-31 18:12:27,380:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,380:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,380:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:27,380:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:27,380:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,380:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,541:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,543:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.
2024-08-31 18:12:27,543:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,543:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,543:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:27,543:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:27,543:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,543:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,665:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,667:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.
2024-08-31 18:12:27,667:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,667:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,667:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:27,667:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:27,668:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,668:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,773:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,775:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.
2024-08-31 18:12:27,775:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,775:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,775:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:27,775:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:27,776:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,776:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:27,889:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:27,890:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2024-08-31 18:12:27,892:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:27,892:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:27,892:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:27,892:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:27,892:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:27,892:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,016:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,018:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.
2024-08-31 18:12:28,018:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,018:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,018:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:28,018:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:28,019:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,019:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,150:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,152:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.
2024-08-31 18:12:28,152:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,152:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,152:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:28,152:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:28,152:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,152:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,260:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,262:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.
2024-08-31 18:12:28,262:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,262:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,262:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:28,262:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:28,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,263:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,369:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,371:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.
2024-08-31 18:12:28,371:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,371:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,371:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:28,371:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:28,371:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,371:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,478:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,480:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.
2024-08-31 18:12:28,480:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,480:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,480:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:28,481:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:28,481:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,481:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,585:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,587:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.
2024-08-31 18:12:28,587:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,587:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,587:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:28,587:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:28,587:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,587:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,685:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,688:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.
2024-08-31 18:12:28,688:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,688:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,688:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:28,688:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:28,688:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,688:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,792:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,794:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.
2024-08-31 18:12:28,794:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,794:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,794:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:28,794:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:28,794:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,794:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:28,955:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:28,957:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000672 seconds.
2024-08-31 18:12:28,957:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:28,957:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:28,957:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:28,957:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 11
2024-08-31 18:12:28,957:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:28,957:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:29,117:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,120:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000658 seconds.
2024-08-31 18:12:29,120:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,120:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,120:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:29,121:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:29,121:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,121:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,283:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,286:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.
2024-08-31 18:12:29,286:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,286:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,286:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:29,286:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:29,286:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,287:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,410:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,413:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.
2024-08-31 18:12:29,413:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,413:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,413:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:29,413:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:29,414:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,414:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,535:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,537:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.
2024-08-31 18:12:29,537:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,538:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,538:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:29,538:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:29,538:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,538:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,643:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,645:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.
2024-08-31 18:12:29,645:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,645:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,645:INFO:[LightGBM] [Info] Total Bins 597
2024-08-31 18:12:29,645:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:29,645:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,645:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,811:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,813:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000675 seconds.
2024-08-31 18:12:29,814:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,814:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,814:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:29,814:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:29,814:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,814:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:29,950:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:29,953:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.
2024-08-31 18:12:29,953:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:29,953:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:29,953:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:29,953:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:29,953:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:29,953:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,141:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,143:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000786 seconds.
2024-08-31 18:12:30,144:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,144:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,144:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:30,144:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:30,144:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,144:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,272:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,274:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.
2024-08-31 18:12:30,274:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,274:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,274:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:12:30,274:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:30,276:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,276:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,440:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,442:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.
2024-08-31 18:12:30,443:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,443:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,443:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:30,443:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:30,443:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,443:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,553:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,555:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.
2024-08-31 18:12:30,555:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,555:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,557:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:30,557:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:30,557:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,557:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,744:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,747:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.
2024-08-31 18:12:30,747:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,747:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,747:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:30,747:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:30,747:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,747:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:30,862:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:30,864:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.
2024-08-31 18:12:30,864:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:30,864:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:30,864:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:30,864:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:30,865:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:30,865:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,001:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,002:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.
2024-08-31 18:12:31,003:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,003:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,003:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:31,003:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:31,003:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,003:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,106:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,107:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.
2024-08-31 18:12:31,107:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,107:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,107:INFO:[LightGBM] [Info] Total Bins 600
2024-08-31 18:12:31,107:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:31,108:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,108:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,217:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,219:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.
2024-08-31 18:12:31,219:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,219:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,219:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:31,219:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:31,220:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,220:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,367:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,369:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.
2024-08-31 18:12:31,369:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,369:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,369:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:31,369:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:31,370:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,370:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,511:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,513:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.
2024-08-31 18:12:31,513:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:31,513:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:12:31,513:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:31,513:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,513:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,634:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,636:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000667 seconds.
2024-08-31 18:12:31,636:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,636:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,636:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:31,637:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:31,637:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,637:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,817:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,819:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.
2024-08-31 18:12:31,820:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,820:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,820:INFO:[LightGBM] [Info] Total Bins 599
2024-08-31 18:12:31,820:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:31,820:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,820:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:31,945:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:31,947:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.
2024-08-31 18:12:31,947:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:31,947:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:31,947:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:31,947:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:31,947:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:31,947:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,079:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,082:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.
2024-08-31 18:12:32,082:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,082:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,083:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:32,083:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:32,083:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,083:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,241:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,243:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.
2024-08-31 18:12:32,243:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,243:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,243:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:32,243:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:32,244:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,244:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,349:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,351:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.
2024-08-31 18:12:32,351:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,351:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,351:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:32,351:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:32,351:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,351:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,446:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,448:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.
2024-08-31 18:12:32,448:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,448:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,448:INFO:[LightGBM] [Info] Total Bins 598
2024-08-31 18:12:32,448:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:32,448:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,448:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,617:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,620:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000954 seconds.
2024-08-31 18:12:32,620:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,620:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,621:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:32,621:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:32,621:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,621:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,762:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,763:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.
2024-08-31 18:12:32,763:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,763:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,763:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:32,763:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:32,763:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,763:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:32,868:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:32,870:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.
2024-08-31 18:12:32,870:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:32,870:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:32,870:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:32,870:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:32,870:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:32,870:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:33,084:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:33,086:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000821 seconds.
2024-08-31 18:12:33,086:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,086:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,086:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:33,086:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:33,086:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:33,086:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:33,201:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:33,203:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.
2024-08-31 18:12:33,203:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,203:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,203:INFO:[LightGBM] [Info] Total Bins 634
2024-08-31 18:12:33,203:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 11
2024-08-31 18:12:33,203:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:33,203:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:33,399:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:33,401:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000832 seconds.
2024-08-31 18:12:33,401:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,401:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,401:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:33,401:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:33,402:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:33,402:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:33,532:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:33,534:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.
2024-08-31 18:12:33,534:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,534:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,534:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:33,534:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:33,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:33,535:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:33,645:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:33,648:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.
2024-08-31 18:12:33,648:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,648:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,648:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:12:33,648:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:33,648:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:33,648:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:33,758:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:33,760:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.
2024-08-31 18:12:33,760:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,760:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,760:INFO:[LightGBM] [Info] Total Bins 605
2024-08-31 18:12:33,760:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:33,760:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:33,760:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:33,880:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:33,882:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.
2024-08-31 18:12:33,882:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:33,882:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:33,882:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:33,882:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:33,883:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:33,883:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,031:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,033:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.
2024-08-31 18:12:34,033:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,033:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,033:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:34,033:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:34,034:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,034:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,176:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,177:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.
2024-08-31 18:12:34,177:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,177:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,177:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:34,177:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:34,178:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,178:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,290:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,292:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000833 seconds.
2024-08-31 18:12:34,292:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,292:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,292:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:34,293:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:34,293:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,293:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,419:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,421:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.
2024-08-31 18:12:34,421:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,421:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,421:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:34,421:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:34,421:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,421:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,542:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,544:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.
2024-08-31 18:12:34,544:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,544:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,544:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:34,544:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:34,544:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,544:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,672:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,674:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000658 seconds.
2024-08-31 18:12:34,674:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,674:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,674:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:34,674:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:34,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,675:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,792:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,794:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.
2024-08-31 18:12:34,794:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,794:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,794:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:34,795:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:34,795:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,795:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:34,971:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:34,973:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000680 seconds.
2024-08-31 18:12:34,974:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:34,974:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:34,974:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:34,974:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:34,974:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:34,974:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:35,111:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:35,114:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.
2024-08-31 18:12:35,114:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:35,114:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:35,114:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:35,114:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:35,115:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:35,115:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:35,282:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:35,284:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000671 seconds.
2024-08-31 18:12:35,284:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:35,284:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:35,284:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:35,285:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:35,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:35,285:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:35,410:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:35,411:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.
2024-08-31 18:12:35,411:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:35,412:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:35,412:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:35,412:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 12
2024-08-31 18:12:35,412:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:35,412:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:35,607:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:35,609:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.
2024-08-31 18:12:35,610:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:35,610:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:35,610:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:35,610:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:35,610:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:35,611:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:35,830:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:35,832:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000871 seconds.
2024-08-31 18:12:35,832:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:35,832:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:35,832:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:35,833:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:35,833:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:35,833:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,013:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,015:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000849 seconds.
2024-08-31 18:12:36,015:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,015:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,015:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:36,017:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:36,017:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,017:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,174:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,176:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.
2024-08-31 18:12:36,176:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,176:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,176:INFO:[LightGBM] [Info] Total Bins 637
2024-08-31 18:12:36,176:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:36,177:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,177:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,338:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.
2024-08-31 18:12:36,341:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,341:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,341:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:36,341:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:36,341:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,341:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,485:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,489:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.
2024-08-31 18:12:36,489:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,489:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,489:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:36,489:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:36,489:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,489:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,653:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,655:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.
2024-08-31 18:12:36,656:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,656:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,656:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:36,656:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:36,656:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,656:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:36,805:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:36,808:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.
2024-08-31 18:12:36,808:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:36,808:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:36,808:INFO:[LightGBM] [Info] Total Bins 606
2024-08-31 18:12:36,808:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:36,808:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:36,808:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,010:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,012:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000813 seconds.
2024-08-31 18:12:37,013:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,013:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,013:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:37,013:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:37,013:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,014:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,176:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,178:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.
2024-08-31 18:12:37,178:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,178:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,178:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:37,178:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:37,178:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,178:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,323:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,325:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.
2024-08-31 18:12:37,325:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,325:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,325:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:37,325:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:37,325:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,325:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,478:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,480:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.
2024-08-31 18:12:37,480:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,480:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,480:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:37,480:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:37,481:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,481:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,642:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,644:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.
2024-08-31 18:12:37,644:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,644:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,644:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:37,644:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:37,644:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,644:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:37,806:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:37,808:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000827 seconds.
2024-08-31 18:12:37,809:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:37,809:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:37,809:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:37,809:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:37,809:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:37,809:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:38,059:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:38,061:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.
2024-08-31 18:12:38,061:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:38,062:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:38,062:INFO:[LightGBM] [Info] Total Bins 641
2024-08-31 18:12:38,062:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:38,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:38,062:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:38,185:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:38,189:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.
2024-08-31 18:12:38,189:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:38,189:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:38,189:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:38,189:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:38,189:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:38,189:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:38,353:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:38,354:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.
2024-08-31 18:12:38,355:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:38,355:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:38,355:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:38,355:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:38,355:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:38,355:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:38,585:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:38,587:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.
2024-08-31 18:12:38,588:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:38,588:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:38,588:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:38,588:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:38,588:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:38,588:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:38,807:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:38,809:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000973 seconds.
2024-08-31 18:12:38,809:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:38,809:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:38,809:INFO:[LightGBM] [Info] Total Bins 640
2024-08-31 18:12:38,809:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:38,809:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:38,809:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,088:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:39,090:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.
2024-08-31 18:12:39,090:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:39,090:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:39,090:INFO:[LightGBM] [Info] Total Bins 638
2024-08-31 18:12:39,090:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:39,090:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:39,090:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,342:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:39,345:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001135 seconds.
2024-08-31 18:12:39,345:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:39,345:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:39,345:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:39,345:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:39,347:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:39,347:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,525:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:39,528:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001681 seconds.
2024-08-31 18:12:39,529:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 18:12:39,530:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:39,530:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:39,531:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:39,531:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,723:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:39,725:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.
2024-08-31 18:12:39,725:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:39,725:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:39,725:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:39,725:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:39,725:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:39,726:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,839:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:39,842:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.
2024-08-31 18:12:39,842:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:39,842:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:39,842:INFO:[LightGBM] [Info] Total Bins 639
2024-08-31 18:12:39,842:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 12
2024-08-31 18:12:39,843:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:39,843:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:39,975:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:39,976:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.
2024-08-31 18:12:39,976:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:39,976:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:39,977:INFO:[LightGBM] [Info] Total Bins 658
2024-08-31 18:12:39,977:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:39,977:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:39,977:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,087:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,089:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.
2024-08-31 18:12:40,089:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,089:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,089:INFO:[LightGBM] [Info] Total Bins 647
2024-08-31 18:12:40,089:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:40,089:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,089:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,197:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,199:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000653 seconds.
2024-08-31 18:12:40,199:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,199:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,199:INFO:[LightGBM] [Info] Total Bins 607
2024-08-31 18:12:40,199:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:40,200:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,200:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,324:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,326:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000736 seconds.
2024-08-31 18:12:40,326:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,326:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,327:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:40,327:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:40,327:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,327:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,472:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,476:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001771 seconds.
2024-08-31 18:12:40,476:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,476:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,476:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:40,476:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:40,476:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,476:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,668:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,671:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.
2024-08-31 18:12:40,671:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,671:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,671:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:40,671:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:40,672:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,672:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:40,869:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:40,871:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.
2024-08-31 18:12:40,871:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:40,871:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:40,871:INFO:[LightGBM] [Info] Total Bins 657
2024-08-31 18:12:40,872:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:40,873:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:40,873:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,041:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:41,042:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.
2024-08-31 18:12:41,044:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,044:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,044:INFO:[LightGBM] [Info] Total Bins 655
2024-08-31 18:12:41,044:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:41,044:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:41,044:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,233:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:41,235:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.
2024-08-31 18:12:41,235:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,235:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,236:INFO:[LightGBM] [Info] Total Bins 649
2024-08-31 18:12:41,236:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:41,236:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:41,236:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,415:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:41,418:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.
2024-08-31 18:12:41,418:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,419:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,419:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:41,419:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 15
2024-08-31 18:12:41,420:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:41,420:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,558:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:41,559:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000658 seconds.
2024-08-31 18:12:41,559:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,559:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,560:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:41,560:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 14
2024-08-31 18:12:41,560:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:41,560:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,664:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12450
2024-08-31 18:12:41,667:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.
2024-08-31 18:12:41,667:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,667:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,667:INFO:[LightGBM] [Info] Total Bins 642
2024-08-31 18:12:41,667:INFO:[LightGBM] [Info] Number of data points in the train set: 16401, number of used features: 13
2024-08-31 18:12:41,667:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240900 -> initscore=-1.147752
2024-08-31 18:12:41,667:INFO:[LightGBM] [Info] Start training from score -1.147752
2024-08-31 18:12:41,775:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:41,778:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.
2024-08-31 18:12:41,778:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:41,778:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:41,778:INFO:[LightGBM] [Info] Total Bins 656
2024-08-31 18:12:41,778:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:41,778:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:41,778:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,001:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,003:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.
2024-08-31 18:12:42,003:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,003:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,004:INFO:[LightGBM] [Info] Total Bins 645
2024-08-31 18:12:42,004:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:42,004:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,004:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,119:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,120:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.
2024-08-31 18:12:42,120:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,121:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,121:INFO:[LightGBM] [Info] Total Bins 643
2024-08-31 18:12:42,121:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:42,121:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,121:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,287:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,289:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.
2024-08-31 18:12:42,289:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,289:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,290:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:42,290:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:42,290:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,290:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,414:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,416:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.
2024-08-31 18:12:42,416:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,416:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,416:INFO:[LightGBM] [Info] Total Bins 648
2024-08-31 18:12:42,416:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 14
2024-08-31 18:12:42,417:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,417:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,528:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,530:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.
2024-08-31 18:12:42,530:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,530:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,530:INFO:[LightGBM] [Info] Total Bins 646
2024-08-31 18:12:42,530:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 13
2024-08-31 18:12:42,530:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,530:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:42,641:INFO:[LightGBM] [Info] Number of positive: 3951, number of negative: 12451
2024-08-31 18:12:42,642:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.
2024-08-31 18:12:42,642:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:12:42,642:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:12:42,642:INFO:[LightGBM] [Info] Total Bins 659
2024-08-31 18:12:42,643:INFO:[LightGBM] [Info] Number of data points in the train set: 16402, number of used features: 15
2024-08-31 18:12:42,643:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240885 -> initscore=-1.147832
2024-08-31 18:12:42,643:INFO:[LightGBM] [Info] Start training from score -1.147832
2024-08-31 18:12:43,739:INFO:PyCaret ClassificationExperiment
2024-08-31 18:12:43,739:INFO:Logging name: clf-default-name
2024-08-31 18:12:43,739:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:12:43,739:INFO:version 3.3.2
2024-08-31 18:12:43,739:INFO:Initializing setup()
2024-08-31 18:12:43,739:INFO:self.USI: ad4c
2024-08-31 18:12:43,739:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:12:43,739:INFO:Checking environment
2024-08-31 18:12:43,739:INFO:python_version: 3.11.9
2024-08-31 18:12:43,739:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:12:43,739:INFO:machine: AMD64
2024-08-31 18:12:43,740:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:12:43,748:INFO:Memory: svmem(total=16867028992, available=2957668352, percent=82.5, used=13909360640, free=2957668352)
2024-08-31 18:12:43,748:INFO:Physical Core: 6
2024-08-31 18:12:43,748:INFO:Logical Core: 12
2024-08-31 18:12:43,748:INFO:Checking libraries
2024-08-31 18:12:43,748:INFO:System:
2024-08-31 18:12:43,748:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:12:43,748:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:12:43,748:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:12:43,748:INFO:PyCaret required dependencies:
2024-08-31 18:12:43,748:INFO:                 pip: 23.2.1
2024-08-31 18:12:43,750:INFO:          setuptools: 67.8.0
2024-08-31 18:12:43,750:INFO:             pycaret: 3.3.2
2024-08-31 18:12:43,750:INFO:             IPython: 8.14.0
2024-08-31 18:12:43,750:INFO:          ipywidgets: 8.1.5
2024-08-31 18:12:43,750:INFO:                tqdm: 4.66.5
2024-08-31 18:12:43,750:INFO:               numpy: 1.24.3
2024-08-31 18:12:43,750:INFO:              pandas: 2.0.3
2024-08-31 18:12:43,750:INFO:              jinja2: 3.1.4
2024-08-31 18:12:43,750:INFO:               scipy: 1.10.1
2024-08-31 18:12:43,750:INFO:              joblib: 1.2.0
2024-08-31 18:12:43,750:INFO:             sklearn: 1.4.2
2024-08-31 18:12:43,750:INFO:                pyod: 2.0.1
2024-08-31 18:12:43,750:INFO:            imblearn: 0.12.3
2024-08-31 18:12:43,750:INFO:   category_encoders: 2.6.3
2024-08-31 18:12:43,750:INFO:            lightgbm: 4.5.0
2024-08-31 18:12:43,750:INFO:               numba: 0.60.0
2024-08-31 18:12:43,751:INFO:            requests: 2.32.3
2024-08-31 18:12:43,751:INFO:          matplotlib: 3.7.1
2024-08-31 18:12:43,751:INFO:          scikitplot: 0.3.7
2024-08-31 18:12:43,751:INFO:         yellowbrick: 1.5
2024-08-31 18:12:43,751:INFO:              plotly: 5.16.1
2024-08-31 18:12:43,751:INFO:    plotly-resampler: Not installed
2024-08-31 18:12:43,751:INFO:             kaleido: 0.2.1
2024-08-31 18:12:43,751:INFO:           schemdraw: 0.15
2024-08-31 18:12:43,751:INFO:         statsmodels: 0.14.2
2024-08-31 18:12:43,751:INFO:              sktime: 0.26.0
2024-08-31 18:12:43,751:INFO:               tbats: 1.1.3
2024-08-31 18:12:43,751:INFO:            pmdarima: 2.0.4
2024-08-31 18:12:43,751:INFO:              psutil: 5.9.0
2024-08-31 18:12:43,751:INFO:          markupsafe: 2.1.3
2024-08-31 18:12:43,751:INFO:             pickle5: Not installed
2024-08-31 18:12:43,751:INFO:         cloudpickle: 3.0.0
2024-08-31 18:12:43,751:INFO:         deprecation: 2.1.0
2024-08-31 18:12:43,751:INFO:              xxhash: 3.5.0
2024-08-31 18:12:43,751:INFO:           wurlitzer: Not installed
2024-08-31 18:12:43,751:INFO:PyCaret optional dependencies:
2024-08-31 18:12:43,751:INFO:                shap: Not installed
2024-08-31 18:12:43,751:INFO:           interpret: Not installed
2024-08-31 18:12:43,751:INFO:                umap: Not installed
2024-08-31 18:12:43,751:INFO:     ydata_profiling: Not installed
2024-08-31 18:12:43,751:INFO:  explainerdashboard: Not installed
2024-08-31 18:12:43,751:INFO:             autoviz: Not installed
2024-08-31 18:12:43,751:INFO:           fairlearn: Not installed
2024-08-31 18:12:43,751:INFO:          deepchecks: Not installed
2024-08-31 18:12:43,751:INFO:             xgboost: 2.0.2
2024-08-31 18:12:43,751:INFO:            catboost: 1.2.5
2024-08-31 18:12:43,752:INFO:              kmodes: Not installed
2024-08-31 18:12:43,752:INFO:             mlxtend: Not installed
2024-08-31 18:12:43,752:INFO:       statsforecast: Not installed
2024-08-31 18:12:43,752:INFO:        tune_sklearn: Not installed
2024-08-31 18:12:43,752:INFO:                 ray: Not installed
2024-08-31 18:12:43,752:INFO:            hyperopt: Not installed
2024-08-31 18:12:43,752:INFO:              optuna: Not installed
2024-08-31 18:12:43,752:INFO:               skopt: Not installed
2024-08-31 18:12:43,752:INFO:              mlflow: Not installed
2024-08-31 18:12:43,752:INFO:              gradio: 4.41.0
2024-08-31 18:12:43,752:INFO:             fastapi: 0.112.1
2024-08-31 18:12:43,752:INFO:             uvicorn: 0.30.6
2024-08-31 18:12:43,752:INFO:              m2cgen: Not installed
2024-08-31 18:12:43,752:INFO:           evidently: Not installed
2024-08-31 18:12:43,752:INFO:               fugue: Not installed
2024-08-31 18:12:43,752:INFO:           streamlit: Not installed
2024-08-31 18:12:43,752:INFO:             prophet: Not installed
2024-08-31 18:12:43,752:INFO:None
2024-08-31 18:12:43,752:INFO:Set up data.
2024-08-31 18:12:43,764:INFO:Set up folding strategy.
2024-08-31 18:12:43,765:INFO:Set up train/test split.
2024-08-31 18:12:43,781:INFO:Set up index.
2024-08-31 18:12:43,782:INFO:Assigning column types.
2024-08-31 18:12:43,795:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:12:43,861:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:12:43,862:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:12:43,887:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:43,890:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:43,927:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:12:43,928:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:12:43,950:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:43,953:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:43,953:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:12:43,988:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:12:44,011:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,013:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,049:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:12:44,072:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,074:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,074:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:12:44,134:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,136:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,197:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,199:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,201:INFO:Preparing preprocessing pipeline...
2024-08-31 18:12:44,202:INFO:Set up simple imputation.
2024-08-31 18:12:44,204:INFO:Set up column name cleaning.
2024-08-31 18:12:44,245:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:12:44,248:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:12:44,249:INFO:Creating final display dataframe.
2024-08-31 18:12:44,374:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              ad4c
2024-08-31 18:12:44,438:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,440:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,502:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:12:44,505:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:12:44,507:INFO:setup() successfully completed in 0.77s...............
2024-08-31 18:12:44,507:INFO:Initializing compare_models()
2024-08-31 18:12:44,507:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 18:12:44,507:INFO:Checking exceptions
2024-08-31 18:12:44,517:INFO:Preparing display monitor
2024-08-31 18:12:44,537:INFO:Initializing Logistic Regression
2024-08-31 18:12:44,537:INFO:Total runtime is 0.0 minutes
2024-08-31 18:12:44,541:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:44,541:INFO:Initializing create_model()
2024-08-31 18:12:44,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:44,541:INFO:Checking exceptions
2024-08-31 18:12:44,541:INFO:Importing libraries
2024-08-31 18:12:44,541:INFO:Copying training dataset
2024-08-31 18:12:44,556:INFO:Defining folds
2024-08-31 18:12:44,556:INFO:Declaring metric variables
2024-08-31 18:12:44,560:INFO:Importing untrained model
2024-08-31 18:12:44,564:INFO:Logistic Regression Imported successfully
2024-08-31 18:12:44,571:INFO:Starting cross validation
2024-08-31 18:12:44,572:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:45,816:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:12:45,852:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:12:45,877:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:12:45,891:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:12:45,900:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:12:45,925:INFO:Calculating mean and std
2024-08-31 18:12:45,926:INFO:Creating metrics dataframe
2024-08-31 18:12:45,927:INFO:Uploading results into container
2024-08-31 18:12:45,927:INFO:Uploading model into container now
2024-08-31 18:12:45,928:INFO:_master_model_container: 1
2024-08-31 18:12:45,928:INFO:_display_container: 2
2024-08-31 18:12:45,928:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 18:12:45,928:INFO:create_model() successfully completed......................................
2024-08-31 18:12:46,131:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:46,131:INFO:Creating metrics dataframe
2024-08-31 18:12:46,141:INFO:Initializing K Neighbors Classifier
2024-08-31 18:12:46,141:INFO:Total runtime is 0.026730926831563313 minutes
2024-08-31 18:12:46,145:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:46,145:INFO:Initializing create_model()
2024-08-31 18:12:46,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:46,145:INFO:Checking exceptions
2024-08-31 18:12:46,147:INFO:Importing libraries
2024-08-31 18:12:46,147:INFO:Copying training dataset
2024-08-31 18:12:46,165:INFO:Defining folds
2024-08-31 18:12:46,165:INFO:Declaring metric variables
2024-08-31 18:12:46,172:INFO:Importing untrained model
2024-08-31 18:12:46,179:INFO:K Neighbors Classifier Imported successfully
2024-08-31 18:12:46,191:INFO:Starting cross validation
2024-08-31 18:12:46,192:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:46,579:INFO:Calculating mean and std
2024-08-31 18:12:46,581:INFO:Creating metrics dataframe
2024-08-31 18:12:46,582:INFO:Uploading results into container
2024-08-31 18:12:46,583:INFO:Uploading model into container now
2024-08-31 18:12:46,584:INFO:_master_model_container: 2
2024-08-31 18:12:46,584:INFO:_display_container: 2
2024-08-31 18:12:46,584:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 18:12:46,585:INFO:create_model() successfully completed......................................
2024-08-31 18:12:46,781:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:46,781:INFO:Creating metrics dataframe
2024-08-31 18:12:46,787:INFO:Initializing Naive Bayes
2024-08-31 18:12:46,787:INFO:Total runtime is 0.037501351038614905 minutes
2024-08-31 18:12:46,790:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:46,790:INFO:Initializing create_model()
2024-08-31 18:12:46,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:46,791:INFO:Checking exceptions
2024-08-31 18:12:46,791:INFO:Importing libraries
2024-08-31 18:12:46,791:INFO:Copying training dataset
2024-08-31 18:12:46,802:INFO:Defining folds
2024-08-31 18:12:46,802:INFO:Declaring metric variables
2024-08-31 18:12:46,805:INFO:Importing untrained model
2024-08-31 18:12:46,808:INFO:Naive Bayes Imported successfully
2024-08-31 18:12:46,815:INFO:Starting cross validation
2024-08-31 18:12:46,815:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:46,913:INFO:Calculating mean and std
2024-08-31 18:12:46,914:INFO:Creating metrics dataframe
2024-08-31 18:12:46,915:INFO:Uploading results into container
2024-08-31 18:12:46,917:INFO:Uploading model into container now
2024-08-31 18:12:46,918:INFO:_master_model_container: 3
2024-08-31 18:12:46,918:INFO:_display_container: 2
2024-08-31 18:12:46,918:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 18:12:46,918:INFO:create_model() successfully completed......................................
2024-08-31 18:12:47,141:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:47,141:INFO:Creating metrics dataframe
2024-08-31 18:12:47,147:INFO:Initializing Decision Tree Classifier
2024-08-31 18:12:47,147:INFO:Total runtime is 0.04349575837453206 minutes
2024-08-31 18:12:47,150:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:47,150:INFO:Initializing create_model()
2024-08-31 18:12:47,151:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:47,151:INFO:Checking exceptions
2024-08-31 18:12:47,151:INFO:Importing libraries
2024-08-31 18:12:47,151:INFO:Copying training dataset
2024-08-31 18:12:47,161:INFO:Defining folds
2024-08-31 18:12:47,162:INFO:Declaring metric variables
2024-08-31 18:12:47,165:INFO:Importing untrained model
2024-08-31 18:12:47,167:INFO:Decision Tree Classifier Imported successfully
2024-08-31 18:12:47,173:INFO:Starting cross validation
2024-08-31 18:12:47,174:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:47,342:INFO:Calculating mean and std
2024-08-31 18:12:47,343:INFO:Creating metrics dataframe
2024-08-31 18:12:47,345:INFO:Uploading results into container
2024-08-31 18:12:47,345:INFO:Uploading model into container now
2024-08-31 18:12:47,347:INFO:_master_model_container: 4
2024-08-31 18:12:47,347:INFO:_display_container: 2
2024-08-31 18:12:47,347:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 18:12:47,347:INFO:create_model() successfully completed......................................
2024-08-31 18:12:47,534:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:47,534:INFO:Creating metrics dataframe
2024-08-31 18:12:47,538:INFO:Initializing SVM - Linear Kernel
2024-08-31 18:12:47,540:INFO:Total runtime is 0.05004332860310872 minutes
2024-08-31 18:12:47,542:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:47,542:INFO:Initializing create_model()
2024-08-31 18:12:47,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:47,543:INFO:Checking exceptions
2024-08-31 18:12:47,543:INFO:Importing libraries
2024-08-31 18:12:47,543:INFO:Copying training dataset
2024-08-31 18:12:47,553:INFO:Defining folds
2024-08-31 18:12:47,554:INFO:Declaring metric variables
2024-08-31 18:12:47,557:INFO:Importing untrained model
2024-08-31 18:12:47,560:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 18:12:47,565:INFO:Starting cross validation
2024-08-31 18:12:47,566:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:47,822:INFO:Calculating mean and std
2024-08-31 18:12:47,824:INFO:Creating metrics dataframe
2024-08-31 18:12:47,825:INFO:Uploading results into container
2024-08-31 18:12:47,825:INFO:Uploading model into container now
2024-08-31 18:12:47,827:INFO:_master_model_container: 5
2024-08-31 18:12:47,827:INFO:_display_container: 2
2024-08-31 18:12:47,828:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 18:12:47,828:INFO:create_model() successfully completed......................................
2024-08-31 18:12:48,030:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:48,031:INFO:Creating metrics dataframe
2024-08-31 18:12:48,037:INFO:Initializing Ridge Classifier
2024-08-31 18:12:48,037:INFO:Total runtime is 0.05833966732025146 minutes
2024-08-31 18:12:48,040:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:48,040:INFO:Initializing create_model()
2024-08-31 18:12:48,040:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:48,040:INFO:Checking exceptions
2024-08-31 18:12:48,040:INFO:Importing libraries
2024-08-31 18:12:48,040:INFO:Copying training dataset
2024-08-31 18:12:48,054:INFO:Defining folds
2024-08-31 18:12:48,054:INFO:Declaring metric variables
2024-08-31 18:12:48,057:INFO:Importing untrained model
2024-08-31 18:12:48,060:INFO:Ridge Classifier Imported successfully
2024-08-31 18:12:48,065:INFO:Starting cross validation
2024-08-31 18:12:48,066:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:48,143:INFO:Calculating mean and std
2024-08-31 18:12:48,143:INFO:Creating metrics dataframe
2024-08-31 18:12:48,145:INFO:Uploading results into container
2024-08-31 18:12:48,145:INFO:Uploading model into container now
2024-08-31 18:12:48,145:INFO:_master_model_container: 6
2024-08-31 18:12:48,145:INFO:_display_container: 2
2024-08-31 18:12:48,145:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 18:12:48,145:INFO:create_model() successfully completed......................................
2024-08-31 18:12:48,352:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:48,352:INFO:Creating metrics dataframe
2024-08-31 18:12:48,359:INFO:Initializing Random Forest Classifier
2024-08-31 18:12:48,359:INFO:Total runtime is 0.06370603640874227 minutes
2024-08-31 18:12:48,361:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:48,362:INFO:Initializing create_model()
2024-08-31 18:12:48,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:48,362:INFO:Checking exceptions
2024-08-31 18:12:48,362:INFO:Importing libraries
2024-08-31 18:12:48,362:INFO:Copying training dataset
2024-08-31 18:12:48,374:INFO:Defining folds
2024-08-31 18:12:48,374:INFO:Declaring metric variables
2024-08-31 18:12:48,377:INFO:Importing untrained model
2024-08-31 18:12:48,380:INFO:Random Forest Classifier Imported successfully
2024-08-31 18:12:48,385:INFO:Starting cross validation
2024-08-31 18:12:48,386:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:49,711:INFO:Calculating mean and std
2024-08-31 18:12:49,713:INFO:Creating metrics dataframe
2024-08-31 18:12:49,715:INFO:Uploading results into container
2024-08-31 18:12:49,716:INFO:Uploading model into container now
2024-08-31 18:12:49,716:INFO:_master_model_container: 7
2024-08-31 18:12:49,716:INFO:_display_container: 2
2024-08-31 18:12:49,716:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 18:12:49,716:INFO:create_model() successfully completed......................................
2024-08-31 18:12:49,908:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:49,908:INFO:Creating metrics dataframe
2024-08-31 18:12:49,914:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 18:12:49,915:INFO:Total runtime is 0.08963404893875122 minutes
2024-08-31 18:12:49,917:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:49,917:INFO:Initializing create_model()
2024-08-31 18:12:49,917:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:49,918:INFO:Checking exceptions
2024-08-31 18:12:49,918:INFO:Importing libraries
2024-08-31 18:12:49,918:INFO:Copying training dataset
2024-08-31 18:12:49,930:INFO:Defining folds
2024-08-31 18:12:49,930:INFO:Declaring metric variables
2024-08-31 18:12:49,933:INFO:Importing untrained model
2024-08-31 18:12:49,935:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 18:12:49,939:INFO:Starting cross validation
2024-08-31 18:12:49,940:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:50,010:INFO:Calculating mean and std
2024-08-31 18:12:50,011:INFO:Creating metrics dataframe
2024-08-31 18:12:50,012:INFO:Uploading results into container
2024-08-31 18:12:50,013:INFO:Uploading model into container now
2024-08-31 18:12:50,013:INFO:_master_model_container: 8
2024-08-31 18:12:50,013:INFO:_display_container: 2
2024-08-31 18:12:50,013:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 18:12:50,013:INFO:create_model() successfully completed......................................
2024-08-31 18:12:50,196:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:50,196:INFO:Creating metrics dataframe
2024-08-31 18:12:50,202:INFO:Initializing Ada Boost Classifier
2024-08-31 18:12:50,203:INFO:Total runtime is 0.09442311922709147 minutes
2024-08-31 18:12:50,206:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:50,206:INFO:Initializing create_model()
2024-08-31 18:12:50,206:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:50,206:INFO:Checking exceptions
2024-08-31 18:12:50,206:INFO:Importing libraries
2024-08-31 18:12:50,206:INFO:Copying training dataset
2024-08-31 18:12:50,218:INFO:Defining folds
2024-08-31 18:12:50,218:INFO:Declaring metric variables
2024-08-31 18:12:50,221:INFO:Importing untrained model
2024-08-31 18:12:50,224:INFO:Ada Boost Classifier Imported successfully
2024-08-31 18:12:50,229:INFO:Starting cross validation
2024-08-31 18:12:50,229:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:50,255:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:12:50,259:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:12:50,263:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:12:50,265:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:12:50,270:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:12:50,864:INFO:Calculating mean and std
2024-08-31 18:12:50,865:INFO:Creating metrics dataframe
2024-08-31 18:12:50,867:INFO:Uploading results into container
2024-08-31 18:12:50,868:INFO:Uploading model into container now
2024-08-31 18:12:50,868:INFO:_master_model_container: 9
2024-08-31 18:12:50,868:INFO:_display_container: 2
2024-08-31 18:12:50,869:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 18:12:50,869:INFO:create_model() successfully completed......................................
2024-08-31 18:12:51,101:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:51,101:INFO:Creating metrics dataframe
2024-08-31 18:12:51,110:INFO:Initializing Gradient Boosting Classifier
2024-08-31 18:12:51,110:INFO:Total runtime is 0.10954841375350952 minutes
2024-08-31 18:12:51,113:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:51,114:INFO:Initializing create_model()
2024-08-31 18:12:51,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:51,114:INFO:Checking exceptions
2024-08-31 18:12:51,115:INFO:Importing libraries
2024-08-31 18:12:51,115:INFO:Copying training dataset
2024-08-31 18:12:51,134:INFO:Defining folds
2024-08-31 18:12:51,134:INFO:Declaring metric variables
2024-08-31 18:12:51,138:INFO:Importing untrained model
2024-08-31 18:12:51,141:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 18:12:51,147:INFO:Starting cross validation
2024-08-31 18:12:51,148:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:53,249:INFO:Calculating mean and std
2024-08-31 18:12:53,250:INFO:Creating metrics dataframe
2024-08-31 18:12:53,252:INFO:Uploading results into container
2024-08-31 18:12:53,253:INFO:Uploading model into container now
2024-08-31 18:12:53,253:INFO:_master_model_container: 10
2024-08-31 18:12:53,253:INFO:_display_container: 2
2024-08-31 18:12:53,254:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 18:12:53,254:INFO:create_model() successfully completed......................................
2024-08-31 18:12:53,445:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:53,445:INFO:Creating metrics dataframe
2024-08-31 18:12:53,452:INFO:Initializing Linear Discriminant Analysis
2024-08-31 18:12:53,452:INFO:Total runtime is 0.14858352343241374 minutes
2024-08-31 18:12:53,455:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:53,455:INFO:Initializing create_model()
2024-08-31 18:12:53,456:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:53,456:INFO:Checking exceptions
2024-08-31 18:12:53,456:INFO:Importing libraries
2024-08-31 18:12:53,456:INFO:Copying training dataset
2024-08-31 18:12:53,468:INFO:Defining folds
2024-08-31 18:12:53,468:INFO:Declaring metric variables
2024-08-31 18:12:53,471:INFO:Importing untrained model
2024-08-31 18:12:53,474:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 18:12:53,478:INFO:Starting cross validation
2024-08-31 18:12:53,479:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:53,571:INFO:Calculating mean and std
2024-08-31 18:12:53,572:INFO:Creating metrics dataframe
2024-08-31 18:12:53,574:INFO:Uploading results into container
2024-08-31 18:12:53,574:INFO:Uploading model into container now
2024-08-31 18:12:53,575:INFO:_master_model_container: 11
2024-08-31 18:12:53,575:INFO:_display_container: 2
2024-08-31 18:12:53,575:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 18:12:53,575:INFO:create_model() successfully completed......................................
2024-08-31 18:12:53,753:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:53,753:INFO:Creating metrics dataframe
2024-08-31 18:12:53,761:INFO:Initializing Extra Trees Classifier
2024-08-31 18:12:53,762:INFO:Total runtime is 0.15374972422917685 minutes
2024-08-31 18:12:53,764:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:53,765:INFO:Initializing create_model()
2024-08-31 18:12:53,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:53,765:INFO:Checking exceptions
2024-08-31 18:12:53,765:INFO:Importing libraries
2024-08-31 18:12:53,765:INFO:Copying training dataset
2024-08-31 18:12:53,779:INFO:Defining folds
2024-08-31 18:12:53,779:INFO:Declaring metric variables
2024-08-31 18:12:53,782:INFO:Importing untrained model
2024-08-31 18:12:53,785:INFO:Extra Trees Classifier Imported successfully
2024-08-31 18:12:53,790:INFO:Starting cross validation
2024-08-31 18:12:53,791:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:54,875:INFO:Calculating mean and std
2024-08-31 18:12:54,877:INFO:Creating metrics dataframe
2024-08-31 18:12:54,878:INFO:Uploading results into container
2024-08-31 18:12:54,879:INFO:Uploading model into container now
2024-08-31 18:12:54,879:INFO:_master_model_container: 12
2024-08-31 18:12:54,880:INFO:_display_container: 2
2024-08-31 18:12:54,880:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 18:12:54,880:INFO:create_model() successfully completed......................................
2024-08-31 18:12:55,075:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:55,075:INFO:Creating metrics dataframe
2024-08-31 18:12:55,083:INFO:Initializing Extreme Gradient Boosting
2024-08-31 18:12:55,083:INFO:Total runtime is 0.17576442162195843 minutes
2024-08-31 18:12:55,086:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:55,086:INFO:Initializing create_model()
2024-08-31 18:12:55,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:55,087:INFO:Checking exceptions
2024-08-31 18:12:55,087:INFO:Importing libraries
2024-08-31 18:12:55,087:INFO:Copying training dataset
2024-08-31 18:12:55,098:INFO:Defining folds
2024-08-31 18:12:55,098:INFO:Declaring metric variables
2024-08-31 18:12:55,101:INFO:Importing untrained model
2024-08-31 18:12:55,104:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 18:12:55,109:INFO:Starting cross validation
2024-08-31 18:12:55,109:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:55,395:INFO:Calculating mean and std
2024-08-31 18:12:55,397:INFO:Creating metrics dataframe
2024-08-31 18:12:55,398:INFO:Uploading results into container
2024-08-31 18:12:55,399:INFO:Uploading model into container now
2024-08-31 18:12:55,399:INFO:_master_model_container: 13
2024-08-31 18:12:55,399:INFO:_display_container: 2
2024-08-31 18:12:55,400:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 18:12:55,400:INFO:create_model() successfully completed......................................
2024-08-31 18:12:55,580:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:55,581:INFO:Creating metrics dataframe
2024-08-31 18:12:55,591:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 18:12:55,591:INFO:Total runtime is 0.18423787752787274 minutes
2024-08-31 18:12:55,593:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:55,593:INFO:Initializing create_model()
2024-08-31 18:12:55,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:55,594:INFO:Checking exceptions
2024-08-31 18:12:55,594:INFO:Importing libraries
2024-08-31 18:12:55,594:INFO:Copying training dataset
2024-08-31 18:12:55,605:INFO:Defining folds
2024-08-31 18:12:55,606:INFO:Declaring metric variables
2024-08-31 18:12:55,609:INFO:Importing untrained model
2024-08-31 18:12:55,612:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:12:55,617:INFO:Starting cross validation
2024-08-31 18:12:55,618:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:12:56,209:INFO:Calculating mean and std
2024-08-31 18:12:56,210:INFO:Creating metrics dataframe
2024-08-31 18:12:56,214:INFO:Uploading results into container
2024-08-31 18:12:56,214:INFO:Uploading model into container now
2024-08-31 18:12:56,215:INFO:_master_model_container: 14
2024-08-31 18:12:56,215:INFO:_display_container: 2
2024-08-31 18:12:56,216:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:12:56,216:INFO:create_model() successfully completed......................................
2024-08-31 18:12:56,425:INFO:SubProcess create_model() end ==================================
2024-08-31 18:12:56,425:INFO:Creating metrics dataframe
2024-08-31 18:12:56,433:INFO:Initializing CatBoost Classifier
2024-08-31 18:12:56,433:INFO:Total runtime is 0.19826796849568687 minutes
2024-08-31 18:12:56,436:INFO:SubProcess create_model() called ==================================
2024-08-31 18:12:56,436:INFO:Initializing create_model()
2024-08-31 18:12:56,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:12:56,436:INFO:Checking exceptions
2024-08-31 18:12:56,436:INFO:Importing libraries
2024-08-31 18:12:56,436:INFO:Copying training dataset
2024-08-31 18:12:56,447:INFO:Defining folds
2024-08-31 18:12:56,447:INFO:Declaring metric variables
2024-08-31 18:12:56,450:INFO:Importing untrained model
2024-08-31 18:12:56,453:INFO:CatBoost Classifier Imported successfully
2024-08-31 18:12:56,457:INFO:Starting cross validation
2024-08-31 18:12:56,458:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:10,998:INFO:Calculating mean and std
2024-08-31 18:13:11,000:INFO:Creating metrics dataframe
2024-08-31 18:13:11,001:INFO:Uploading results into container
2024-08-31 18:13:11,002:INFO:Uploading model into container now
2024-08-31 18:13:11,002:INFO:_master_model_container: 15
2024-08-31 18:13:11,003:INFO:_display_container: 2
2024-08-31 18:13:11,003:INFO:<catboost.core.CatBoostClassifier object at 0x00000266A8EBDC10>
2024-08-31 18:13:11,003:INFO:create_model() successfully completed......................................
2024-08-31 18:13:11,197:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:11,197:INFO:Creating metrics dataframe
2024-08-31 18:13:11,205:INFO:Initializing Dummy Classifier
2024-08-31 18:13:11,205:INFO:Total runtime is 0.44447018702824914 minutes
2024-08-31 18:13:11,209:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:11,209:INFO:Initializing create_model()
2024-08-31 18:13:11,209:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A97C88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:11,209:INFO:Checking exceptions
2024-08-31 18:13:11,209:INFO:Importing libraries
2024-08-31 18:13:11,209:INFO:Copying training dataset
2024-08-31 18:13:11,221:INFO:Defining folds
2024-08-31 18:13:11,221:INFO:Declaring metric variables
2024-08-31 18:13:11,223:INFO:Importing untrained model
2024-08-31 18:13:11,227:INFO:Dummy Classifier Imported successfully
2024-08-31 18:13:11,231:INFO:Starting cross validation
2024-08-31 18:13:11,232:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:11,281:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:13:11,282:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:13:11,283:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:13:11,284:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:13:11,288:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:13:11,297:INFO:Calculating mean and std
2024-08-31 18:13:11,297:INFO:Creating metrics dataframe
2024-08-31 18:13:11,299:INFO:Uploading results into container
2024-08-31 18:13:11,299:INFO:Uploading model into container now
2024-08-31 18:13:11,299:INFO:_master_model_container: 16
2024-08-31 18:13:11,299:INFO:_display_container: 2
2024-08-31 18:13:11,300:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 18:13:11,300:INFO:create_model() successfully completed......................................
2024-08-31 18:13:11,495:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:11,496:INFO:Creating metrics dataframe
2024-08-31 18:13:11,511:INFO:Initializing create_model()
2024-08-31 18:13:11,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:11,513:INFO:Checking exceptions
2024-08-31 18:13:11,514:INFO:Importing libraries
2024-08-31 18:13:11,514:INFO:Copying training dataset
2024-08-31 18:13:11,524:INFO:Defining folds
2024-08-31 18:13:11,524:INFO:Declaring metric variables
2024-08-31 18:13:11,524:INFO:Importing untrained model
2024-08-31 18:13:11,524:INFO:Declaring custom model
2024-08-31 18:13:11,524:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:11,525:INFO:Cross validation set to False
2024-08-31 18:13:11,525:INFO:Fitting Model
2024-08-31 18:13:11,549:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:13:11,549:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:13:11,551:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.
2024-08-31 18:13:11,552:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:13:11,552:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:13:11,552:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:13:11,552:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:13:11,552:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:13:11,552:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:13:11,672:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:11,672:INFO:create_model() successfully completed......................................
2024-08-31 18:13:11,905:INFO:_master_model_container: 16
2024-08-31 18:13:11,905:INFO:_display_container: 2
2024-08-31 18:13:11,905:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:11,905:INFO:compare_models() successfully completed......................................
2024-08-31 18:13:11,907:INFO:Initializing create_model()
2024-08-31 18:13:11,907:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:11,907:INFO:Checking exceptions
2024-08-31 18:13:11,917:INFO:Importing libraries
2024-08-31 18:13:11,917:INFO:Copying training dataset
2024-08-31 18:13:11,929:INFO:Defining folds
2024-08-31 18:13:11,929:INFO:Declaring metric variables
2024-08-31 18:13:11,932:INFO:Importing untrained model
2024-08-31 18:13:11,932:INFO:Declaring custom model
2024-08-31 18:13:11,935:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:11,939:INFO:Starting cross validation
2024-08-31 18:13:11,940:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:13,470:INFO:Calculating mean and std
2024-08-31 18:13:13,471:INFO:Creating metrics dataframe
2024-08-31 18:13:13,478:INFO:Finalizing model
2024-08-31 18:13:13,514:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:13:13,514:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:13:13,517:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000912 seconds.
2024-08-31 18:13:13,517:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:13:13,517:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:13:13,517:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:13:13,517:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:13:13,517:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:13:13,518:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:13:13,644:INFO:Uploading results into container
2024-08-31 18:13:13,646:INFO:Uploading model into container now
2024-08-31 18:13:13,657:INFO:_master_model_container: 17
2024-08-31 18:13:13,657:INFO:_display_container: 3
2024-08-31 18:13:13,658:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:13,658:INFO:create_model() successfully completed......................................
2024-08-31 18:13:13,870:INFO:Initializing tune_model()
2024-08-31 18:13:13,870:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 18:13:13,870:INFO:Checking exceptions
2024-08-31 18:13:13,884:INFO:Copying training dataset
2024-08-31 18:13:13,892:INFO:Checking base model
2024-08-31 18:13:13,892:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 18:13:13,895:INFO:Declaring metric variables
2024-08-31 18:13:13,897:INFO:Defining Hyperparameters
2024-08-31 18:13:14,072:INFO:Tuning with n_jobs=-1
2024-08-31 18:13:14,072:INFO:Initializing RandomizedSearchCV
2024-08-31 18:13:34,574:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 18:13:34,575:INFO:Hyperparameter search completed
2024-08-31 18:13:34,577:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:34,578:INFO:Initializing create_model()
2024-08-31 18:13:34,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266962D9D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 18:13:34,578:INFO:Checking exceptions
2024-08-31 18:13:34,578:INFO:Importing libraries
2024-08-31 18:13:34,578:INFO:Copying training dataset
2024-08-31 18:13:34,607:INFO:Defining folds
2024-08-31 18:13:34,608:INFO:Declaring metric variables
2024-08-31 18:13:34,614:INFO:Importing untrained model
2024-08-31 18:13:34,614:INFO:Declaring custom model
2024-08-31 18:13:34,624:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:34,637:INFO:Starting cross validation
2024-08-31 18:13:34,639:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:37,191:INFO:Calculating mean and std
2024-08-31 18:13:37,193:INFO:Creating metrics dataframe
2024-08-31 18:13:37,199:INFO:Finalizing model
2024-08-31 18:13:37,228:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:13:37,228:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:13:37,228:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:13:37,240:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:13:37,240:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:13:37,240:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:13:37,240:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:13:37,240:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:13:37,243:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.
2024-08-31 18:13:37,243:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:13:37,243:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:13:37,243:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:13:37,243:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:13:37,243:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:13:37,244:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:13:37,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,485:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,499:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,519:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,519:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,555:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,563:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,565:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,583:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:13:37,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:13:37,606:INFO:Uploading results into container
2024-08-31 18:13:37,607:INFO:Uploading model into container now
2024-08-31 18:13:37,608:INFO:_master_model_container: 18
2024-08-31 18:13:37,609:INFO:_display_container: 4
2024-08-31 18:13:37,609:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:37,610:INFO:create_model() successfully completed......................................
2024-08-31 18:13:37,855:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:37,855:INFO:choose_better activated
2024-08-31 18:13:37,860:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:37,861:INFO:Initializing create_model()
2024-08-31 18:13:37,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:37,861:INFO:Checking exceptions
2024-08-31 18:13:37,862:INFO:Importing libraries
2024-08-31 18:13:37,862:INFO:Copying training dataset
2024-08-31 18:13:37,874:INFO:Defining folds
2024-08-31 18:13:37,875:INFO:Declaring metric variables
2024-08-31 18:13:37,875:INFO:Importing untrained model
2024-08-31 18:13:37,875:INFO:Declaring custom model
2024-08-31 18:13:37,875:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:37,875:INFO:Starting cross validation
2024-08-31 18:13:37,876:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:39,157:INFO:Calculating mean and std
2024-08-31 18:13:39,157:INFO:Creating metrics dataframe
2024-08-31 18:13:39,159:INFO:Finalizing model
2024-08-31 18:13:39,199:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:13:39,199:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:13:39,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.
2024-08-31 18:13:39,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:13:39,201:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:13:39,201:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:13:39,201:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:13:39,202:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:13:39,202:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:13:39,379:INFO:Uploading results into container
2024-08-31 18:13:39,380:INFO:Uploading model into container now
2024-08-31 18:13:39,381:INFO:_master_model_container: 19
2024-08-31 18:13:39,381:INFO:_display_container: 5
2024-08-31 18:13:39,381:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:39,381:INFO:create_model() successfully completed......................................
2024-08-31 18:13:39,616:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:39,617:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 18:13:39,617:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-08-31 18:13:39,618:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 18:13:39,618:INFO:choose_better completed
2024-08-31 18:13:39,618:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 18:13:39,628:INFO:_master_model_container: 19
2024-08-31 18:13:39,628:INFO:_display_container: 4
2024-08-31 18:13:39,629:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:39,629:INFO:tune_model() successfully completed......................................
2024-08-31 18:13:39,844:INFO:Initializing finalize_model()
2024-08-31 18:13:39,844:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 18:13:39,844:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:39,851:INFO:Initializing create_model()
2024-08-31 18:13:39,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:39,851:INFO:Checking exceptions
2024-08-31 18:13:39,852:INFO:Importing libraries
2024-08-31 18:13:39,852:INFO:Copying training dataset
2024-08-31 18:13:39,853:INFO:Defining folds
2024-08-31 18:13:39,853:INFO:Declaring metric variables
2024-08-31 18:13:39,853:INFO:Importing untrained model
2024-08-31 18:13:39,853:INFO:Declaring custom model
2024-08-31 18:13:39,854:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:39,854:INFO:Cross validation set to False
2024-08-31 18:13:39,854:INFO:Fitting Model
2024-08-31 18:13:39,895:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:13:39,896:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-08-31 18:13:39,898:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.
2024-08-31 18:13:39,898:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:13:39,898:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:13:39,898:INFO:[LightGBM] [Info] Total Bins 681
2024-08-31 18:13:39,898:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-08-31 18:13:39,898:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-08-31 18:13:39,899:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-08-31 18:13:40,007:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:13:40,007:INFO:create_model() successfully completed......................................
2024-08-31 18:13:40,222:INFO:_master_model_container: 19
2024-08-31 18:13:40,222:INFO:_display_container: 4
2024-08-31 18:13:40,226:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:13:40,226:INFO:finalize_model() successfully completed......................................
2024-08-31 18:13:40,414:INFO:Initializing predict_model()
2024-08-31 18:13:40,414:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266A66BD940>)
2024-08-31 18:13:40,414:INFO:Checking exceptions
2024-08-31 18:13:40,414:INFO:Preloading libraries
2024-08-31 18:13:40,416:INFO:Set up data.
2024-08-31 18:13:40,422:INFO:Set up index.
2024-08-31 18:13:40,708:INFO:Initializing evaluate_model()
2024-08-31 18:13:40,708:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-31 18:13:40,722:INFO:Initializing plot_model()
2024-08-31 18:13:40,723:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A25DCDD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:13:40,723:INFO:Checking exceptions
2024-08-31 18:13:40,728:INFO:Preloading libraries
2024-08-31 18:13:40,732:INFO:Copying training dataset
2024-08-31 18:13:40,732:INFO:Plot type: pipeline
2024-08-31 18:13:40,839:INFO:Visual Rendered Successfully
2024-08-31 18:13:41,015:INFO:plot_model() successfully completed......................................
2024-08-31 18:13:41,026:INFO:PyCaret ClassificationExperiment
2024-08-31 18:13:41,026:INFO:Logging name: clf-default-name
2024-08-31 18:13:41,026:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:13:41,026:INFO:version 3.3.2
2024-08-31 18:13:41,026:INFO:Initializing setup()
2024-08-31 18:13:41,026:INFO:self.USI: e66f
2024-08-31 18:13:41,026:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:13:41,026:INFO:Checking environment
2024-08-31 18:13:41,027:INFO:python_version: 3.11.9
2024-08-31 18:13:41,027:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:13:41,027:INFO:machine: AMD64
2024-08-31 18:13:41,027:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:13:41,031:INFO:Memory: svmem(total=16867028992, available=2435465216, percent=85.6, used=14431563776, free=2435465216)
2024-08-31 18:13:41,031:INFO:Physical Core: 6
2024-08-31 18:13:41,031:INFO:Logical Core: 12
2024-08-31 18:13:41,031:INFO:Checking libraries
2024-08-31 18:13:41,031:INFO:System:
2024-08-31 18:13:41,031:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:13:41,031:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:13:41,031:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:13:41,031:INFO:PyCaret required dependencies:
2024-08-31 18:13:41,031:INFO:                 pip: 23.2.1
2024-08-31 18:13:41,031:INFO:          setuptools: 67.8.0
2024-08-31 18:13:41,031:INFO:             pycaret: 3.3.2
2024-08-31 18:13:41,031:INFO:             IPython: 8.14.0
2024-08-31 18:13:41,031:INFO:          ipywidgets: 8.1.5
2024-08-31 18:13:41,031:INFO:                tqdm: 4.66.5
2024-08-31 18:13:41,031:INFO:               numpy: 1.24.3
2024-08-31 18:13:41,031:INFO:              pandas: 2.0.3
2024-08-31 18:13:41,031:INFO:              jinja2: 3.1.4
2024-08-31 18:13:41,031:INFO:               scipy: 1.10.1
2024-08-31 18:13:41,031:INFO:              joblib: 1.2.0
2024-08-31 18:13:41,031:INFO:             sklearn: 1.4.2
2024-08-31 18:13:41,031:INFO:                pyod: 2.0.1
2024-08-31 18:13:41,031:INFO:            imblearn: 0.12.3
2024-08-31 18:13:41,031:INFO:   category_encoders: 2.6.3
2024-08-31 18:13:41,031:INFO:            lightgbm: 4.5.0
2024-08-31 18:13:41,031:INFO:               numba: 0.60.0
2024-08-31 18:13:41,031:INFO:            requests: 2.32.3
2024-08-31 18:13:41,032:INFO:          matplotlib: 3.7.1
2024-08-31 18:13:41,032:INFO:          scikitplot: 0.3.7
2024-08-31 18:13:41,032:INFO:         yellowbrick: 1.5
2024-08-31 18:13:41,032:INFO:              plotly: 5.16.1
2024-08-31 18:13:41,032:INFO:    plotly-resampler: Not installed
2024-08-31 18:13:41,032:INFO:             kaleido: 0.2.1
2024-08-31 18:13:41,032:INFO:           schemdraw: 0.15
2024-08-31 18:13:41,032:INFO:         statsmodels: 0.14.2
2024-08-31 18:13:41,032:INFO:              sktime: 0.26.0
2024-08-31 18:13:41,032:INFO:               tbats: 1.1.3
2024-08-31 18:13:41,032:INFO:            pmdarima: 2.0.4
2024-08-31 18:13:41,032:INFO:              psutil: 5.9.0
2024-08-31 18:13:41,032:INFO:          markupsafe: 2.1.3
2024-08-31 18:13:41,032:INFO:             pickle5: Not installed
2024-08-31 18:13:41,032:INFO:         cloudpickle: 3.0.0
2024-08-31 18:13:41,032:INFO:         deprecation: 2.1.0
2024-08-31 18:13:41,032:INFO:              xxhash: 3.5.0
2024-08-31 18:13:41,032:INFO:           wurlitzer: Not installed
2024-08-31 18:13:41,032:INFO:PyCaret optional dependencies:
2024-08-31 18:13:41,032:INFO:                shap: Not installed
2024-08-31 18:13:41,032:INFO:           interpret: Not installed
2024-08-31 18:13:41,032:INFO:                umap: Not installed
2024-08-31 18:13:41,032:INFO:     ydata_profiling: Not installed
2024-08-31 18:13:41,032:INFO:  explainerdashboard: Not installed
2024-08-31 18:13:41,032:INFO:             autoviz: Not installed
2024-08-31 18:13:41,032:INFO:           fairlearn: Not installed
2024-08-31 18:13:41,032:INFO:          deepchecks: Not installed
2024-08-31 18:13:41,032:INFO:             xgboost: 2.0.2
2024-08-31 18:13:41,032:INFO:            catboost: 1.2.5
2024-08-31 18:13:41,032:INFO:              kmodes: Not installed
2024-08-31 18:13:41,032:INFO:             mlxtend: Not installed
2024-08-31 18:13:41,032:INFO:       statsforecast: Not installed
2024-08-31 18:13:41,032:INFO:        tune_sklearn: Not installed
2024-08-31 18:13:41,032:INFO:                 ray: Not installed
2024-08-31 18:13:41,032:INFO:            hyperopt: Not installed
2024-08-31 18:13:41,032:INFO:              optuna: Not installed
2024-08-31 18:13:41,032:INFO:               skopt: Not installed
2024-08-31 18:13:41,033:INFO:              mlflow: Not installed
2024-08-31 18:13:41,033:INFO:              gradio: 4.41.0
2024-08-31 18:13:41,033:INFO:             fastapi: 0.112.1
2024-08-31 18:13:41,033:INFO:             uvicorn: 0.30.6
2024-08-31 18:13:41,033:INFO:              m2cgen: Not installed
2024-08-31 18:13:41,033:INFO:           evidently: Not installed
2024-08-31 18:13:41,033:INFO:               fugue: Not installed
2024-08-31 18:13:41,033:INFO:           streamlit: Not installed
2024-08-31 18:13:41,033:INFO:             prophet: Not installed
2024-08-31 18:13:41,033:INFO:None
2024-08-31 18:13:41,033:INFO:Set up data.
2024-08-31 18:13:41,041:INFO:Set up folding strategy.
2024-08-31 18:13:41,042:INFO:Set up train/test split.
2024-08-31 18:13:41,053:INFO:Set up index.
2024-08-31 18:13:41,054:INFO:Assigning column types.
2024-08-31 18:13:41,061:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:13:41,096:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,096:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,118:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,120:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,155:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,156:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,179:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,180:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,181:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:13:41,215:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,235:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,238:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,273:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:13:41,294:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,295:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,296:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:13:41,356:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,358:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,416:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,417:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,418:INFO:Preparing preprocessing pipeline...
2024-08-31 18:13:41,421:INFO:Set up simple imputation.
2024-08-31 18:13:41,422:INFO:Set up column name cleaning.
2024-08-31 18:13:41,451:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:13:41,454:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:13:41,454:INFO:Creating final display dataframe.
2024-08-31 18:13:41,547:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              e66f
2024-08-31 18:13:41,621:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,623:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,678:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:13:41,681:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:13:41,683:INFO:setup() successfully completed in 0.66s...............
2024-08-31 18:13:41,683:INFO:Initializing compare_models()
2024-08-31 18:13:41,683:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 18:13:41,683:INFO:Checking exceptions
2024-08-31 18:13:41,690:INFO:Preparing display monitor
2024-08-31 18:13:41,705:INFO:Initializing Logistic Regression
2024-08-31 18:13:41,707:INFO:Total runtime is 0.0 minutes
2024-08-31 18:13:41,709:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:41,709:INFO:Initializing create_model()
2024-08-31 18:13:41,709:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:41,709:INFO:Checking exceptions
2024-08-31 18:13:41,709:INFO:Importing libraries
2024-08-31 18:13:41,709:INFO:Copying training dataset
2024-08-31 18:13:41,722:INFO:Defining folds
2024-08-31 18:13:41,722:INFO:Declaring metric variables
2024-08-31 18:13:41,725:INFO:Importing untrained model
2024-08-31 18:13:41,727:INFO:Logistic Regression Imported successfully
2024-08-31 18:13:41,732:INFO:Starting cross validation
2024-08-31 18:13:41,733:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:42,946:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:13:42,959:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:13:42,965:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:13:42,975:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:13:42,978:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:13:43,003:INFO:Calculating mean and std
2024-08-31 18:13:43,005:INFO:Creating metrics dataframe
2024-08-31 18:13:43,006:INFO:Uploading results into container
2024-08-31 18:13:43,007:INFO:Uploading model into container now
2024-08-31 18:13:43,007:INFO:_master_model_container: 1
2024-08-31 18:13:43,007:INFO:_display_container: 2
2024-08-31 18:13:43,007:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 18:13:43,007:INFO:create_model() successfully completed......................................
2024-08-31 18:13:43,206:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:43,206:INFO:Creating metrics dataframe
2024-08-31 18:13:43,212:INFO:Initializing K Neighbors Classifier
2024-08-31 18:13:43,212:INFO:Total runtime is 0.025109545389811198 minutes
2024-08-31 18:13:43,215:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:43,216:INFO:Initializing create_model()
2024-08-31 18:13:43,216:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:43,216:INFO:Checking exceptions
2024-08-31 18:13:43,216:INFO:Importing libraries
2024-08-31 18:13:43,216:INFO:Copying training dataset
2024-08-31 18:13:43,228:INFO:Defining folds
2024-08-31 18:13:43,228:INFO:Declaring metric variables
2024-08-31 18:13:43,231:INFO:Importing untrained model
2024-08-31 18:13:43,235:INFO:K Neighbors Classifier Imported successfully
2024-08-31 18:13:43,240:INFO:Starting cross validation
2024-08-31 18:13:43,241:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:43,584:INFO:Calculating mean and std
2024-08-31 18:13:43,585:INFO:Creating metrics dataframe
2024-08-31 18:13:43,587:INFO:Uploading results into container
2024-08-31 18:13:43,587:INFO:Uploading model into container now
2024-08-31 18:13:43,588:INFO:_master_model_container: 2
2024-08-31 18:13:43,588:INFO:_display_container: 2
2024-08-31 18:13:43,588:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 18:13:43,589:INFO:create_model() successfully completed......................................
2024-08-31 18:13:43,787:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:43,787:INFO:Creating metrics dataframe
2024-08-31 18:13:43,794:INFO:Initializing Naive Bayes
2024-08-31 18:13:43,794:INFO:Total runtime is 0.03480678002039592 minutes
2024-08-31 18:13:43,797:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:43,798:INFO:Initializing create_model()
2024-08-31 18:13:43,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:43,798:INFO:Checking exceptions
2024-08-31 18:13:43,798:INFO:Importing libraries
2024-08-31 18:13:43,798:INFO:Copying training dataset
2024-08-31 18:13:43,810:INFO:Defining folds
2024-08-31 18:13:43,810:INFO:Declaring metric variables
2024-08-31 18:13:43,813:INFO:Importing untrained model
2024-08-31 18:13:43,816:INFO:Naive Bayes Imported successfully
2024-08-31 18:13:43,822:INFO:Starting cross validation
2024-08-31 18:13:43,823:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:43,956:INFO:Calculating mean and std
2024-08-31 18:13:43,957:INFO:Creating metrics dataframe
2024-08-31 18:13:43,958:INFO:Uploading results into container
2024-08-31 18:13:43,959:INFO:Uploading model into container now
2024-08-31 18:13:43,959:INFO:_master_model_container: 3
2024-08-31 18:13:43,959:INFO:_display_container: 2
2024-08-31 18:13:43,959:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 18:13:43,959:INFO:create_model() successfully completed......................................
2024-08-31 18:13:44,151:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:44,152:INFO:Creating metrics dataframe
2024-08-31 18:13:44,158:INFO:Initializing Decision Tree Classifier
2024-08-31 18:13:44,158:INFO:Total runtime is 0.04087915817896525 minutes
2024-08-31 18:13:44,160:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:44,161:INFO:Initializing create_model()
2024-08-31 18:13:44,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:44,161:INFO:Checking exceptions
2024-08-31 18:13:44,161:INFO:Importing libraries
2024-08-31 18:13:44,161:INFO:Copying training dataset
2024-08-31 18:13:44,179:INFO:Defining folds
2024-08-31 18:13:44,179:INFO:Declaring metric variables
2024-08-31 18:13:44,181:INFO:Importing untrained model
2024-08-31 18:13:44,184:INFO:Decision Tree Classifier Imported successfully
2024-08-31 18:13:44,190:INFO:Starting cross validation
2024-08-31 18:13:44,191:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:44,330:INFO:Calculating mean and std
2024-08-31 18:13:44,332:INFO:Creating metrics dataframe
2024-08-31 18:13:44,333:INFO:Uploading results into container
2024-08-31 18:13:44,334:INFO:Uploading model into container now
2024-08-31 18:13:44,334:INFO:_master_model_container: 4
2024-08-31 18:13:44,335:INFO:_display_container: 2
2024-08-31 18:13:44,335:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 18:13:44,335:INFO:create_model() successfully completed......................................
2024-08-31 18:13:44,531:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:44,532:INFO:Creating metrics dataframe
2024-08-31 18:13:44,538:INFO:Initializing SVM - Linear Kernel
2024-08-31 18:13:44,538:INFO:Total runtime is 0.047205913066864016 minutes
2024-08-31 18:13:44,541:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:44,542:INFO:Initializing create_model()
2024-08-31 18:13:44,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:44,542:INFO:Checking exceptions
2024-08-31 18:13:44,542:INFO:Importing libraries
2024-08-31 18:13:44,542:INFO:Copying training dataset
2024-08-31 18:13:44,553:INFO:Defining folds
2024-08-31 18:13:44,553:INFO:Declaring metric variables
2024-08-31 18:13:44,557:INFO:Importing untrained model
2024-08-31 18:13:44,560:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 18:13:44,565:INFO:Starting cross validation
2024-08-31 18:13:44,566:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:44,822:INFO:Calculating mean and std
2024-08-31 18:13:44,823:INFO:Creating metrics dataframe
2024-08-31 18:13:44,825:INFO:Uploading results into container
2024-08-31 18:13:44,826:INFO:Uploading model into container now
2024-08-31 18:13:44,826:INFO:_master_model_container: 5
2024-08-31 18:13:44,826:INFO:_display_container: 2
2024-08-31 18:13:44,826:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 18:13:44,827:INFO:create_model() successfully completed......................................
2024-08-31 18:13:45,025:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:45,025:INFO:Creating metrics dataframe
2024-08-31 18:13:45,032:INFO:Initializing Ridge Classifier
2024-08-31 18:13:45,032:INFO:Total runtime is 0.05544997056325277 minutes
2024-08-31 18:13:45,035:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:45,035:INFO:Initializing create_model()
2024-08-31 18:13:45,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:45,035:INFO:Checking exceptions
2024-08-31 18:13:45,035:INFO:Importing libraries
2024-08-31 18:13:45,035:INFO:Copying training dataset
2024-08-31 18:13:45,055:INFO:Defining folds
2024-08-31 18:13:45,055:INFO:Declaring metric variables
2024-08-31 18:13:45,057:INFO:Importing untrained model
2024-08-31 18:13:45,062:INFO:Ridge Classifier Imported successfully
2024-08-31 18:13:45,066:INFO:Starting cross validation
2024-08-31 18:13:45,067:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:45,144:INFO:Calculating mean and std
2024-08-31 18:13:45,144:INFO:Creating metrics dataframe
2024-08-31 18:13:45,145:INFO:Uploading results into container
2024-08-31 18:13:45,147:INFO:Uploading model into container now
2024-08-31 18:13:45,147:INFO:_master_model_container: 6
2024-08-31 18:13:45,147:INFO:_display_container: 2
2024-08-31 18:13:45,147:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 18:13:45,147:INFO:create_model() successfully completed......................................
2024-08-31 18:13:45,345:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:45,345:INFO:Creating metrics dataframe
2024-08-31 18:13:45,356:INFO:Initializing Random Forest Classifier
2024-08-31 18:13:45,356:INFO:Total runtime is 0.0608399788538615 minutes
2024-08-31 18:13:45,359:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:45,360:INFO:Initializing create_model()
2024-08-31 18:13:45,360:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:45,360:INFO:Checking exceptions
2024-08-31 18:13:45,360:INFO:Importing libraries
2024-08-31 18:13:45,360:INFO:Copying training dataset
2024-08-31 18:13:45,372:INFO:Defining folds
2024-08-31 18:13:45,373:INFO:Declaring metric variables
2024-08-31 18:13:45,376:INFO:Importing untrained model
2024-08-31 18:13:45,379:INFO:Random Forest Classifier Imported successfully
2024-08-31 18:13:45,384:INFO:Starting cross validation
2024-08-31 18:13:45,385:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:46,667:INFO:Calculating mean and std
2024-08-31 18:13:46,669:INFO:Creating metrics dataframe
2024-08-31 18:13:46,671:INFO:Uploading results into container
2024-08-31 18:13:46,672:INFO:Uploading model into container now
2024-08-31 18:13:46,672:INFO:_master_model_container: 7
2024-08-31 18:13:46,672:INFO:_display_container: 2
2024-08-31 18:13:46,673:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 18:13:46,673:INFO:create_model() successfully completed......................................
2024-08-31 18:13:46,871:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:46,871:INFO:Creating metrics dataframe
2024-08-31 18:13:46,879:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 18:13:46,879:INFO:Total runtime is 0.08622093200683595 minutes
2024-08-31 18:13:46,882:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:46,883:INFO:Initializing create_model()
2024-08-31 18:13:46,883:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:46,883:INFO:Checking exceptions
2024-08-31 18:13:46,883:INFO:Importing libraries
2024-08-31 18:13:46,883:INFO:Copying training dataset
2024-08-31 18:13:46,896:INFO:Defining folds
2024-08-31 18:13:46,897:INFO:Declaring metric variables
2024-08-31 18:13:46,900:INFO:Importing untrained model
2024-08-31 18:13:46,903:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 18:13:46,910:INFO:Starting cross validation
2024-08-31 18:13:46,911:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:46,988:INFO:Calculating mean and std
2024-08-31 18:13:46,989:INFO:Creating metrics dataframe
2024-08-31 18:13:46,990:INFO:Uploading results into container
2024-08-31 18:13:46,991:INFO:Uploading model into container now
2024-08-31 18:13:46,991:INFO:_master_model_container: 8
2024-08-31 18:13:46,991:INFO:_display_container: 2
2024-08-31 18:13:46,991:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 18:13:46,991:INFO:create_model() successfully completed......................................
2024-08-31 18:13:47,185:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:47,185:INFO:Creating metrics dataframe
2024-08-31 18:13:47,193:INFO:Initializing Ada Boost Classifier
2024-08-31 18:13:47,193:INFO:Total runtime is 0.09145687818527223 minutes
2024-08-31 18:13:47,198:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:47,198:INFO:Initializing create_model()
2024-08-31 18:13:47,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:47,198:INFO:Checking exceptions
2024-08-31 18:13:47,198:INFO:Importing libraries
2024-08-31 18:13:47,198:INFO:Copying training dataset
2024-08-31 18:13:47,227:INFO:Defining folds
2024-08-31 18:13:47,228:INFO:Declaring metric variables
2024-08-31 18:13:47,233:INFO:Importing untrained model
2024-08-31 18:13:47,237:INFO:Ada Boost Classifier Imported successfully
2024-08-31 18:13:47,244:INFO:Starting cross validation
2024-08-31 18:13:47,245:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:47,275:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:13:47,281:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:13:47,284:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:13:47,285:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:13:47,289:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:13:47,855:INFO:Calculating mean and std
2024-08-31 18:13:47,856:INFO:Creating metrics dataframe
2024-08-31 18:13:47,858:INFO:Uploading results into container
2024-08-31 18:13:47,858:INFO:Uploading model into container now
2024-08-31 18:13:47,859:INFO:_master_model_container: 9
2024-08-31 18:13:47,859:INFO:_display_container: 2
2024-08-31 18:13:47,859:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 18:13:47,859:INFO:create_model() successfully completed......................................
2024-08-31 18:13:48,043:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:48,043:INFO:Creating metrics dataframe
2024-08-31 18:13:48,051:INFO:Initializing Gradient Boosting Classifier
2024-08-31 18:13:48,051:INFO:Total runtime is 0.10575057665507001 minutes
2024-08-31 18:13:48,054:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:48,054:INFO:Initializing create_model()
2024-08-31 18:13:48,054:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:48,054:INFO:Checking exceptions
2024-08-31 18:13:48,054:INFO:Importing libraries
2024-08-31 18:13:48,054:INFO:Copying training dataset
2024-08-31 18:13:48,066:INFO:Defining folds
2024-08-31 18:13:48,066:INFO:Declaring metric variables
2024-08-31 18:13:48,070:INFO:Importing untrained model
2024-08-31 18:13:48,073:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 18:13:48,078:INFO:Starting cross validation
2024-08-31 18:13:48,079:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:49,933:INFO:Calculating mean and std
2024-08-31 18:13:49,934:INFO:Creating metrics dataframe
2024-08-31 18:13:49,936:INFO:Uploading results into container
2024-08-31 18:13:49,937:INFO:Uploading model into container now
2024-08-31 18:13:49,937:INFO:_master_model_container: 10
2024-08-31 18:13:49,937:INFO:_display_container: 2
2024-08-31 18:13:49,938:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 18:13:49,938:INFO:create_model() successfully completed......................................
2024-08-31 18:13:50,127:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:50,127:INFO:Creating metrics dataframe
2024-08-31 18:13:50,135:INFO:Initializing Linear Discriminant Analysis
2024-08-31 18:13:50,135:INFO:Total runtime is 0.14049750169118247 minutes
2024-08-31 18:13:50,138:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:50,139:INFO:Initializing create_model()
2024-08-31 18:13:50,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:50,139:INFO:Checking exceptions
2024-08-31 18:13:50,139:INFO:Importing libraries
2024-08-31 18:13:50,139:INFO:Copying training dataset
2024-08-31 18:13:50,152:INFO:Defining folds
2024-08-31 18:13:50,152:INFO:Declaring metric variables
2024-08-31 18:13:50,155:INFO:Importing untrained model
2024-08-31 18:13:50,157:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 18:13:50,164:INFO:Starting cross validation
2024-08-31 18:13:50,165:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:50,259:INFO:Calculating mean and std
2024-08-31 18:13:50,260:INFO:Creating metrics dataframe
2024-08-31 18:13:50,262:INFO:Uploading results into container
2024-08-31 18:13:50,263:INFO:Uploading model into container now
2024-08-31 18:13:50,263:INFO:_master_model_container: 11
2024-08-31 18:13:50,263:INFO:_display_container: 2
2024-08-31 18:13:50,263:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 18:13:50,264:INFO:create_model() successfully completed......................................
2024-08-31 18:13:50,450:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:50,450:INFO:Creating metrics dataframe
2024-08-31 18:13:50,458:INFO:Initializing Extra Trees Classifier
2024-08-31 18:13:50,458:INFO:Total runtime is 0.14587629636128746 minutes
2024-08-31 18:13:50,461:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:50,462:INFO:Initializing create_model()
2024-08-31 18:13:50,462:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:50,462:INFO:Checking exceptions
2024-08-31 18:13:50,462:INFO:Importing libraries
2024-08-31 18:13:50,462:INFO:Copying training dataset
2024-08-31 18:13:50,473:INFO:Defining folds
2024-08-31 18:13:50,473:INFO:Declaring metric variables
2024-08-31 18:13:50,477:INFO:Importing untrained model
2024-08-31 18:13:50,480:INFO:Extra Trees Classifier Imported successfully
2024-08-31 18:13:50,485:INFO:Starting cross validation
2024-08-31 18:13:50,485:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:51,564:INFO:Calculating mean and std
2024-08-31 18:13:51,566:INFO:Creating metrics dataframe
2024-08-31 18:13:51,569:INFO:Uploading results into container
2024-08-31 18:13:51,569:INFO:Uploading model into container now
2024-08-31 18:13:51,570:INFO:_master_model_container: 12
2024-08-31 18:13:51,570:INFO:_display_container: 2
2024-08-31 18:13:51,570:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 18:13:51,570:INFO:create_model() successfully completed......................................
2024-08-31 18:13:51,810:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:51,811:INFO:Creating metrics dataframe
2024-08-31 18:13:51,819:INFO:Initializing Extreme Gradient Boosting
2024-08-31 18:13:51,819:INFO:Total runtime is 0.16855579614639285 minutes
2024-08-31 18:13:51,822:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:51,823:INFO:Initializing create_model()
2024-08-31 18:13:51,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:51,823:INFO:Checking exceptions
2024-08-31 18:13:51,823:INFO:Importing libraries
2024-08-31 18:13:51,823:INFO:Copying training dataset
2024-08-31 18:13:51,834:INFO:Defining folds
2024-08-31 18:13:51,835:INFO:Declaring metric variables
2024-08-31 18:13:51,838:INFO:Importing untrained model
2024-08-31 18:13:51,841:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 18:13:51,845:INFO:Starting cross validation
2024-08-31 18:13:51,847:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:52,109:INFO:Calculating mean and std
2024-08-31 18:13:52,110:INFO:Creating metrics dataframe
2024-08-31 18:13:52,112:INFO:Uploading results into container
2024-08-31 18:13:52,112:INFO:Uploading model into container now
2024-08-31 18:13:52,113:INFO:_master_model_container: 13
2024-08-31 18:13:52,113:INFO:_display_container: 2
2024-08-31 18:13:52,114:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 18:13:52,114:INFO:create_model() successfully completed......................................
2024-08-31 18:13:52,307:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:52,307:INFO:Creating metrics dataframe
2024-08-31 18:13:52,315:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 18:13:52,316:INFO:Total runtime is 0.1768410086631775 minutes
2024-08-31 18:13:52,318:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:52,318:INFO:Initializing create_model()
2024-08-31 18:13:52,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:52,318:INFO:Checking exceptions
2024-08-31 18:13:52,318:INFO:Importing libraries
2024-08-31 18:13:52,318:INFO:Copying training dataset
2024-08-31 18:13:52,331:INFO:Defining folds
2024-08-31 18:13:52,331:INFO:Declaring metric variables
2024-08-31 18:13:52,334:INFO:Importing untrained model
2024-08-31 18:13:52,337:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:13:52,341:INFO:Starting cross validation
2024-08-31 18:13:52,342:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:13:52,945:INFO:Calculating mean and std
2024-08-31 18:13:52,947:INFO:Creating metrics dataframe
2024-08-31 18:13:52,950:INFO:Uploading results into container
2024-08-31 18:13:52,951:INFO:Uploading model into container now
2024-08-31 18:13:52,952:INFO:_master_model_container: 14
2024-08-31 18:13:52,952:INFO:_display_container: 2
2024-08-31 18:13:52,953:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:13:52,953:INFO:create_model() successfully completed......................................
2024-08-31 18:13:53,166:INFO:SubProcess create_model() end ==================================
2024-08-31 18:13:53,166:INFO:Creating metrics dataframe
2024-08-31 18:13:53,175:INFO:Initializing CatBoost Classifier
2024-08-31 18:13:53,175:INFO:Total runtime is 0.19116158882776899 minutes
2024-08-31 18:13:53,178:INFO:SubProcess create_model() called ==================================
2024-08-31 18:13:53,179:INFO:Initializing create_model()
2024-08-31 18:13:53,179:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A27BEA90>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AF7750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:13:53,179:INFO:Checking exceptions
2024-08-31 18:13:53,179:INFO:Importing libraries
2024-08-31 18:13:53,179:INFO:Copying training dataset
2024-08-31 18:13:53,191:INFO:Defining folds
2024-08-31 18:13:53,191:INFO:Declaring metric variables
2024-08-31 18:13:53,194:INFO:Importing untrained model
2024-08-31 18:13:53,197:INFO:CatBoost Classifier Imported successfully
2024-08-31 18:13:53,202:INFO:Starting cross validation
2024-08-31 18:13:53,203:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:14:11,949:INFO:Initializing plot_model()
2024-08-31 18:14:11,949:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6161590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:14:11,950:INFO:Checking exceptions
2024-08-31 18:14:11,958:INFO:Preloading libraries
2024-08-31 18:14:11,965:INFO:Copying training dataset
2024-08-31 18:14:11,965:INFO:Plot type: parameter
2024-08-31 18:14:11,972:INFO:Visual Rendered Successfully
2024-08-31 18:14:12,275:INFO:plot_model() successfully completed......................................
2024-08-31 18:14:57,521:INFO:PyCaret ClassificationExperiment
2024-08-31 18:14:57,521:INFO:Logging name: clf-default-name
2024-08-31 18:14:57,521:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:14:57,521:INFO:version 3.3.2
2024-08-31 18:14:57,521:INFO:Initializing setup()
2024-08-31 18:14:57,521:INFO:self.USI: 4769
2024-08-31 18:14:57,521:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:14:57,522:INFO:Checking environment
2024-08-31 18:14:57,522:INFO:python_version: 3.11.9
2024-08-31 18:14:57,522:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:14:57,522:INFO:machine: AMD64
2024-08-31 18:14:57,522:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:14:57,529:INFO:Memory: svmem(total=16867028992, available=4824260608, percent=71.4, used=12042768384, free=4824260608)
2024-08-31 18:14:57,529:INFO:Physical Core: 6
2024-08-31 18:14:57,529:INFO:Logical Core: 12
2024-08-31 18:14:57,529:INFO:Checking libraries
2024-08-31 18:14:57,529:INFO:System:
2024-08-31 18:14:57,529:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:14:57,529:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:14:57,529:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:14:57,529:INFO:PyCaret required dependencies:
2024-08-31 18:14:57,530:INFO:                 pip: 23.2.1
2024-08-31 18:14:57,530:INFO:          setuptools: 67.8.0
2024-08-31 18:14:57,530:INFO:             pycaret: 3.3.2
2024-08-31 18:14:57,530:INFO:             IPython: 8.14.0
2024-08-31 18:14:57,530:INFO:          ipywidgets: 8.1.5
2024-08-31 18:14:57,530:INFO:                tqdm: 4.66.5
2024-08-31 18:14:57,530:INFO:               numpy: 1.24.3
2024-08-31 18:14:57,530:INFO:              pandas: 2.0.3
2024-08-31 18:14:57,530:INFO:              jinja2: 3.1.4
2024-08-31 18:14:57,530:INFO:               scipy: 1.10.1
2024-08-31 18:14:57,530:INFO:              joblib: 1.2.0
2024-08-31 18:14:57,530:INFO:             sklearn: 1.4.2
2024-08-31 18:14:57,530:INFO:                pyod: 2.0.1
2024-08-31 18:14:57,530:INFO:            imblearn: 0.12.3
2024-08-31 18:14:57,530:INFO:   category_encoders: 2.6.3
2024-08-31 18:14:57,530:INFO:            lightgbm: 4.5.0
2024-08-31 18:14:57,530:INFO:               numba: 0.60.0
2024-08-31 18:14:57,530:INFO:            requests: 2.32.3
2024-08-31 18:14:57,530:INFO:          matplotlib: 3.7.1
2024-08-31 18:14:57,531:INFO:          scikitplot: 0.3.7
2024-08-31 18:14:57,531:INFO:         yellowbrick: 1.5
2024-08-31 18:14:57,531:INFO:              plotly: 5.16.1
2024-08-31 18:14:57,531:INFO:    plotly-resampler: Not installed
2024-08-31 18:14:57,531:INFO:             kaleido: 0.2.1
2024-08-31 18:14:57,531:INFO:           schemdraw: 0.15
2024-08-31 18:14:57,531:INFO:         statsmodels: 0.14.2
2024-08-31 18:14:57,531:INFO:              sktime: 0.26.0
2024-08-31 18:14:57,531:INFO:               tbats: 1.1.3
2024-08-31 18:14:57,531:INFO:            pmdarima: 2.0.4
2024-08-31 18:14:57,531:INFO:              psutil: 5.9.0
2024-08-31 18:14:57,531:INFO:          markupsafe: 2.1.3
2024-08-31 18:14:57,531:INFO:             pickle5: Not installed
2024-08-31 18:14:57,531:INFO:         cloudpickle: 3.0.0
2024-08-31 18:14:57,531:INFO:         deprecation: 2.1.0
2024-08-31 18:14:57,531:INFO:              xxhash: 3.5.0
2024-08-31 18:14:57,531:INFO:           wurlitzer: Not installed
2024-08-31 18:14:57,532:INFO:PyCaret optional dependencies:
2024-08-31 18:14:57,532:INFO:                shap: Not installed
2024-08-31 18:14:57,532:INFO:           interpret: Not installed
2024-08-31 18:14:57,532:INFO:                umap: Not installed
2024-08-31 18:14:57,532:INFO:     ydata_profiling: Not installed
2024-08-31 18:14:57,532:INFO:  explainerdashboard: Not installed
2024-08-31 18:14:57,532:INFO:             autoviz: Not installed
2024-08-31 18:14:57,532:INFO:           fairlearn: Not installed
2024-08-31 18:14:57,532:INFO:          deepchecks: Not installed
2024-08-31 18:14:57,532:INFO:             xgboost: 2.0.2
2024-08-31 18:14:57,532:INFO:            catboost: 1.2.5
2024-08-31 18:14:57,532:INFO:              kmodes: Not installed
2024-08-31 18:14:57,532:INFO:             mlxtend: Not installed
2024-08-31 18:14:57,532:INFO:       statsforecast: Not installed
2024-08-31 18:14:57,532:INFO:        tune_sklearn: Not installed
2024-08-31 18:14:57,532:INFO:                 ray: Not installed
2024-08-31 18:14:57,532:INFO:            hyperopt: Not installed
2024-08-31 18:14:57,532:INFO:              optuna: Not installed
2024-08-31 18:14:57,533:INFO:               skopt: Not installed
2024-08-31 18:14:57,533:INFO:              mlflow: Not installed
2024-08-31 18:14:57,533:INFO:              gradio: 4.41.0
2024-08-31 18:14:57,533:INFO:             fastapi: 0.112.1
2024-08-31 18:14:57,533:INFO:             uvicorn: 0.30.6
2024-08-31 18:14:57,533:INFO:              m2cgen: Not installed
2024-08-31 18:14:57,533:INFO:           evidently: Not installed
2024-08-31 18:14:57,533:INFO:               fugue: Not installed
2024-08-31 18:14:57,533:INFO:           streamlit: Not installed
2024-08-31 18:14:57,533:INFO:             prophet: Not installed
2024-08-31 18:14:57,533:INFO:None
2024-08-31 18:14:57,534:INFO:Set up data.
2024-08-31 18:14:57,549:INFO:Set up folding strategy.
2024-08-31 18:14:57,550:INFO:Set up train/test split.
2024-08-31 18:14:57,564:INFO:Set up index.
2024-08-31 18:14:57,565:INFO:Assigning column types.
2024-08-31 18:14:57,576:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:14:57,613:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,613:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,637:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,639:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,675:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,677:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,699:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,701:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,702:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:14:57,737:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,760:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,762:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,797:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:14:57,819:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,821:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,822:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:14:57,881:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,883:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,944:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:57,945:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:57,947:INFO:Preparing preprocessing pipeline...
2024-08-31 18:14:57,949:INFO:Set up simple imputation.
2024-08-31 18:14:57,950:INFO:Set up column name cleaning.
2024-08-31 18:14:57,981:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:14:57,985:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:14:57,985:INFO:Creating final display dataframe.
2024-08-31 18:14:58,088:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              4769
2024-08-31 18:14:58,154:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:58,157:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:58,216:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:14:58,218:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:14:58,219:INFO:setup() successfully completed in 0.7s...............
2024-08-31 18:14:58,219:INFO:Initializing compare_models()
2024-08-31 18:14:58,219:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 18:14:58,219:INFO:Checking exceptions
2024-08-31 18:14:58,226:INFO:Preparing display monitor
2024-08-31 18:14:58,242:INFO:Initializing Logistic Regression
2024-08-31 18:14:58,243:INFO:Total runtime is 1.6677379608154298e-05 minutes
2024-08-31 18:14:58,246:INFO:SubProcess create_model() called ==================================
2024-08-31 18:14:58,246:INFO:Initializing create_model()
2024-08-31 18:14:58,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:14:58,246:INFO:Checking exceptions
2024-08-31 18:14:58,246:INFO:Importing libraries
2024-08-31 18:14:58,246:INFO:Copying training dataset
2024-08-31 18:14:58,258:INFO:Defining folds
2024-08-31 18:14:58,258:INFO:Declaring metric variables
2024-08-31 18:14:58,261:INFO:Importing untrained model
2024-08-31 18:14:58,264:INFO:Logistic Regression Imported successfully
2024-08-31 18:14:58,270:INFO:Starting cross validation
2024-08-31 18:14:58,270:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:03,917:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:15:03,923:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:15:03,933:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:15:03,947:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:15:03,962:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 18:15:03,995:INFO:Calculating mean and std
2024-08-31 18:15:03,997:INFO:Creating metrics dataframe
2024-08-31 18:15:04,001:INFO:Uploading results into container
2024-08-31 18:15:04,002:INFO:Uploading model into container now
2024-08-31 18:15:04,002:INFO:_master_model_container: 1
2024-08-31 18:15:04,002:INFO:_display_container: 2
2024-08-31 18:15:04,003:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 18:15:04,003:INFO:create_model() successfully completed......................................
2024-08-31 18:15:04,217:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:04,218:INFO:Creating metrics dataframe
2024-08-31 18:15:04,223:INFO:Initializing K Neighbors Classifier
2024-08-31 18:15:04,223:INFO:Total runtime is 0.09968270063400268 minutes
2024-08-31 18:15:04,226:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:04,226:INFO:Initializing create_model()
2024-08-31 18:15:04,226:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:04,226:INFO:Checking exceptions
2024-08-31 18:15:04,226:INFO:Importing libraries
2024-08-31 18:15:04,227:INFO:Copying training dataset
2024-08-31 18:15:04,239:INFO:Defining folds
2024-08-31 18:15:04,239:INFO:Declaring metric variables
2024-08-31 18:15:04,244:INFO:Importing untrained model
2024-08-31 18:15:04,246:INFO:K Neighbors Classifier Imported successfully
2024-08-31 18:15:04,252:INFO:Starting cross validation
2024-08-31 18:15:04,253:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:07,574:INFO:Calculating mean and std
2024-08-31 18:15:07,575:INFO:Creating metrics dataframe
2024-08-31 18:15:07,577:INFO:Uploading results into container
2024-08-31 18:15:07,577:INFO:Uploading model into container now
2024-08-31 18:15:07,578:INFO:_master_model_container: 2
2024-08-31 18:15:07,578:INFO:_display_container: 2
2024-08-31 18:15:07,578:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 18:15:07,578:INFO:create_model() successfully completed......................................
2024-08-31 18:15:07,805:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:07,805:INFO:Creating metrics dataframe
2024-08-31 18:15:07,811:INFO:Initializing Naive Bayes
2024-08-31 18:15:07,812:INFO:Total runtime is 0.15948485533396403 minutes
2024-08-31 18:15:07,815:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:07,815:INFO:Initializing create_model()
2024-08-31 18:15:07,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:07,815:INFO:Checking exceptions
2024-08-31 18:15:07,815:INFO:Importing libraries
2024-08-31 18:15:07,815:INFO:Copying training dataset
2024-08-31 18:15:07,827:INFO:Defining folds
2024-08-31 18:15:07,827:INFO:Declaring metric variables
2024-08-31 18:15:07,830:INFO:Importing untrained model
2024-08-31 18:15:07,833:INFO:Naive Bayes Imported successfully
2024-08-31 18:15:07,838:INFO:Starting cross validation
2024-08-31 18:15:07,839:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:09,885:INFO:Calculating mean and std
2024-08-31 18:15:09,885:INFO:Creating metrics dataframe
2024-08-31 18:15:09,888:INFO:Uploading results into container
2024-08-31 18:15:09,888:INFO:Uploading model into container now
2024-08-31 18:15:09,889:INFO:_master_model_container: 3
2024-08-31 18:15:09,889:INFO:_display_container: 2
2024-08-31 18:15:09,889:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 18:15:09,889:INFO:create_model() successfully completed......................................
2024-08-31 18:15:10,086:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:10,086:INFO:Creating metrics dataframe
2024-08-31 18:15:10,093:INFO:Initializing Decision Tree Classifier
2024-08-31 18:15:10,093:INFO:Total runtime is 0.19751759767532348 minutes
2024-08-31 18:15:10,095:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:10,096:INFO:Initializing create_model()
2024-08-31 18:15:10,096:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:10,096:INFO:Checking exceptions
2024-08-31 18:15:10,096:INFO:Importing libraries
2024-08-31 18:15:10,096:INFO:Copying training dataset
2024-08-31 18:15:10,107:INFO:Defining folds
2024-08-31 18:15:10,108:INFO:Declaring metric variables
2024-08-31 18:15:10,111:INFO:Importing untrained model
2024-08-31 18:15:10,114:INFO:Decision Tree Classifier Imported successfully
2024-08-31 18:15:10,121:INFO:Starting cross validation
2024-08-31 18:15:10,122:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:10,260:INFO:Calculating mean and std
2024-08-31 18:15:10,260:INFO:Creating metrics dataframe
2024-08-31 18:15:10,262:INFO:Uploading results into container
2024-08-31 18:15:10,263:INFO:Uploading model into container now
2024-08-31 18:15:10,263:INFO:_master_model_container: 4
2024-08-31 18:15:10,263:INFO:_display_container: 2
2024-08-31 18:15:10,263:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 18:15:10,263:INFO:create_model() successfully completed......................................
2024-08-31 18:15:10,458:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:10,458:INFO:Creating metrics dataframe
2024-08-31 18:15:10,464:INFO:Initializing SVM - Linear Kernel
2024-08-31 18:15:10,464:INFO:Total runtime is 0.20370011727015178 minutes
2024-08-31 18:15:10,467:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:10,467:INFO:Initializing create_model()
2024-08-31 18:15:10,468:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:10,468:INFO:Checking exceptions
2024-08-31 18:15:10,468:INFO:Importing libraries
2024-08-31 18:15:10,468:INFO:Copying training dataset
2024-08-31 18:15:10,480:INFO:Defining folds
2024-08-31 18:15:10,480:INFO:Declaring metric variables
2024-08-31 18:15:10,483:INFO:Importing untrained model
2024-08-31 18:15:10,486:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 18:15:10,493:INFO:Starting cross validation
2024-08-31 18:15:10,493:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:10,778:INFO:Calculating mean and std
2024-08-31 18:15:10,778:INFO:Creating metrics dataframe
2024-08-31 18:15:10,780:INFO:Uploading results into container
2024-08-31 18:15:10,780:INFO:Uploading model into container now
2024-08-31 18:15:10,781:INFO:_master_model_container: 5
2024-08-31 18:15:10,781:INFO:_display_container: 2
2024-08-31 18:15:10,781:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 18:15:10,781:INFO:create_model() successfully completed......................................
2024-08-31 18:15:10,972:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:10,973:INFO:Creating metrics dataframe
2024-08-31 18:15:10,980:INFO:Initializing Ridge Classifier
2024-08-31 18:15:10,980:INFO:Total runtime is 0.2123001217842102 minutes
2024-08-31 18:15:10,982:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:10,983:INFO:Initializing create_model()
2024-08-31 18:15:10,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:10,983:INFO:Checking exceptions
2024-08-31 18:15:10,983:INFO:Importing libraries
2024-08-31 18:15:10,983:INFO:Copying training dataset
2024-08-31 18:15:10,994:INFO:Defining folds
2024-08-31 18:15:10,994:INFO:Declaring metric variables
2024-08-31 18:15:10,998:INFO:Importing untrained model
2024-08-31 18:15:11,001:INFO:Ridge Classifier Imported successfully
2024-08-31 18:15:11,006:INFO:Starting cross validation
2024-08-31 18:15:11,007:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:11,102:INFO:Calculating mean and std
2024-08-31 18:15:11,103:INFO:Creating metrics dataframe
2024-08-31 18:15:11,105:INFO:Uploading results into container
2024-08-31 18:15:11,105:INFO:Uploading model into container now
2024-08-31 18:15:11,106:INFO:_master_model_container: 6
2024-08-31 18:15:11,106:INFO:_display_container: 2
2024-08-31 18:15:11,106:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 18:15:11,106:INFO:create_model() successfully completed......................................
2024-08-31 18:15:11,324:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:11,325:INFO:Creating metrics dataframe
2024-08-31 18:15:11,334:INFO:Initializing Random Forest Classifier
2024-08-31 18:15:11,334:INFO:Total runtime is 0.21820398569107055 minutes
2024-08-31 18:15:11,338:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:11,339:INFO:Initializing create_model()
2024-08-31 18:15:11,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:11,339:INFO:Checking exceptions
2024-08-31 18:15:11,339:INFO:Importing libraries
2024-08-31 18:15:11,339:INFO:Copying training dataset
2024-08-31 18:15:11,362:INFO:Defining folds
2024-08-31 18:15:11,362:INFO:Declaring metric variables
2024-08-31 18:15:11,366:INFO:Importing untrained model
2024-08-31 18:15:11,370:INFO:Random Forest Classifier Imported successfully
2024-08-31 18:15:11,376:INFO:Starting cross validation
2024-08-31 18:15:11,377:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:12,747:INFO:Calculating mean and std
2024-08-31 18:15:12,749:INFO:Creating metrics dataframe
2024-08-31 18:15:12,751:INFO:Uploading results into container
2024-08-31 18:15:12,751:INFO:Uploading model into container now
2024-08-31 18:15:12,752:INFO:_master_model_container: 7
2024-08-31 18:15:12,752:INFO:_display_container: 2
2024-08-31 18:15:12,753:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 18:15:12,753:INFO:create_model() successfully completed......................................
2024-08-31 18:15:12,963:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:12,963:INFO:Creating metrics dataframe
2024-08-31 18:15:12,970:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 18:15:12,970:INFO:Total runtime is 0.2454625368118286 minutes
2024-08-31 18:15:12,972:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:12,972:INFO:Initializing create_model()
2024-08-31 18:15:12,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:12,972:INFO:Checking exceptions
2024-08-31 18:15:12,972:INFO:Importing libraries
2024-08-31 18:15:12,972:INFO:Copying training dataset
2024-08-31 18:15:12,985:INFO:Defining folds
2024-08-31 18:15:12,985:INFO:Declaring metric variables
2024-08-31 18:15:12,988:INFO:Importing untrained model
2024-08-31 18:15:12,992:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 18:15:12,997:INFO:Starting cross validation
2024-08-31 18:15:12,998:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:13,088:INFO:Calculating mean and std
2024-08-31 18:15:13,089:INFO:Creating metrics dataframe
2024-08-31 18:15:13,090:INFO:Uploading results into container
2024-08-31 18:15:13,091:INFO:Uploading model into container now
2024-08-31 18:15:13,091:INFO:_master_model_container: 8
2024-08-31 18:15:13,091:INFO:_display_container: 2
2024-08-31 18:15:13,091:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 18:15:13,091:INFO:create_model() successfully completed......................................
2024-08-31 18:15:13,288:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:13,288:INFO:Creating metrics dataframe
2024-08-31 18:15:13,295:INFO:Initializing Ada Boost Classifier
2024-08-31 18:15:13,295:INFO:Total runtime is 0.2508887489636739 minutes
2024-08-31 18:15:13,298:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:13,298:INFO:Initializing create_model()
2024-08-31 18:15:13,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:13,298:INFO:Checking exceptions
2024-08-31 18:15:13,298:INFO:Importing libraries
2024-08-31 18:15:13,298:INFO:Copying training dataset
2024-08-31 18:15:13,312:INFO:Defining folds
2024-08-31 18:15:13,313:INFO:Declaring metric variables
2024-08-31 18:15:13,316:INFO:Importing untrained model
2024-08-31 18:15:13,318:INFO:Ada Boost Classifier Imported successfully
2024-08-31 18:15:13,324:INFO:Starting cross validation
2024-08-31 18:15:13,324:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:13,364:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:15:13,372:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:15:13,374:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:15:13,378:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:15:13,378:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 18:15:14,037:INFO:Calculating mean and std
2024-08-31 18:15:14,038:INFO:Creating metrics dataframe
2024-08-31 18:15:14,039:INFO:Uploading results into container
2024-08-31 18:15:14,040:INFO:Uploading model into container now
2024-08-31 18:15:14,040:INFO:_master_model_container: 9
2024-08-31 18:15:14,040:INFO:_display_container: 2
2024-08-31 18:15:14,041:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 18:15:14,041:INFO:create_model() successfully completed......................................
2024-08-31 18:15:14,234:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:14,234:INFO:Creating metrics dataframe
2024-08-31 18:15:14,242:INFO:Initializing Gradient Boosting Classifier
2024-08-31 18:15:14,242:INFO:Total runtime is 0.26666174332300824 minutes
2024-08-31 18:15:14,245:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:14,245:INFO:Initializing create_model()
2024-08-31 18:15:14,245:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:14,245:INFO:Checking exceptions
2024-08-31 18:15:14,245:INFO:Importing libraries
2024-08-31 18:15:14,246:INFO:Copying training dataset
2024-08-31 18:15:14,256:INFO:Defining folds
2024-08-31 18:15:14,256:INFO:Declaring metric variables
2024-08-31 18:15:14,260:INFO:Importing untrained model
2024-08-31 18:15:14,264:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 18:15:14,269:INFO:Starting cross validation
2024-08-31 18:15:14,270:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:16,123:INFO:Calculating mean and std
2024-08-31 18:15:16,124:INFO:Creating metrics dataframe
2024-08-31 18:15:16,126:INFO:Uploading results into container
2024-08-31 18:15:16,127:INFO:Uploading model into container now
2024-08-31 18:15:16,127:INFO:_master_model_container: 10
2024-08-31 18:15:16,127:INFO:_display_container: 2
2024-08-31 18:15:16,128:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 18:15:16,128:INFO:create_model() successfully completed......................................
2024-08-31 18:15:16,318:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:16,318:INFO:Creating metrics dataframe
2024-08-31 18:15:16,326:INFO:Initializing Linear Discriminant Analysis
2024-08-31 18:15:16,326:INFO:Total runtime is 0.301402751604716 minutes
2024-08-31 18:15:16,328:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:16,329:INFO:Initializing create_model()
2024-08-31 18:15:16,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:16,329:INFO:Checking exceptions
2024-08-31 18:15:16,329:INFO:Importing libraries
2024-08-31 18:15:16,329:INFO:Copying training dataset
2024-08-31 18:15:16,341:INFO:Defining folds
2024-08-31 18:15:16,341:INFO:Declaring metric variables
2024-08-31 18:15:16,344:INFO:Importing untrained model
2024-08-31 18:15:16,348:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 18:15:16,353:INFO:Starting cross validation
2024-08-31 18:15:16,354:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:16,467:INFO:Calculating mean and std
2024-08-31 18:15:16,467:INFO:Creating metrics dataframe
2024-08-31 18:15:16,469:INFO:Uploading results into container
2024-08-31 18:15:16,469:INFO:Uploading model into container now
2024-08-31 18:15:16,470:INFO:_master_model_container: 11
2024-08-31 18:15:16,470:INFO:_display_container: 2
2024-08-31 18:15:16,470:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 18:15:16,470:INFO:create_model() successfully completed......................................
2024-08-31 18:15:16,655:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:16,655:INFO:Creating metrics dataframe
2024-08-31 18:15:16,663:INFO:Initializing Extra Trees Classifier
2024-08-31 18:15:16,663:INFO:Total runtime is 0.3070196866989136 minutes
2024-08-31 18:15:16,667:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:16,667:INFO:Initializing create_model()
2024-08-31 18:15:16,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:16,667:INFO:Checking exceptions
2024-08-31 18:15:16,667:INFO:Importing libraries
2024-08-31 18:15:16,667:INFO:Copying training dataset
2024-08-31 18:15:16,678:INFO:Defining folds
2024-08-31 18:15:16,678:INFO:Declaring metric variables
2024-08-31 18:15:16,681:INFO:Importing untrained model
2024-08-31 18:15:16,684:INFO:Extra Trees Classifier Imported successfully
2024-08-31 18:15:16,689:INFO:Starting cross validation
2024-08-31 18:15:16,690:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:17,842:INFO:Calculating mean and std
2024-08-31 18:15:17,842:INFO:Creating metrics dataframe
2024-08-31 18:15:17,844:INFO:Uploading results into container
2024-08-31 18:15:17,844:INFO:Uploading model into container now
2024-08-31 18:15:17,845:INFO:_master_model_container: 12
2024-08-31 18:15:17,845:INFO:_display_container: 2
2024-08-31 18:15:17,845:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 18:15:17,846:INFO:create_model() successfully completed......................................
2024-08-31 18:15:18,039:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:18,039:INFO:Creating metrics dataframe
2024-08-31 18:15:18,047:INFO:Initializing Extreme Gradient Boosting
2024-08-31 18:15:18,047:INFO:Total runtime is 0.3300810138384501 minutes
2024-08-31 18:15:18,049:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:18,050:INFO:Initializing create_model()
2024-08-31 18:15:18,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:18,050:INFO:Checking exceptions
2024-08-31 18:15:18,050:INFO:Importing libraries
2024-08-31 18:15:18,050:INFO:Copying training dataset
2024-08-31 18:15:18,061:INFO:Defining folds
2024-08-31 18:15:18,062:INFO:Declaring metric variables
2024-08-31 18:15:18,065:INFO:Importing untrained model
2024-08-31 18:15:18,069:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 18:15:18,073:INFO:Starting cross validation
2024-08-31 18:15:18,074:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:18,451:INFO:Calculating mean and std
2024-08-31 18:15:18,453:INFO:Creating metrics dataframe
2024-08-31 18:15:18,454:INFO:Uploading results into container
2024-08-31 18:15:18,454:INFO:Uploading model into container now
2024-08-31 18:15:18,455:INFO:_master_model_container: 13
2024-08-31 18:15:18,455:INFO:_display_container: 2
2024-08-31 18:15:18,455:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 18:15:18,455:INFO:create_model() successfully completed......................................
2024-08-31 18:15:18,657:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:18,657:INFO:Creating metrics dataframe
2024-08-31 18:15:18,670:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 18:15:18,670:INFO:Total runtime is 0.3404641389846802 minutes
2024-08-31 18:15:18,675:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:18,675:INFO:Initializing create_model()
2024-08-31 18:15:18,675:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:18,675:INFO:Checking exceptions
2024-08-31 18:15:18,675:INFO:Importing libraries
2024-08-31 18:15:18,675:INFO:Copying training dataset
2024-08-31 18:15:18,694:INFO:Defining folds
2024-08-31 18:15:18,694:INFO:Declaring metric variables
2024-08-31 18:15:18,698:INFO:Importing untrained model
2024-08-31 18:15:18,701:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:15:18,709:INFO:Starting cross validation
2024-08-31 18:15:18,710:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:19,364:INFO:Calculating mean and std
2024-08-31 18:15:19,366:INFO:Creating metrics dataframe
2024-08-31 18:15:19,368:INFO:Uploading results into container
2024-08-31 18:15:19,369:INFO:Uploading model into container now
2024-08-31 18:15:19,369:INFO:_master_model_container: 14
2024-08-31 18:15:19,369:INFO:_display_container: 2
2024-08-31 18:15:19,371:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:15:19,371:INFO:create_model() successfully completed......................................
2024-08-31 18:15:19,583:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:19,583:INFO:Creating metrics dataframe
2024-08-31 18:15:19,594:INFO:Initializing CatBoost Classifier
2024-08-31 18:15:19,594:INFO:Total runtime is 0.3558668295542399 minutes
2024-08-31 18:15:19,597:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:19,597:INFO:Initializing create_model()
2024-08-31 18:15:19,597:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:19,598:INFO:Checking exceptions
2024-08-31 18:15:19,598:INFO:Importing libraries
2024-08-31 18:15:19,598:INFO:Copying training dataset
2024-08-31 18:15:19,614:INFO:Defining folds
2024-08-31 18:15:19,614:INFO:Declaring metric variables
2024-08-31 18:15:19,619:INFO:Importing untrained model
2024-08-31 18:15:19,623:INFO:CatBoost Classifier Imported successfully
2024-08-31 18:15:19,630:INFO:Starting cross validation
2024-08-31 18:15:19,631:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:38,319:INFO:Calculating mean and std
2024-08-31 18:15:38,320:INFO:Creating metrics dataframe
2024-08-31 18:15:38,322:INFO:Uploading results into container
2024-08-31 18:15:38,323:INFO:Uploading model into container now
2024-08-31 18:15:38,323:INFO:_master_model_container: 15
2024-08-31 18:15:38,323:INFO:_display_container: 2
2024-08-31 18:15:38,324:INFO:<catboost.core.CatBoostClassifier object at 0x00000266A2673610>
2024-08-31 18:15:38,324:INFO:create_model() successfully completed......................................
2024-08-31 18:15:38,581:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:38,581:INFO:Creating metrics dataframe
2024-08-31 18:15:38,592:INFO:Initializing Dummy Classifier
2024-08-31 18:15:38,593:INFO:Total runtime is 0.6725171963373819 minutes
2024-08-31 18:15:38,597:INFO:SubProcess create_model() called ==================================
2024-08-31 18:15:38,597:INFO:Initializing create_model()
2024-08-31 18:15:38,597:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8B7D710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:38,597:INFO:Checking exceptions
2024-08-31 18:15:38,597:INFO:Importing libraries
2024-08-31 18:15:38,597:INFO:Copying training dataset
2024-08-31 18:15:38,612:INFO:Defining folds
2024-08-31 18:15:38,612:INFO:Declaring metric variables
2024-08-31 18:15:38,615:INFO:Importing untrained model
2024-08-31 18:15:38,620:INFO:Dummy Classifier Imported successfully
2024-08-31 18:15:38,629:INFO:Starting cross validation
2024-08-31 18:15:38,630:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:38,706:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:15:38,706:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:15:38,709:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:15:38,713:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:15:38,716:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 18:15:38,723:INFO:Calculating mean and std
2024-08-31 18:15:38,725:INFO:Creating metrics dataframe
2024-08-31 18:15:38,728:INFO:Uploading results into container
2024-08-31 18:15:38,728:INFO:Uploading model into container now
2024-08-31 18:15:38,729:INFO:_master_model_container: 16
2024-08-31 18:15:38,729:INFO:_display_container: 2
2024-08-31 18:15:38,729:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 18:15:38,730:INFO:create_model() successfully completed......................................
2024-08-31 18:15:38,956:INFO:SubProcess create_model() end ==================================
2024-08-31 18:15:38,956:INFO:Creating metrics dataframe
2024-08-31 18:15:38,976:INFO:Initializing create_model()
2024-08-31 18:15:38,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:38,977:INFO:Checking exceptions
2024-08-31 18:15:38,978:INFO:Importing libraries
2024-08-31 18:15:38,978:INFO:Copying training dataset
2024-08-31 18:15:38,992:INFO:Defining folds
2024-08-31 18:15:38,992:INFO:Declaring metric variables
2024-08-31 18:15:38,992:INFO:Importing untrained model
2024-08-31 18:15:38,992:INFO:Declaring custom model
2024-08-31 18:15:38,993:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:15:38,993:INFO:Cross validation set to False
2024-08-31 18:15:38,993:INFO:Fitting Model
2024-08-31 18:15:39,028:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:15:39,029:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:15:39,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.
2024-08-31 18:15:39,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:15:39,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:15:39,031:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:15:39,031:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:15:39,031:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:15:39,031:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:15:39,184:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:15:39,185:INFO:create_model() successfully completed......................................
2024-08-31 18:15:39,486:INFO:_master_model_container: 16
2024-08-31 18:15:39,486:INFO:_display_container: 2
2024-08-31 18:15:39,488:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:15:39,488:INFO:compare_models() successfully completed......................................
2024-08-31 18:15:39,489:INFO:Initializing create_model()
2024-08-31 18:15:39,489:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:15:39,489:INFO:Checking exceptions
2024-08-31 18:15:39,508:INFO:Importing libraries
2024-08-31 18:15:39,508:INFO:Copying training dataset
2024-08-31 18:15:39,526:INFO:Defining folds
2024-08-31 18:15:39,526:INFO:Declaring metric variables
2024-08-31 18:15:39,529:INFO:Importing untrained model
2024-08-31 18:15:39,530:INFO:Declaring custom model
2024-08-31 18:15:39,534:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:15:39,544:INFO:Starting cross validation
2024-08-31 18:15:39,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:15:41,100:INFO:Calculating mean and std
2024-08-31 18:15:41,103:INFO:Creating metrics dataframe
2024-08-31 18:15:41,109:INFO:Finalizing model
2024-08-31 18:15:41,163:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:15:41,164:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:15:41,166:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.
2024-08-31 18:15:41,166:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:15:41,166:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:15:41,166:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:15:41,166:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:15:41,167:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:15:41,167:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:15:41,380:INFO:Uploading results into container
2024-08-31 18:15:41,383:INFO:Uploading model into container now
2024-08-31 18:15:41,401:INFO:_master_model_container: 17
2024-08-31 18:15:41,401:INFO:_display_container: 3
2024-08-31 18:15:41,402:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:15:41,404:INFO:create_model() successfully completed......................................
2024-08-31 18:15:41,672:INFO:Initializing tune_model()
2024-08-31 18:15:41,672:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 18:15:41,672:INFO:Checking exceptions
2024-08-31 18:15:41,689:INFO:Copying training dataset
2024-08-31 18:15:41,699:INFO:Checking base model
2024-08-31 18:15:41,699:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 18:15:41,703:INFO:Declaring metric variables
2024-08-31 18:15:41,705:INFO:Defining Hyperparameters
2024-08-31 18:15:41,919:INFO:Tuning with n_jobs=-1
2024-08-31 18:15:41,919:INFO:Initializing RandomizedSearchCV
2024-08-31 18:16:02,552:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 18:16:02,555:INFO:Hyperparameter search completed
2024-08-31 18:16:02,555:INFO:SubProcess create_model() called ==================================
2024-08-31 18:16:02,556:INFO:Initializing create_model()
2024-08-31 18:16:02,556:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A8AD8A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 18:16:02,557:INFO:Checking exceptions
2024-08-31 18:16:02,557:INFO:Importing libraries
2024-08-31 18:16:02,557:INFO:Copying training dataset
2024-08-31 18:16:02,578:INFO:Defining folds
2024-08-31 18:16:02,578:INFO:Declaring metric variables
2024-08-31 18:16:02,582:INFO:Importing untrained model
2024-08-31 18:16:02,582:INFO:Declaring custom model
2024-08-31 18:16:02,587:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:16:02,596:INFO:Starting cross validation
2024-08-31 18:16:02,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:16:04,930:INFO:Calculating mean and std
2024-08-31 18:16:04,932:INFO:Creating metrics dataframe
2024-08-31 18:16:04,941:INFO:Finalizing model
2024-08-31 18:16:04,979:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:16:04,979:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:16:04,979:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:16:04,990:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:16:04,990:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:16:04,990:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:16:04,991:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:16:04,991:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:16:04,993:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.
2024-08-31 18:16:04,993:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:16:04,993:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:16:04,993:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:16:04,993:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:16:04,994:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:16:04,994:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:16:04,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:04,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,310:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,359:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,370:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,371:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,389:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:16:05,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:16:05,410:INFO:Uploading results into container
2024-08-31 18:16:05,411:INFO:Uploading model into container now
2024-08-31 18:16:05,412:INFO:_master_model_container: 18
2024-08-31 18:16:05,412:INFO:_display_container: 4
2024-08-31 18:16:05,414:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:16:05,414:INFO:create_model() successfully completed......................................
2024-08-31 18:16:05,675:INFO:SubProcess create_model() end ==================================
2024-08-31 18:16:05,675:INFO:choose_better activated
2024-08-31 18:16:05,678:INFO:SubProcess create_model() called ==================================
2024-08-31 18:16:05,679:INFO:Initializing create_model()
2024-08-31 18:16:05,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:16:05,679:INFO:Checking exceptions
2024-08-31 18:16:05,680:INFO:Importing libraries
2024-08-31 18:16:05,680:INFO:Copying training dataset
2024-08-31 18:16:05,692:INFO:Defining folds
2024-08-31 18:16:05,692:INFO:Declaring metric variables
2024-08-31 18:16:05,693:INFO:Importing untrained model
2024-08-31 18:16:05,693:INFO:Declaring custom model
2024-08-31 18:16:05,693:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:16:05,694:INFO:Starting cross validation
2024-08-31 18:16:05,694:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:16:07,090:INFO:Calculating mean and std
2024-08-31 18:16:07,091:INFO:Creating metrics dataframe
2024-08-31 18:16:07,094:INFO:Finalizing model
2024-08-31 18:16:07,134:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:16:07,134:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:16:07,136:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.
2024-08-31 18:16:07,137:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:16:07,137:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:16:07,137:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:16:07,137:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:16:07,138:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:16:07,138:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:16:07,289:INFO:Uploading results into container
2024-08-31 18:16:07,290:INFO:Uploading model into container now
2024-08-31 18:16:07,290:INFO:_master_model_container: 19
2024-08-31 18:16:07,290:INFO:_display_container: 5
2024-08-31 18:16:07,291:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:16:07,291:INFO:create_model() successfully completed......................................
2024-08-31 18:16:07,544:INFO:SubProcess create_model() end ==================================
2024-08-31 18:16:07,545:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 18:16:07,545:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-08-31 18:16:07,545:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 18:16:07,545:INFO:choose_better completed
2024-08-31 18:16:07,547:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 18:16:07,557:INFO:_master_model_container: 19
2024-08-31 18:16:07,557:INFO:_display_container: 4
2024-08-31 18:16:07,557:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:16:07,557:INFO:tune_model() successfully completed......................................
2024-08-31 18:16:07,771:INFO:Initializing finalize_model()
2024-08-31 18:16:07,771:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 18:16:07,773:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:16:07,777:INFO:Initializing create_model()
2024-08-31 18:16:07,778:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:16:07,778:INFO:Checking exceptions
2024-08-31 18:16:07,779:INFO:Importing libraries
2024-08-31 18:16:07,779:INFO:Copying training dataset
2024-08-31 18:16:07,779:INFO:Defining folds
2024-08-31 18:16:07,779:INFO:Declaring metric variables
2024-08-31 18:16:07,780:INFO:Importing untrained model
2024-08-31 18:16:07,780:INFO:Declaring custom model
2024-08-31 18:16:07,780:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:16:07,781:INFO:Cross validation set to False
2024-08-31 18:16:07,781:INFO:Fitting Model
2024-08-31 18:16:07,811:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:16:07,811:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-08-31 18:16:07,813:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.
2024-08-31 18:16:07,813:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:16:07,813:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:16:07,813:INFO:[LightGBM] [Info] Total Bins 681
2024-08-31 18:16:07,814:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-08-31 18:16:07,814:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-08-31 18:16:07,814:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-08-31 18:16:07,930:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:16:07,930:INFO:create_model() successfully completed......................................
2024-08-31 18:16:08,145:INFO:_master_model_container: 19
2024-08-31 18:16:08,145:INFO:_display_container: 4
2024-08-31 18:16:08,149:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:16:08,149:INFO:finalize_model() successfully completed......................................
2024-08-31 18:16:08,341:INFO:Initializing predict_model()
2024-08-31 18:16:08,341:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266EF8702C0>)
2024-08-31 18:16:08,341:INFO:Checking exceptions
2024-08-31 18:16:08,341:INFO:Preloading libraries
2024-08-31 18:16:08,343:INFO:Set up data.
2024-08-31 18:16:08,348:INFO:Set up index.
2024-08-31 18:16:08,648:INFO:Initializing evaluate_model()
2024-08-31 18:16:08,648:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-31 18:16:08,661:INFO:Initializing plot_model()
2024-08-31 18:16:08,661:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:16:08,662:INFO:Checking exceptions
2024-08-31 18:16:08,667:INFO:Preloading libraries
2024-08-31 18:16:08,671:INFO:Copying training dataset
2024-08-31 18:16:08,672:INFO:Plot type: pipeline
2024-08-31 18:16:08,782:INFO:Visual Rendered Successfully
2024-08-31 18:16:08,969:INFO:plot_model() successfully completed......................................
2024-08-31 18:16:17,942:INFO:Initializing plot_model()
2024-08-31 18:16:17,942:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A6E38290>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:16:17,942:INFO:Checking exceptions
2024-08-31 18:16:17,948:INFO:Preloading libraries
2024-08-31 18:16:17,953:INFO:Copying training dataset
2024-08-31 18:16:17,953:INFO:Plot type: parameter
2024-08-31 18:16:17,958:INFO:Visual Rendered Successfully
2024-08-31 18:16:18,177:INFO:plot_model() successfully completed......................................
2024-08-31 18:17:47,844:INFO:PyCaret ClassificationExperiment
2024-08-31 18:17:47,845:INFO:Logging name: clf-default-name
2024-08-31 18:17:47,845:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 18:17:47,845:INFO:version 3.3.2
2024-08-31 18:17:47,845:INFO:Initializing setup()
2024-08-31 18:17:47,845:INFO:self.USI: b2d1
2024-08-31 18:17:47,845:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 18:17:47,845:INFO:Checking environment
2024-08-31 18:17:47,845:INFO:python_version: 3.11.9
2024-08-31 18:17:47,845:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 18:17:47,845:INFO:machine: AMD64
2024-08-31 18:17:47,845:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 18:17:47,850:INFO:Memory: svmem(total=16867028992, available=2880589824, percent=82.9, used=13986439168, free=2880589824)
2024-08-31 18:17:47,851:INFO:Physical Core: 6
2024-08-31 18:17:47,851:INFO:Logical Core: 12
2024-08-31 18:17:47,851:INFO:Checking libraries
2024-08-31 18:17:47,851:INFO:System:
2024-08-31 18:17:47,851:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 18:17:47,851:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 18:17:47,851:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 18:17:47,851:INFO:PyCaret required dependencies:
2024-08-31 18:17:47,851:INFO:                 pip: 23.2.1
2024-08-31 18:17:47,851:INFO:          setuptools: 67.8.0
2024-08-31 18:17:47,851:INFO:             pycaret: 3.3.2
2024-08-31 18:17:47,851:INFO:             IPython: 8.14.0
2024-08-31 18:17:47,851:INFO:          ipywidgets: 8.1.5
2024-08-31 18:17:47,851:INFO:                tqdm: 4.66.5
2024-08-31 18:17:47,851:INFO:               numpy: 1.24.3
2024-08-31 18:17:47,851:INFO:              pandas: 2.0.3
2024-08-31 18:17:47,851:INFO:              jinja2: 3.1.4
2024-08-31 18:17:47,851:INFO:               scipy: 1.10.1
2024-08-31 18:17:47,851:INFO:              joblib: 1.2.0
2024-08-31 18:17:47,851:INFO:             sklearn: 1.4.2
2024-08-31 18:17:47,852:INFO:                pyod: 2.0.1
2024-08-31 18:17:47,852:INFO:            imblearn: 0.12.3
2024-08-31 18:17:47,852:INFO:   category_encoders: 2.6.3
2024-08-31 18:17:47,852:INFO:            lightgbm: 4.5.0
2024-08-31 18:17:47,852:INFO:               numba: 0.60.0
2024-08-31 18:17:47,852:INFO:            requests: 2.32.3
2024-08-31 18:17:47,852:INFO:          matplotlib: 3.7.1
2024-08-31 18:17:47,852:INFO:          scikitplot: 0.3.7
2024-08-31 18:17:47,852:INFO:         yellowbrick: 1.5
2024-08-31 18:17:47,852:INFO:              plotly: 5.16.1
2024-08-31 18:17:47,852:INFO:    plotly-resampler: Not installed
2024-08-31 18:17:47,852:INFO:             kaleido: 0.2.1
2024-08-31 18:17:47,852:INFO:           schemdraw: 0.15
2024-08-31 18:17:47,852:INFO:         statsmodels: 0.14.2
2024-08-31 18:17:47,852:INFO:              sktime: 0.26.0
2024-08-31 18:17:47,852:INFO:               tbats: 1.1.3
2024-08-31 18:17:47,852:INFO:            pmdarima: 2.0.4
2024-08-31 18:17:47,852:INFO:              psutil: 5.9.0
2024-08-31 18:17:47,852:INFO:          markupsafe: 2.1.3
2024-08-31 18:17:47,852:INFO:             pickle5: Not installed
2024-08-31 18:17:47,852:INFO:         cloudpickle: 3.0.0
2024-08-31 18:17:47,852:INFO:         deprecation: 2.1.0
2024-08-31 18:17:47,852:INFO:              xxhash: 3.5.0
2024-08-31 18:17:47,852:INFO:           wurlitzer: Not installed
2024-08-31 18:17:47,852:INFO:PyCaret optional dependencies:
2024-08-31 18:17:47,852:INFO:                shap: Not installed
2024-08-31 18:17:47,852:INFO:           interpret: Not installed
2024-08-31 18:17:47,852:INFO:                umap: Not installed
2024-08-31 18:17:47,852:INFO:     ydata_profiling: Not installed
2024-08-31 18:17:47,852:INFO:  explainerdashboard: Not installed
2024-08-31 18:17:47,852:INFO:             autoviz: Not installed
2024-08-31 18:17:47,852:INFO:           fairlearn: Not installed
2024-08-31 18:17:47,852:INFO:          deepchecks: Not installed
2024-08-31 18:17:47,852:INFO:             xgboost: 2.0.2
2024-08-31 18:17:47,852:INFO:            catboost: 1.2.5
2024-08-31 18:17:47,853:INFO:              kmodes: Not installed
2024-08-31 18:17:47,853:INFO:             mlxtend: Not installed
2024-08-31 18:17:47,853:INFO:       statsforecast: Not installed
2024-08-31 18:17:47,853:INFO:        tune_sklearn: Not installed
2024-08-31 18:17:47,853:INFO:                 ray: Not installed
2024-08-31 18:17:47,853:INFO:            hyperopt: Not installed
2024-08-31 18:17:47,853:INFO:              optuna: Not installed
2024-08-31 18:17:47,853:INFO:               skopt: Not installed
2024-08-31 18:17:47,853:INFO:              mlflow: Not installed
2024-08-31 18:17:47,853:INFO:              gradio: 4.41.0
2024-08-31 18:17:47,853:INFO:             fastapi: 0.112.1
2024-08-31 18:17:47,853:INFO:             uvicorn: 0.30.6
2024-08-31 18:17:47,853:INFO:              m2cgen: Not installed
2024-08-31 18:17:47,853:INFO:           evidently: Not installed
2024-08-31 18:17:47,853:INFO:               fugue: Not installed
2024-08-31 18:17:47,853:INFO:           streamlit: Not installed
2024-08-31 18:17:47,853:INFO:             prophet: Not installed
2024-08-31 18:17:47,853:INFO:None
2024-08-31 18:17:47,853:INFO:Set up data.
2024-08-31 18:17:47,863:INFO:Set up folding strategy.
2024-08-31 18:17:47,863:INFO:Set up train/test split.
2024-08-31 18:17:47,875:INFO:Set up index.
2024-08-31 18:17:47,875:INFO:Assigning column types.
2024-08-31 18:17:47,882:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 18:17:47,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:17:47,920:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:17:47,942:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:47,945:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:47,986:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 18:17:47,987:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:17:48,009:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,011:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,012:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 18:17:48,048:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:17:48,071:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,073:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,108:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 18:17:48,130:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,132:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,133:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 18:17:48,190:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,192:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,250:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,252:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,253:INFO:Preparing preprocessing pipeline...
2024-08-31 18:17:48,255:INFO:Set up simple imputation.
2024-08-31 18:17:48,256:INFO:Set up column name cleaning.
2024-08-31 18:17:48,287:INFO:Finished creating preprocessing pipeline.
2024-08-31 18:17:48,290:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 18:17:48,290:INFO:Creating final display dataframe.
2024-08-31 18:17:48,391:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              b2d1
2024-08-31 18:17:48,454:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,456:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,515:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 18:17:48,518:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 18:17:48,519:INFO:setup() successfully completed in 0.68s...............
2024-08-31 18:17:48,519:INFO:Initializing create_model()
2024-08-31 18:17:48,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:17:48,519:INFO:Checking exceptions
2024-08-31 18:17:48,529:INFO:Importing libraries
2024-08-31 18:17:48,529:INFO:Copying training dataset
2024-08-31 18:17:48,548:INFO:Defining folds
2024-08-31 18:17:48,548:INFO:Declaring metric variables
2024-08-31 18:17:48,551:INFO:Importing untrained model
2024-08-31 18:17:48,554:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:17:48,560:INFO:Starting cross validation
2024-08-31 18:17:48,561:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:17:50,085:INFO:Calculating mean and std
2024-08-31 18:17:50,088:INFO:Creating metrics dataframe
2024-08-31 18:17:50,099:INFO:Finalizing model
2024-08-31 18:17:50,143:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:17:50,143:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:17:50,147:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001300 seconds.
2024-08-31 18:17:50,147:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:17:50,147:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:17:50,148:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:17:50,148:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:17:50,148:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:17:50,148:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:17:50,355:INFO:Uploading results into container
2024-08-31 18:17:50,357:INFO:Uploading model into container now
2024-08-31 18:17:50,371:INFO:_master_model_container: 1
2024-08-31 18:17:50,371:INFO:_display_container: 2
2024-08-31 18:17:50,372:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:17:50,372:INFO:create_model() successfully completed......................................
2024-08-31 18:17:50,612:INFO:Initializing tune_model()
2024-08-31 18:17:50,612:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 18:17:50,612:INFO:Checking exceptions
2024-08-31 18:17:50,630:INFO:Copying training dataset
2024-08-31 18:17:50,637:INFO:Checking base model
2024-08-31 18:17:50,637:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 18:17:50,641:INFO:Declaring metric variables
2024-08-31 18:17:50,644:INFO:Defining Hyperparameters
2024-08-31 18:17:50,841:INFO:Tuning with n_jobs=-1
2024-08-31 18:17:50,841:INFO:Initializing RandomizedSearchCV
2024-08-31 18:18:09,833:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 18:18:09,835:INFO:Hyperparameter search completed
2024-08-31 18:18:09,835:INFO:SubProcess create_model() called ==================================
2024-08-31 18:18:09,837:INFO:Initializing create_model()
2024-08-31 18:18:09,837:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266A7691790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 18:18:09,837:INFO:Checking exceptions
2024-08-31 18:18:09,837:INFO:Importing libraries
2024-08-31 18:18:09,837:INFO:Copying training dataset
2024-08-31 18:18:09,859:INFO:Defining folds
2024-08-31 18:18:09,859:INFO:Declaring metric variables
2024-08-31 18:18:09,863:INFO:Importing untrained model
2024-08-31 18:18:09,863:INFO:Declaring custom model
2024-08-31 18:18:09,867:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:18:09,875:INFO:Starting cross validation
2024-08-31 18:18:09,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:18:11,754:INFO:Calculating mean and std
2024-08-31 18:18:11,755:INFO:Creating metrics dataframe
2024-08-31 18:18:11,763:INFO:Finalizing model
2024-08-31 18:18:11,793:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:18:11,794:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:18:11,794:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:18:11,804:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:18:11,805:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 18:18:11,805:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 18:18:11,805:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 18:18:11,805:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:18:11,806:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.
2024-08-31 18:18:11,807:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:18:11,807:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:18:11,807:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:18:11,807:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:18:11,808:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:18:11,808:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:18:11,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,958:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:11,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:11,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:11,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,986:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:11,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,986:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:11,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:12,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:12,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,031:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 18:18:12,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 18:18:12,049:INFO:Uploading results into container
2024-08-31 18:18:12,050:INFO:Uploading model into container now
2024-08-31 18:18:12,051:INFO:_master_model_container: 2
2024-08-31 18:18:12,051:INFO:_display_container: 3
2024-08-31 18:18:12,052:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:18:12,052:INFO:create_model() successfully completed......................................
2024-08-31 18:18:12,285:INFO:SubProcess create_model() end ==================================
2024-08-31 18:18:12,285:INFO:choose_better activated
2024-08-31 18:18:12,287:INFO:SubProcess create_model() called ==================================
2024-08-31 18:18:12,288:INFO:Initializing create_model()
2024-08-31 18:18:12,288:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:18:12,288:INFO:Checking exceptions
2024-08-31 18:18:12,290:INFO:Importing libraries
2024-08-31 18:18:12,290:INFO:Copying training dataset
2024-08-31 18:18:12,300:INFO:Defining folds
2024-08-31 18:18:12,300:INFO:Declaring metric variables
2024-08-31 18:18:12,301:INFO:Importing untrained model
2024-08-31 18:18:12,301:INFO:Declaring custom model
2024-08-31 18:18:12,301:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:18:12,301:INFO:Starting cross validation
2024-08-31 18:18:12,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 18:18:13,745:INFO:Calculating mean and std
2024-08-31 18:18:13,746:INFO:Creating metrics dataframe
2024-08-31 18:18:13,748:INFO:Finalizing model
2024-08-31 18:18:13,784:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:18:13,784:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-08-31 18:18:13,785:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.
2024-08-31 18:18:13,785:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:18:13,785:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:18:13,785:INFO:[LightGBM] [Info] Total Bins 665
2024-08-31 18:18:13,786:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-08-31 18:18:13,786:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-08-31 18:18:13,786:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-08-31 18:18:13,921:INFO:Uploading results into container
2024-08-31 18:18:13,922:INFO:Uploading model into container now
2024-08-31 18:18:13,923:INFO:_master_model_container: 3
2024-08-31 18:18:13,923:INFO:_display_container: 4
2024-08-31 18:18:13,923:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:18:13,923:INFO:create_model() successfully completed......................................
2024-08-31 18:18:14,139:INFO:SubProcess create_model() end ==================================
2024-08-31 18:18:14,139:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 18:18:14,141:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-08-31 18:18:14,141:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 18:18:14,141:INFO:choose_better completed
2024-08-31 18:18:14,141:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 18:18:14,149:INFO:_master_model_container: 3
2024-08-31 18:18:14,149:INFO:_display_container: 3
2024-08-31 18:18:14,150:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:18:14,150:INFO:tune_model() successfully completed......................................
2024-08-31 18:18:14,392:INFO:Initializing finalize_model()
2024-08-31 18:18:14,392:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 18:18:14,393:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 18:18:14,400:INFO:Initializing create_model()
2024-08-31 18:18:14,400:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 18:18:14,400:INFO:Checking exceptions
2024-08-31 18:18:14,400:INFO:Importing libraries
2024-08-31 18:18:14,401:INFO:Copying training dataset
2024-08-31 18:18:14,401:INFO:Defining folds
2024-08-31 18:18:14,401:INFO:Declaring metric variables
2024-08-31 18:18:14,401:INFO:Importing untrained model
2024-08-31 18:18:14,401:INFO:Declaring custom model
2024-08-31 18:18:14,402:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 18:18:14,402:INFO:Cross validation set to False
2024-08-31 18:18:14,402:INFO:Fitting Model
2024-08-31 18:18:14,436:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 18:18:14,436:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-08-31 18:18:14,438:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.
2024-08-31 18:18:14,438:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 18:18:14,438:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 18:18:14,438:INFO:[LightGBM] [Info] Total Bins 681
2024-08-31 18:18:14,438:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-08-31 18:18:14,439:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-08-31 18:18:14,439:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-08-31 18:18:14,590:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:18:14,590:INFO:create_model() successfully completed......................................
2024-08-31 18:18:14,808:INFO:_master_model_container: 3
2024-08-31 18:18:14,808:INFO:_display_container: 3
2024-08-31 18:18:14,812:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 18:18:14,812:INFO:finalize_model() successfully completed......................................
2024-08-31 18:18:15,007:INFO:Initializing evaluate_model()
2024-08-31 18:18:15,007:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-08-31 18:18:15,021:INFO:Initializing plot_model()
2024-08-31 18:18:15,021:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:18:15,021:INFO:Checking exceptions
2024-08-31 18:18:15,025:INFO:Preloading libraries
2024-08-31 18:18:15,030:INFO:Copying training dataset
2024-08-31 18:18:15,030:INFO:Plot type: pipeline
2024-08-31 18:18:15,138:INFO:Visual Rendered Successfully
2024-08-31 18:18:15,336:INFO:plot_model() successfully completed......................................
2024-08-31 18:18:17,325:INFO:Initializing plot_model()
2024-08-31 18:18:17,326:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-08-31 18:18:17,326:INFO:Checking exceptions
2024-08-31 18:18:17,330:INFO:Preloading libraries
2024-08-31 18:18:17,336:INFO:Copying training dataset
2024-08-31 18:18:17,336:INFO:Plot type: parameter
2024-08-31 18:18:17,340:INFO:Visual Rendered Successfully
2024-08-31 18:18:17,556:INFO:plot_model() successfully completed......................................
2024-08-31 18:20:39,464:INFO:Initializing predict_model()
2024-08-31 18:20:39,464:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266A6DC8680>)
2024-08-31 18:20:39,465:INFO:Checking exceptions
2024-08-31 18:20:39,465:INFO:Preloading libraries
2024-08-31 18:20:39,466:INFO:Set up data.
2024-08-31 18:20:39,473:INFO:Set up index.
2024-08-31 18:21:19,411:INFO:Initializing predict_model()
2024-08-31 18:21:19,411:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266A8D3EE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266A3C42520>)
2024-08-31 18:21:19,411:INFO:Checking exceptions
2024-08-31 18:21:19,411:INFO:Preloading libraries
2024-08-31 18:21:19,412:INFO:Set up data.
2024-08-31 18:21:19,417:INFO:Set up index.
2024-08-31 21:12:03,245:INFO:PyCaret ClassificationExperiment
2024-08-31 21:12:03,245:INFO:Logging name: clf-default-name
2024-08-31 21:12:03,245:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 21:12:03,245:INFO:version 3.3.2
2024-08-31 21:12:03,245:INFO:Initializing setup()
2024-08-31 21:12:03,245:INFO:self.USI: 8215
2024-08-31 21:12:03,245:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 21:12:03,245:INFO:Checking environment
2024-08-31 21:12:03,245:INFO:python_version: 3.11.9
2024-08-31 21:12:03,245:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 21:12:03,245:INFO:machine: AMD64
2024-08-31 21:12:03,245:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 21:12:03,251:INFO:Memory: svmem(total=16867028992, available=3509227520, percent=79.2, used=13357801472, free=3509227520)
2024-08-31 21:12:03,251:INFO:Physical Core: 6
2024-08-31 21:12:03,251:INFO:Logical Core: 12
2024-08-31 21:12:03,251:INFO:Checking libraries
2024-08-31 21:12:03,251:INFO:System:
2024-08-31 21:12:03,251:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 21:12:03,251:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 21:12:03,251:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 21:12:03,251:INFO:PyCaret required dependencies:
2024-08-31 21:12:03,251:INFO:                 pip: 23.2.1
2024-08-31 21:12:03,251:INFO:          setuptools: 67.8.0
2024-08-31 21:12:03,251:INFO:             pycaret: 3.3.2
2024-08-31 21:12:03,251:INFO:             IPython: 8.14.0
2024-08-31 21:12:03,251:INFO:          ipywidgets: 8.1.5
2024-08-31 21:12:03,251:INFO:                tqdm: 4.66.5
2024-08-31 21:12:03,251:INFO:               numpy: 1.24.3
2024-08-31 21:12:03,251:INFO:              pandas: 2.0.3
2024-08-31 21:12:03,251:INFO:              jinja2: 3.1.4
2024-08-31 21:12:03,251:INFO:               scipy: 1.10.1
2024-08-31 21:12:03,251:INFO:              joblib: 1.2.0
2024-08-31 21:12:03,251:INFO:             sklearn: 1.4.2
2024-08-31 21:12:03,251:INFO:                pyod: 2.0.1
2024-08-31 21:12:03,251:INFO:            imblearn: 0.12.3
2024-08-31 21:12:03,251:INFO:   category_encoders: 2.6.3
2024-08-31 21:12:03,251:INFO:            lightgbm: 4.5.0
2024-08-31 21:12:03,252:INFO:               numba: 0.60.0
2024-08-31 21:12:03,252:INFO:            requests: 2.32.3
2024-08-31 21:12:03,252:INFO:          matplotlib: 3.7.1
2024-08-31 21:12:03,252:INFO:          scikitplot: 0.3.7
2024-08-31 21:12:03,252:INFO:         yellowbrick: 1.5
2024-08-31 21:12:03,252:INFO:              plotly: 5.16.1
2024-08-31 21:12:03,252:INFO:    plotly-resampler: Not installed
2024-08-31 21:12:03,252:INFO:             kaleido: 0.2.1
2024-08-31 21:12:03,252:INFO:           schemdraw: 0.15
2024-08-31 21:12:03,252:INFO:         statsmodels: 0.14.2
2024-08-31 21:12:03,252:INFO:              sktime: 0.26.0
2024-08-31 21:12:03,252:INFO:               tbats: 1.1.3
2024-08-31 21:12:03,252:INFO:            pmdarima: 2.0.4
2024-08-31 21:12:03,252:INFO:              psutil: 5.9.0
2024-08-31 21:12:03,252:INFO:          markupsafe: 2.1.3
2024-08-31 21:12:03,252:INFO:             pickle5: Not installed
2024-08-31 21:12:03,252:INFO:         cloudpickle: 3.0.0
2024-08-31 21:12:03,252:INFO:         deprecation: 2.1.0
2024-08-31 21:12:03,252:INFO:              xxhash: 3.5.0
2024-08-31 21:12:03,252:INFO:           wurlitzer: Not installed
2024-08-31 21:12:03,252:INFO:PyCaret optional dependencies:
2024-08-31 21:12:03,252:INFO:                shap: Not installed
2024-08-31 21:12:03,252:INFO:           interpret: Not installed
2024-08-31 21:12:03,252:INFO:                umap: Not installed
2024-08-31 21:12:03,252:INFO:     ydata_profiling: Not installed
2024-08-31 21:12:03,252:INFO:  explainerdashboard: Not installed
2024-08-31 21:12:03,252:INFO:             autoviz: Not installed
2024-08-31 21:12:03,252:INFO:           fairlearn: Not installed
2024-08-31 21:12:03,252:INFO:          deepchecks: Not installed
2024-08-31 21:12:03,252:INFO:             xgboost: 2.0.2
2024-08-31 21:12:03,252:INFO:            catboost: 1.2.5
2024-08-31 21:12:03,252:INFO:              kmodes: Not installed
2024-08-31 21:12:03,252:INFO:             mlxtend: Not installed
2024-08-31 21:12:03,252:INFO:       statsforecast: Not installed
2024-08-31 21:12:03,252:INFO:        tune_sklearn: Not installed
2024-08-31 21:12:03,252:INFO:                 ray: Not installed
2024-08-31 21:12:03,252:INFO:            hyperopt: Not installed
2024-08-31 21:12:03,253:INFO:              optuna: Not installed
2024-08-31 21:12:03,253:INFO:               skopt: Not installed
2024-08-31 21:12:03,253:INFO:              mlflow: Not installed
2024-08-31 21:12:03,253:INFO:              gradio: 4.41.0
2024-08-31 21:12:03,253:INFO:             fastapi: 0.112.1
2024-08-31 21:12:03,253:INFO:             uvicorn: 0.30.6
2024-08-31 21:12:03,253:INFO:              m2cgen: Not installed
2024-08-31 21:12:03,253:INFO:           evidently: Not installed
2024-08-31 21:12:03,253:INFO:               fugue: Not installed
2024-08-31 21:12:03,253:INFO:           streamlit: Not installed
2024-08-31 21:12:03,253:INFO:             prophet: Not installed
2024-08-31 21:12:03,253:INFO:None
2024-08-31 21:12:03,253:INFO:Set up data.
2024-08-31 21:12:03,265:INFO:Set up folding strategy.
2024-08-31 21:12:03,265:INFO:Set up train/test split.
2024-08-31 21:12:03,277:INFO:Set up index.
2024-08-31 21:12:03,278:INFO:Assigning column types.
2024-08-31 21:12:03,285:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 21:12:03,322:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,323:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,345:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,347:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,384:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,385:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,407:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,410:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,410:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 21:12:03,445:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,468:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,470:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,507:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:12:03,531:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,533:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,533:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 21:12:03,590:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,594:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,654:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:03,656:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:03,657:INFO:Preparing preprocessing pipeline...
2024-08-31 21:12:03,659:INFO:Set up simple imputation.
2024-08-31 21:12:03,659:INFO:Set up imbalanced handling.
2024-08-31 21:12:03,660:INFO:Set up column name cleaning.
2024-08-31 21:12:03,774:INFO:Finished creating preprocessing pipeline.
2024-08-31 21:12:03,779:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=123,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 21:12:03,779:INFO:Creating final display dataframe.
2024-08-31 21:12:04,009:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (35479, 16)
5   Transformed train set shape       (27668, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             smote
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              8215
2024-08-31 21:12:04,081:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:04,083:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:04,150:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:12:04,153:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:12:04,154:INFO:setup() successfully completed in 0.91s...............
2024-08-31 21:12:04,155:INFO:Initializing compare_models()
2024-08-31 21:12:04,155:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 21:12:04,155:INFO:Checking exceptions
2024-08-31 21:12:04,162:INFO:Preparing display monitor
2024-08-31 21:12:04,180:INFO:Initializing Logistic Regression
2024-08-31 21:12:04,181:INFO:Total runtime is 0.0 minutes
2024-08-31 21:12:04,183:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:04,183:INFO:Initializing create_model()
2024-08-31 21:12:04,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:04,184:INFO:Checking exceptions
2024-08-31 21:12:04,184:INFO:Importing libraries
2024-08-31 21:12:04,184:INFO:Copying training dataset
2024-08-31 21:12:04,195:INFO:Defining folds
2024-08-31 21:12:04,196:INFO:Declaring metric variables
2024-08-31 21:12:04,199:INFO:Importing untrained model
2024-08-31 21:12:04,203:INFO:Logistic Regression Imported successfully
2024-08-31 21:12:04,209:INFO:Starting cross validation
2024-08-31 21:12:04,210:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:18,598:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,665:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,703:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,725:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,759:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,814:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,918:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:18,926:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:19,024:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:19,039:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:12:19,073:INFO:Calculating mean and std
2024-08-31 21:12:19,075:INFO:Creating metrics dataframe
2024-08-31 21:12:19,079:INFO:Uploading results into container
2024-08-31 21:12:19,080:INFO:Uploading model into container now
2024-08-31 21:12:19,081:INFO:_master_model_container: 1
2024-08-31 21:12:19,081:INFO:_display_container: 2
2024-08-31 21:12:19,082:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 21:12:19,084:INFO:create_model() successfully completed......................................
2024-08-31 21:12:21,785:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:21,785:INFO:Creating metrics dataframe
2024-08-31 21:12:21,791:INFO:Initializing K Neighbors Classifier
2024-08-31 21:12:21,791:INFO:Total runtime is 0.29352078437805174 minutes
2024-08-31 21:12:21,795:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:21,795:INFO:Initializing create_model()
2024-08-31 21:12:21,795:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:21,795:INFO:Checking exceptions
2024-08-31 21:12:21,795:INFO:Importing libraries
2024-08-31 21:12:21,795:INFO:Copying training dataset
2024-08-31 21:12:21,806:INFO:Defining folds
2024-08-31 21:12:21,807:INFO:Declaring metric variables
2024-08-31 21:12:21,810:INFO:Importing untrained model
2024-08-31 21:12:21,814:INFO:K Neighbors Classifier Imported successfully
2024-08-31 21:12:21,818:INFO:Starting cross validation
2024-08-31 21:12:21,820:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:24,454:INFO:Calculating mean and std
2024-08-31 21:12:24,455:INFO:Creating metrics dataframe
2024-08-31 21:12:24,457:INFO:Uploading results into container
2024-08-31 21:12:24,458:INFO:Uploading model into container now
2024-08-31 21:12:24,458:INFO:_master_model_container: 2
2024-08-31 21:12:24,458:INFO:_display_container: 2
2024-08-31 21:12:24,458:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 21:12:24,459:INFO:create_model() successfully completed......................................
2024-08-31 21:12:27,566:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:27,567:INFO:Creating metrics dataframe
2024-08-31 21:12:27,575:INFO:Initializing Naive Bayes
2024-08-31 21:12:27,575:INFO:Total runtime is 0.38992382685343424 minutes
2024-08-31 21:12:27,577:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:27,577:INFO:Initializing create_model()
2024-08-31 21:12:27,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:27,577:INFO:Checking exceptions
2024-08-31 21:12:27,577:INFO:Importing libraries
2024-08-31 21:12:27,577:INFO:Copying training dataset
2024-08-31 21:12:27,588:INFO:Defining folds
2024-08-31 21:12:27,588:INFO:Declaring metric variables
2024-08-31 21:12:27,592:INFO:Importing untrained model
2024-08-31 21:12:27,595:INFO:Naive Bayes Imported successfully
2024-08-31 21:12:27,601:INFO:Starting cross validation
2024-08-31 21:12:27,602:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:27,893:INFO:Calculating mean and std
2024-08-31 21:12:27,895:INFO:Creating metrics dataframe
2024-08-31 21:12:27,896:INFO:Uploading results into container
2024-08-31 21:12:27,896:INFO:Uploading model into container now
2024-08-31 21:12:27,896:INFO:_master_model_container: 3
2024-08-31 21:12:27,896:INFO:_display_container: 2
2024-08-31 21:12:27,897:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 21:12:27,897:INFO:create_model() successfully completed......................................
2024-08-31 21:12:30,974:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:30,974:INFO:Creating metrics dataframe
2024-08-31 21:12:30,980:INFO:Initializing Decision Tree Classifier
2024-08-31 21:12:30,980:INFO:Total runtime is 0.44667512973149615 minutes
2024-08-31 21:12:30,983:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:30,983:INFO:Initializing create_model()
2024-08-31 21:12:30,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:30,983:INFO:Checking exceptions
2024-08-31 21:12:30,983:INFO:Importing libraries
2024-08-31 21:12:30,983:INFO:Copying training dataset
2024-08-31 21:12:30,994:INFO:Defining folds
2024-08-31 21:12:30,994:INFO:Declaring metric variables
2024-08-31 21:12:30,998:INFO:Importing untrained model
2024-08-31 21:12:31,001:INFO:Decision Tree Classifier Imported successfully
2024-08-31 21:12:31,007:INFO:Starting cross validation
2024-08-31 21:12:31,008:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:31,531:INFO:Calculating mean and std
2024-08-31 21:12:31,532:INFO:Creating metrics dataframe
2024-08-31 21:12:31,535:INFO:Uploading results into container
2024-08-31 21:12:31,535:INFO:Uploading model into container now
2024-08-31 21:12:31,535:INFO:_master_model_container: 4
2024-08-31 21:12:31,535:INFO:_display_container: 2
2024-08-31 21:12:31,536:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 21:12:31,536:INFO:create_model() successfully completed......................................
2024-08-31 21:12:34,530:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:34,530:INFO:Creating metrics dataframe
2024-08-31 21:12:34,537:INFO:Initializing SVM - Linear Kernel
2024-08-31 21:12:34,537:INFO:Total runtime is 0.5059581359227499 minutes
2024-08-31 21:12:34,540:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:34,540:INFO:Initializing create_model()
2024-08-31 21:12:34,540:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:34,540:INFO:Checking exceptions
2024-08-31 21:12:34,540:INFO:Importing libraries
2024-08-31 21:12:34,540:INFO:Copying training dataset
2024-08-31 21:12:34,552:INFO:Defining folds
2024-08-31 21:12:34,552:INFO:Declaring metric variables
2024-08-31 21:12:34,555:INFO:Importing untrained model
2024-08-31 21:12:34,559:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 21:12:34,564:INFO:Starting cross validation
2024-08-31 21:12:34,565:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:35,930:INFO:Calculating mean and std
2024-08-31 21:12:35,931:INFO:Creating metrics dataframe
2024-08-31 21:12:35,932:INFO:Uploading results into container
2024-08-31 21:12:35,933:INFO:Uploading model into container now
2024-08-31 21:12:35,933:INFO:_master_model_container: 5
2024-08-31 21:12:35,933:INFO:_display_container: 2
2024-08-31 21:12:35,934:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 21:12:35,934:INFO:create_model() successfully completed......................................
2024-08-31 21:12:38,852:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:38,852:INFO:Creating metrics dataframe
2024-08-31 21:12:38,859:INFO:Initializing Ridge Classifier
2024-08-31 21:12:38,859:INFO:Total runtime is 0.5779862443606059 minutes
2024-08-31 21:12:38,862:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:38,862:INFO:Initializing create_model()
2024-08-31 21:12:38,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:38,862:INFO:Checking exceptions
2024-08-31 21:12:38,862:INFO:Importing libraries
2024-08-31 21:12:38,862:INFO:Copying training dataset
2024-08-31 21:12:38,873:INFO:Defining folds
2024-08-31 21:12:38,873:INFO:Declaring metric variables
2024-08-31 21:12:38,877:INFO:Importing untrained model
2024-08-31 21:12:38,879:INFO:Ridge Classifier Imported successfully
2024-08-31 21:12:38,885:INFO:Starting cross validation
2024-08-31 21:12:38,887:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:39,151:INFO:Calculating mean and std
2024-08-31 21:12:39,152:INFO:Creating metrics dataframe
2024-08-31 21:12:39,153:INFO:Uploading results into container
2024-08-31 21:12:39,154:INFO:Uploading model into container now
2024-08-31 21:12:39,154:INFO:_master_model_container: 6
2024-08-31 21:12:39,154:INFO:_display_container: 2
2024-08-31 21:12:39,154:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 21:12:39,154:INFO:create_model() successfully completed......................................
2024-08-31 21:12:42,145:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:42,145:INFO:Creating metrics dataframe
2024-08-31 21:12:42,151:INFO:Initializing Random Forest Classifier
2024-08-31 21:12:42,151:INFO:Total runtime is 0.6328635215759277 minutes
2024-08-31 21:12:42,155:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:42,155:INFO:Initializing create_model()
2024-08-31 21:12:42,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:42,155:INFO:Checking exceptions
2024-08-31 21:12:42,155:INFO:Importing libraries
2024-08-31 21:12:42,155:INFO:Copying training dataset
2024-08-31 21:12:42,167:INFO:Defining folds
2024-08-31 21:12:42,167:INFO:Declaring metric variables
2024-08-31 21:12:42,170:INFO:Importing untrained model
2024-08-31 21:12:42,173:INFO:Random Forest Classifier Imported successfully
2024-08-31 21:12:42,179:INFO:Starting cross validation
2024-08-31 21:12:42,181:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:47,455:INFO:Calculating mean and std
2024-08-31 21:12:47,456:INFO:Creating metrics dataframe
2024-08-31 21:12:47,458:INFO:Uploading results into container
2024-08-31 21:12:47,459:INFO:Uploading model into container now
2024-08-31 21:12:47,459:INFO:_master_model_container: 7
2024-08-31 21:12:47,460:INFO:_display_container: 2
2024-08-31 21:12:47,460:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 21:12:47,460:INFO:create_model() successfully completed......................................
2024-08-31 21:12:50,425:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:50,425:INFO:Creating metrics dataframe
2024-08-31 21:12:50,432:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 21:12:50,432:INFO:Total runtime is 0.7708705464998881 minutes
2024-08-31 21:12:50,434:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:50,435:INFO:Initializing create_model()
2024-08-31 21:12:50,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:50,435:INFO:Checking exceptions
2024-08-31 21:12:50,435:INFO:Importing libraries
2024-08-31 21:12:50,435:INFO:Copying training dataset
2024-08-31 21:12:50,445:INFO:Defining folds
2024-08-31 21:12:50,446:INFO:Declaring metric variables
2024-08-31 21:12:50,448:INFO:Importing untrained model
2024-08-31 21:12:50,452:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 21:12:50,458:INFO:Starting cross validation
2024-08-31 21:12:50,459:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:50,753:INFO:Calculating mean and std
2024-08-31 21:12:50,755:INFO:Creating metrics dataframe
2024-08-31 21:12:50,757:INFO:Uploading results into container
2024-08-31 21:12:50,757:INFO:Uploading model into container now
2024-08-31 21:12:50,758:INFO:_master_model_container: 8
2024-08-31 21:12:50,758:INFO:_display_container: 2
2024-08-31 21:12:50,758:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 21:12:50,758:INFO:create_model() successfully completed......................................
2024-08-31 21:12:53,742:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:53,742:INFO:Creating metrics dataframe
2024-08-31 21:12:53,751:INFO:Initializing Ada Boost Classifier
2024-08-31 21:12:53,751:INFO:Total runtime is 0.8261896212895711 minutes
2024-08-31 21:12:53,754:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:53,755:INFO:Initializing create_model()
2024-08-31 21:12:53,755:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:53,755:INFO:Checking exceptions
2024-08-31 21:12:53,755:INFO:Importing libraries
2024-08-31 21:12:53,755:INFO:Copying training dataset
2024-08-31 21:12:53,766:INFO:Defining folds
2024-08-31 21:12:53,766:INFO:Declaring metric variables
2024-08-31 21:12:53,769:INFO:Importing untrained model
2024-08-31 21:12:53,772:INFO:Ada Boost Classifier Imported successfully
2024-08-31 21:12:53,779:INFO:Starting cross validation
2024-08-31 21:12:53,780:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:12:53,968:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,969:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,969:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,970:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,971:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,973:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,974:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,975:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,980:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:53,980:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:12:55,904:INFO:Calculating mean and std
2024-08-31 21:12:55,905:INFO:Creating metrics dataframe
2024-08-31 21:12:55,906:INFO:Uploading results into container
2024-08-31 21:12:55,907:INFO:Uploading model into container now
2024-08-31 21:12:55,907:INFO:_master_model_container: 9
2024-08-31 21:12:55,908:INFO:_display_container: 2
2024-08-31 21:12:55,908:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 21:12:55,908:INFO:create_model() successfully completed......................................
2024-08-31 21:12:58,852:INFO:SubProcess create_model() end ==================================
2024-08-31 21:12:58,852:INFO:Creating metrics dataframe
2024-08-31 21:12:58,860:INFO:Initializing Gradient Boosting Classifier
2024-08-31 21:12:58,860:INFO:Total runtime is 0.9113331953684488 minutes
2024-08-31 21:12:58,863:INFO:SubProcess create_model() called ==================================
2024-08-31 21:12:58,863:INFO:Initializing create_model()
2024-08-31 21:12:58,863:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:12:58,863:INFO:Checking exceptions
2024-08-31 21:12:58,863:INFO:Importing libraries
2024-08-31 21:12:58,863:INFO:Copying training dataset
2024-08-31 21:12:58,873:INFO:Defining folds
2024-08-31 21:12:58,874:INFO:Declaring metric variables
2024-08-31 21:12:58,877:INFO:Importing untrained model
2024-08-31 21:12:58,880:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 21:12:58,886:INFO:Starting cross validation
2024-08-31 21:12:58,887:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:13:06,207:INFO:Calculating mean and std
2024-08-31 21:13:06,208:INFO:Creating metrics dataframe
2024-08-31 21:13:06,209:INFO:Uploading results into container
2024-08-31 21:13:06,210:INFO:Uploading model into container now
2024-08-31 21:13:06,210:INFO:_master_model_container: 10
2024-08-31 21:13:06,210:INFO:_display_container: 2
2024-08-31 21:13:06,210:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 21:13:06,211:INFO:create_model() successfully completed......................................
2024-08-31 21:13:09,152:INFO:SubProcess create_model() end ==================================
2024-08-31 21:13:09,152:INFO:Creating metrics dataframe
2024-08-31 21:13:09,160:INFO:Initializing Linear Discriminant Analysis
2024-08-31 21:13:09,160:INFO:Total runtime is 1.0830047686894735 minutes
2024-08-31 21:13:09,165:INFO:SubProcess create_model() called ==================================
2024-08-31 21:13:09,165:INFO:Initializing create_model()
2024-08-31 21:13:09,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:13:09,165:INFO:Checking exceptions
2024-08-31 21:13:09,165:INFO:Importing libraries
2024-08-31 21:13:09,165:INFO:Copying training dataset
2024-08-31 21:13:09,177:INFO:Defining folds
2024-08-31 21:13:09,177:INFO:Declaring metric variables
2024-08-31 21:13:09,180:INFO:Importing untrained model
2024-08-31 21:13:09,183:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 21:13:09,188:INFO:Starting cross validation
2024-08-31 21:13:09,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:13:09,573:INFO:Calculating mean and std
2024-08-31 21:13:09,575:INFO:Creating metrics dataframe
2024-08-31 21:13:09,576:INFO:Uploading results into container
2024-08-31 21:13:09,576:INFO:Uploading model into container now
2024-08-31 21:13:09,577:INFO:_master_model_container: 11
2024-08-31 21:13:09,577:INFO:_display_container: 2
2024-08-31 21:13:09,577:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 21:13:09,577:INFO:create_model() successfully completed......................................
2024-08-31 21:13:12,563:INFO:SubProcess create_model() end ==================================
2024-08-31 21:13:12,563:INFO:Creating metrics dataframe
2024-08-31 21:13:12,572:INFO:Initializing Extra Trees Classifier
2024-08-31 21:13:12,572:INFO:Total runtime is 1.1398732582728068 minutes
2024-08-31 21:13:12,575:INFO:SubProcess create_model() called ==================================
2024-08-31 21:13:12,576:INFO:Initializing create_model()
2024-08-31 21:13:12,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:13:12,576:INFO:Checking exceptions
2024-08-31 21:13:12,576:INFO:Importing libraries
2024-08-31 21:13:12,576:INFO:Copying training dataset
2024-08-31 21:13:12,586:INFO:Defining folds
2024-08-31 21:13:12,586:INFO:Declaring metric variables
2024-08-31 21:13:12,590:INFO:Importing untrained model
2024-08-31 21:13:12,598:INFO:Extra Trees Classifier Imported successfully
2024-08-31 21:13:12,606:INFO:Starting cross validation
2024-08-31 21:13:12,607:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:13:16,082:INFO:Calculating mean and std
2024-08-31 21:13:16,083:INFO:Creating metrics dataframe
2024-08-31 21:13:16,085:INFO:Uploading results into container
2024-08-31 21:13:16,086:INFO:Uploading model into container now
2024-08-31 21:13:16,086:INFO:_master_model_container: 12
2024-08-31 21:13:16,086:INFO:_display_container: 2
2024-08-31 21:13:16,087:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 21:13:16,087:INFO:create_model() successfully completed......................................
2024-08-31 21:13:19,058:INFO:SubProcess create_model() end ==================================
2024-08-31 21:13:19,058:INFO:Creating metrics dataframe
2024-08-31 21:13:19,068:INFO:Initializing Extreme Gradient Boosting
2024-08-31 21:13:19,068:INFO:Total runtime is 1.248137402534485 minutes
2024-08-31 21:13:19,071:INFO:SubProcess create_model() called ==================================
2024-08-31 21:13:19,071:INFO:Initializing create_model()
2024-08-31 21:13:19,071:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:13:19,071:INFO:Checking exceptions
2024-08-31 21:13:19,071:INFO:Importing libraries
2024-08-31 21:13:19,071:INFO:Copying training dataset
2024-08-31 21:13:19,082:INFO:Defining folds
2024-08-31 21:13:19,082:INFO:Declaring metric variables
2024-08-31 21:13:19,085:INFO:Importing untrained model
2024-08-31 21:13:19,089:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 21:13:19,095:INFO:Starting cross validation
2024-08-31 21:13:19,096:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:13:20,242:INFO:Calculating mean and std
2024-08-31 21:13:20,243:INFO:Creating metrics dataframe
2024-08-31 21:13:20,245:INFO:Uploading results into container
2024-08-31 21:13:20,245:INFO:Uploading model into container now
2024-08-31 21:13:20,245:INFO:_master_model_container: 13
2024-08-31 21:13:20,245:INFO:_display_container: 2
2024-08-31 21:13:20,246:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 21:13:20,246:INFO:create_model() successfully completed......................................
2024-08-31 21:13:23,211:INFO:SubProcess create_model() end ==================================
2024-08-31 21:13:23,211:INFO:Creating metrics dataframe
2024-08-31 21:13:23,218:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 21:13:23,219:INFO:Total runtime is 1.3173235972722372 minutes
2024-08-31 21:13:23,221:INFO:SubProcess create_model() called ==================================
2024-08-31 21:13:23,222:INFO:Initializing create_model()
2024-08-31 21:13:23,222:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:13:23,222:INFO:Checking exceptions
2024-08-31 21:13:23,222:INFO:Importing libraries
2024-08-31 21:13:23,222:INFO:Copying training dataset
2024-08-31 21:13:23,233:INFO:Defining folds
2024-08-31 21:13:23,234:INFO:Declaring metric variables
2024-08-31 21:13:23,236:INFO:Importing untrained model
2024-08-31 21:13:23,240:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:13:23,245:INFO:Starting cross validation
2024-08-31 21:13:23,246:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:13:25,088:INFO:Calculating mean and std
2024-08-31 21:13:25,090:INFO:Creating metrics dataframe
2024-08-31 21:13:25,093:INFO:Uploading results into container
2024-08-31 21:13:25,094:INFO:Uploading model into container now
2024-08-31 21:13:25,095:INFO:_master_model_container: 14
2024-08-31 21:13:25,095:INFO:_display_container: 2
2024-08-31 21:13:25,095:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:13:25,095:INFO:create_model() successfully completed......................................
2024-08-31 21:13:28,051:INFO:SubProcess create_model() end ==================================
2024-08-31 21:13:28,051:INFO:Creating metrics dataframe
2024-08-31 21:13:28,059:INFO:Initializing CatBoost Classifier
2024-08-31 21:13:28,060:INFO:Total runtime is 1.3980063120524089 minutes
2024-08-31 21:13:28,062:INFO:SubProcess create_model() called ==================================
2024-08-31 21:13:28,062:INFO:Initializing create_model()
2024-08-31 21:13:28,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:13:28,062:INFO:Checking exceptions
2024-08-31 21:13:28,062:INFO:Importing libraries
2024-08-31 21:13:28,063:INFO:Copying training dataset
2024-08-31 21:13:28,074:INFO:Defining folds
2024-08-31 21:13:28,074:INFO:Declaring metric variables
2024-08-31 21:13:28,078:INFO:Importing untrained model
2024-08-31 21:13:28,081:INFO:CatBoost Classifier Imported successfully
2024-08-31 21:13:28,085:INFO:Starting cross validation
2024-08-31 21:13:28,086:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:14:23,731:INFO:Calculating mean and std
2024-08-31 21:14:23,732:INFO:Creating metrics dataframe
2024-08-31 21:14:23,734:INFO:Uploading results into container
2024-08-31 21:14:23,735:INFO:Uploading model into container now
2024-08-31 21:14:23,735:INFO:_master_model_container: 15
2024-08-31 21:14:23,735:INFO:_display_container: 2
2024-08-31 21:14:23,735:INFO:<catboost.core.CatBoostClassifier object at 0x00000266DB255190>
2024-08-31 21:14:23,736:INFO:create_model() successfully completed......................................
2024-08-31 21:14:27,104:INFO:SubProcess create_model() end ==================================
2024-08-31 21:14:27,104:INFO:Creating metrics dataframe
2024-08-31 21:14:27,113:INFO:Initializing Dummy Classifier
2024-08-31 21:14:27,114:INFO:Total runtime is 2.3822435895601908 minutes
2024-08-31 21:14:27,116:INFO:SubProcess create_model() called ==================================
2024-08-31 21:14:27,117:INFO:Initializing create_model()
2024-08-31 21:14:27,117:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266AB58B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:14:27,117:INFO:Checking exceptions
2024-08-31 21:14:27,117:INFO:Importing libraries
2024-08-31 21:14:27,117:INFO:Copying training dataset
2024-08-31 21:14:27,131:INFO:Defining folds
2024-08-31 21:14:27,131:INFO:Declaring metric variables
2024-08-31 21:14:27,134:INFO:Importing untrained model
2024-08-31 21:14:27,137:INFO:Dummy Classifier Imported successfully
2024-08-31 21:14:27,143:INFO:Starting cross validation
2024-08-31 21:14:27,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:14:27,378:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,387:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,387:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,388:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,389:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,389:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,390:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,391:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,391:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,395:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:14:27,407:INFO:Calculating mean and std
2024-08-31 21:14:27,408:INFO:Creating metrics dataframe
2024-08-31 21:14:27,410:INFO:Uploading results into container
2024-08-31 21:14:27,412:INFO:Uploading model into container now
2024-08-31 21:14:27,412:INFO:_master_model_container: 16
2024-08-31 21:14:27,412:INFO:_display_container: 2
2024-08-31 21:14:27,413:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 21:14:27,413:INFO:create_model() successfully completed......................................
2024-08-31 21:14:30,862:INFO:SubProcess create_model() end ==================================
2024-08-31 21:14:30,862:INFO:Creating metrics dataframe
2024-08-31 21:14:30,883:INFO:Initializing create_model()
2024-08-31 21:14:30,883:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:14:30,883:INFO:Checking exceptions
2024-08-31 21:14:30,885:INFO:Importing libraries
2024-08-31 21:14:30,885:INFO:Copying training dataset
2024-08-31 21:14:30,897:INFO:Defining folds
2024-08-31 21:14:30,897:INFO:Declaring metric variables
2024-08-31 21:14:30,897:INFO:Importing untrained model
2024-08-31 21:14:30,897:INFO:Declaring custom model
2024-08-31 21:14:30,898:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:14:30,899:INFO:Cross validation set to False
2024-08-31 21:14:30,899:INFO:Fitting Model
2024-08-31 21:14:31,021:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:14:31,021:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:14:31,025:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000932 seconds.
2024-08-31 21:14:31,025:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:14:31,025:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:14:31,025:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:14:31,025:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:14:31,025:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:14:31,233:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:14:31,233:INFO:create_model() successfully completed......................................
2024-08-31 21:14:34,695:INFO:_master_model_container: 16
2024-08-31 21:14:34,695:INFO:_display_container: 2
2024-08-31 21:14:34,695:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:14:34,695:INFO:compare_models() successfully completed......................................
2024-08-31 21:14:34,699:INFO:Initializing tune_model()
2024-08-31 21:14:34,699:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 21:14:34,699:INFO:Checking exceptions
2024-08-31 21:14:34,716:INFO:Copying training dataset
2024-08-31 21:14:34,727:INFO:Checking base model
2024-08-31 21:14:34,728:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 21:14:34,732:INFO:Declaring metric variables
2024-08-31 21:14:34,736:INFO:Defining Hyperparameters
2024-08-31 21:14:38,233:INFO:Tuning with n_jobs=-1
2024-08-31 21:14:38,233:INFO:Initializing RandomizedSearchCV
2024-08-31 21:15:25,618:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 21:15:25,620:INFO:Hyperparameter search completed
2024-08-31 21:15:25,620:INFO:SubProcess create_model() called ==================================
2024-08-31 21:15:25,622:INFO:Initializing create_model()
2024-08-31 21:15:25,622:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAA47E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 21:15:25,622:INFO:Checking exceptions
2024-08-31 21:15:25,622:INFO:Importing libraries
2024-08-31 21:15:25,623:INFO:Copying training dataset
2024-08-31 21:15:25,650:INFO:Defining folds
2024-08-31 21:15:25,650:INFO:Declaring metric variables
2024-08-31 21:15:25,655:INFO:Importing untrained model
2024-08-31 21:15:25,656:INFO:Declaring custom model
2024-08-31 21:15:25,663:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:15:25,675:INFO:Starting cross validation
2024-08-31 21:15:25,678:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:15:29,218:INFO:Calculating mean and std
2024-08-31 21:15:29,220:INFO:Creating metrics dataframe
2024-08-31 21:15:29,228:INFO:Finalizing model
2024-08-31 21:15:29,378:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 21:15:29,379:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 21:15:29,379:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 21:15:29,398:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:15:29,399:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 21:15:29,399:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 21:15:29,399:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 21:15:29,400:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:15:29,403:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.
2024-08-31 21:15:29,403:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:15:29,403:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:15:29,403:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:15:29,404:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:15:29,405:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:15:29,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,813:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,869:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,968:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:29,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:29,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:30,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:30,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:30,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:30,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:15:30,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:15:30,083:INFO:Uploading results into container
2024-08-31 21:15:30,084:INFO:Uploading model into container now
2024-08-31 21:15:30,085:INFO:_master_model_container: 17
2024-08-31 21:15:30,085:INFO:_display_container: 3
2024-08-31 21:15:30,086:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:15:30,086:INFO:create_model() successfully completed......................................
2024-08-31 21:15:33,267:INFO:SubProcess create_model() end ==================================
2024-08-31 21:15:33,267:INFO:choose_better activated
2024-08-31 21:15:33,269:INFO:SubProcess create_model() called ==================================
2024-08-31 21:15:33,271:INFO:Initializing create_model()
2024-08-31 21:15:33,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:15:33,271:INFO:Checking exceptions
2024-08-31 21:15:33,273:INFO:Importing libraries
2024-08-31 21:15:33,273:INFO:Copying training dataset
2024-08-31 21:15:33,286:INFO:Defining folds
2024-08-31 21:15:33,286:INFO:Declaring metric variables
2024-08-31 21:15:33,286:INFO:Importing untrained model
2024-08-31 21:15:33,286:INFO:Declaring custom model
2024-08-31 21:15:33,287:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:15:33,287:INFO:Starting cross validation
2024-08-31 21:15:33,288:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:15:35,334:INFO:Calculating mean and std
2024-08-31 21:15:35,335:INFO:Creating metrics dataframe
2024-08-31 21:15:35,337:INFO:Finalizing model
2024-08-31 21:15:35,461:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:15:35,462:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:15:35,465:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.
2024-08-31 21:15:35,465:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-08-31 21:15:35,465:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:15:35,465:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:15:35,465:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:15:35,665:INFO:Uploading results into container
2024-08-31 21:15:35,665:INFO:Uploading model into container now
2024-08-31 21:15:35,665:INFO:_master_model_container: 18
2024-08-31 21:15:35,665:INFO:_display_container: 4
2024-08-31 21:15:35,666:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:15:35,666:INFO:create_model() successfully completed......................................
2024-08-31 21:15:38,636:INFO:SubProcess create_model() end ==================================
2024-08-31 21:15:38,636:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.7056
2024-08-31 21:15:38,637:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 21:15:38,637:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 21:15:38,637:INFO:choose_better completed
2024-08-31 21:15:38,638:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 21:15:38,646:INFO:_master_model_container: 18
2024-08-31 21:15:38,646:INFO:_display_container: 3
2024-08-31 21:15:38,646:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:15:38,646:INFO:tune_model() successfully completed......................................
2024-08-31 21:15:41,570:INFO:Initializing finalize_model()
2024-08-31 21:15:41,570:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 21:15:41,570:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:15:41,576:INFO:Initializing create_model()
2024-08-31 21:15:41,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266AB66FB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:15:41,577:INFO:Checking exceptions
2024-08-31 21:15:41,578:INFO:Importing libraries
2024-08-31 21:15:41,578:INFO:Copying training dataset
2024-08-31 21:15:41,578:INFO:Defining folds
2024-08-31 21:15:41,578:INFO:Declaring metric variables
2024-08-31 21:15:41,578:INFO:Importing untrained model
2024-08-31 21:15:41,578:INFO:Declaring custom model
2024-08-31 21:15:41,579:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:15:41,580:INFO:Cross validation set to False
2024-08-31 21:15:41,580:INFO:Fitting Model
2024-08-31 21:15:41,705:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:15:41,706:INFO:[LightGBM] [Info] Number of positive: 19763, number of negative: 19763
2024-08-31 21:15:41,710:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001112 seconds.
2024-08-31 21:15:41,710:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:15:41,710:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:15:41,710:INFO:[LightGBM] [Info] Total Bins 3807
2024-08-31 21:15:41,710:INFO:[LightGBM] [Info] Number of data points in the train set: 39526, number of used features: 15
2024-08-31 21:15:41,710:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:15:41,890:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 21:15:41,891:INFO:create_model() successfully completed......................................
2024-08-31 21:15:44,809:INFO:_master_model_container: 18
2024-08-31 21:15:44,810:INFO:_display_container: 3
2024-08-31 21:15:44,816:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 21:15:44,816:INFO:finalize_model() successfully completed......................................
2024-08-31 21:16:09,768:INFO:PyCaret ClassificationExperiment
2024-08-31 21:16:09,768:INFO:Logging name: clf-default-name
2024-08-31 21:16:09,768:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 21:16:09,768:INFO:version 3.3.2
2024-08-31 21:16:09,768:INFO:Initializing setup()
2024-08-31 21:16:09,769:INFO:self.USI: 6ca1
2024-08-31 21:16:09,769:INFO:self._variable_keys: {'USI', 'y', 'X_train', 'y_train', 'X', 'seed', 'y_test', 'memory', 'gpu_param', 'is_multiclass', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'html_param', 'X_test', 'data', 'exp_id', 'fold_generator', 'exp_name_log', 'logging_param', 'fold_groups_param', '_available_plots', 'target_param', 'pipeline', 'idx', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase'}
2024-08-31 21:16:09,769:INFO:Checking environment
2024-08-31 21:16:09,769:INFO:python_version: 3.11.9
2024-08-31 21:16:09,769:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-08-31 21:16:09,769:INFO:machine: AMD64
2024-08-31 21:16:09,770:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 21:16:09,775:INFO:Memory: svmem(total=16867028992, available=2303172608, percent=86.3, used=14563856384, free=2303172608)
2024-08-31 21:16:09,775:INFO:Physical Core: 6
2024-08-31 21:16:09,775:INFO:Logical Core: 12
2024-08-31 21:16:09,775:INFO:Checking libraries
2024-08-31 21:16:09,775:INFO:System:
2024-08-31 21:16:09,776:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-08-31 21:16:09,776:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-08-31 21:16:09,776:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 21:16:09,776:INFO:PyCaret required dependencies:
2024-08-31 21:16:09,776:INFO:                 pip: 23.2.1
2024-08-31 21:16:09,776:INFO:          setuptools: 67.8.0
2024-08-31 21:16:09,776:INFO:             pycaret: 3.3.2
2024-08-31 21:16:09,776:INFO:             IPython: 8.14.0
2024-08-31 21:16:09,776:INFO:          ipywidgets: 8.1.5
2024-08-31 21:16:09,776:INFO:                tqdm: 4.66.5
2024-08-31 21:16:09,776:INFO:               numpy: 1.24.3
2024-08-31 21:16:09,776:INFO:              pandas: 2.0.3
2024-08-31 21:16:09,776:INFO:              jinja2: 3.1.4
2024-08-31 21:16:09,776:INFO:               scipy: 1.10.1
2024-08-31 21:16:09,776:INFO:              joblib: 1.2.0
2024-08-31 21:16:09,776:INFO:             sklearn: 1.4.2
2024-08-31 21:16:09,776:INFO:                pyod: 2.0.1
2024-08-31 21:16:09,776:INFO:            imblearn: 0.12.3
2024-08-31 21:16:09,776:INFO:   category_encoders: 2.6.3
2024-08-31 21:16:09,776:INFO:            lightgbm: 4.5.0
2024-08-31 21:16:09,776:INFO:               numba: 0.60.0
2024-08-31 21:16:09,777:INFO:            requests: 2.32.3
2024-08-31 21:16:09,777:INFO:          matplotlib: 3.7.1
2024-08-31 21:16:09,777:INFO:          scikitplot: 0.3.7
2024-08-31 21:16:09,777:INFO:         yellowbrick: 1.5
2024-08-31 21:16:09,777:INFO:              plotly: 5.16.1
2024-08-31 21:16:09,777:INFO:    plotly-resampler: Not installed
2024-08-31 21:16:09,777:INFO:             kaleido: 0.2.1
2024-08-31 21:16:09,777:INFO:           schemdraw: 0.15
2024-08-31 21:16:09,777:INFO:         statsmodels: 0.14.2
2024-08-31 21:16:09,777:INFO:              sktime: 0.26.0
2024-08-31 21:16:09,777:INFO:               tbats: 1.1.3
2024-08-31 21:16:09,777:INFO:            pmdarima: 2.0.4
2024-08-31 21:16:09,777:INFO:              psutil: 5.9.0
2024-08-31 21:16:09,777:INFO:          markupsafe: 2.1.3
2024-08-31 21:16:09,777:INFO:             pickle5: Not installed
2024-08-31 21:16:09,777:INFO:         cloudpickle: 3.0.0
2024-08-31 21:16:09,777:INFO:         deprecation: 2.1.0
2024-08-31 21:16:09,778:INFO:              xxhash: 3.5.0
2024-08-31 21:16:09,778:INFO:           wurlitzer: Not installed
2024-08-31 21:16:09,778:INFO:PyCaret optional dependencies:
2024-08-31 21:16:09,778:INFO:                shap: Not installed
2024-08-31 21:16:09,778:INFO:           interpret: Not installed
2024-08-31 21:16:09,778:INFO:                umap: Not installed
2024-08-31 21:16:09,778:INFO:     ydata_profiling: Not installed
2024-08-31 21:16:09,778:INFO:  explainerdashboard: Not installed
2024-08-31 21:16:09,778:INFO:             autoviz: Not installed
2024-08-31 21:16:09,778:INFO:           fairlearn: Not installed
2024-08-31 21:16:09,778:INFO:          deepchecks: Not installed
2024-08-31 21:16:09,778:INFO:             xgboost: 2.0.2
2024-08-31 21:16:09,778:INFO:            catboost: 1.2.5
2024-08-31 21:16:09,778:INFO:              kmodes: Not installed
2024-08-31 21:16:09,778:INFO:             mlxtend: Not installed
2024-08-31 21:16:09,779:INFO:       statsforecast: Not installed
2024-08-31 21:16:09,779:INFO:        tune_sklearn: Not installed
2024-08-31 21:16:09,779:INFO:                 ray: Not installed
2024-08-31 21:16:09,779:INFO:            hyperopt: Not installed
2024-08-31 21:16:09,779:INFO:              optuna: Not installed
2024-08-31 21:16:09,779:INFO:               skopt: Not installed
2024-08-31 21:16:09,779:INFO:              mlflow: Not installed
2024-08-31 21:16:09,779:INFO:              gradio: 4.41.0
2024-08-31 21:16:09,779:INFO:             fastapi: 0.112.1
2024-08-31 21:16:09,780:INFO:             uvicorn: 0.30.6
2024-08-31 21:16:09,780:INFO:              m2cgen: Not installed
2024-08-31 21:16:09,780:INFO:           evidently: Not installed
2024-08-31 21:16:09,780:INFO:               fugue: Not installed
2024-08-31 21:16:09,780:INFO:           streamlit: Not installed
2024-08-31 21:16:09,780:INFO:             prophet: Not installed
2024-08-31 21:16:09,780:INFO:None
2024-08-31 21:16:09,780:INFO:Set up data.
2024-08-31 21:16:09,795:INFO:Set up folding strategy.
2024-08-31 21:16:09,795:INFO:Set up train/test split.
2024-08-31 21:16:09,817:INFO:Set up index.
2024-08-31 21:16:09,818:INFO:Assigning column types.
2024-08-31 21:16:09,827:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 21:16:09,867:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 21:16:09,867:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:16:09,890:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:09,892:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:09,928:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 21:16:09,930:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:16:09,953:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:09,955:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:09,955:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 21:16:09,992:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:16:10,015:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,017:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,054:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 21:16:10,077:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,079:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,079:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 21:16:10,137:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,140:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,199:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,201:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,202:INFO:Preparing preprocessing pipeline...
2024-08-31 21:16:10,203:INFO:Set up simple imputation.
2024-08-31 21:16:10,203:INFO:Set up imbalanced handling.
2024-08-31 21:16:10,206:INFO:Set up column name cleaning.
2024-08-31 21:16:10,296:INFO:Finished creating preprocessing pipeline.
2024-08-31 21:16:10,302:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=123,
                                                                              sampling_strategy='auto')))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 21:16:10,303:INFO:Creating final display dataframe.
2024-08-31 21:16:10,530:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (35479, 16)
5   Transformed train set shape       (27668, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             smote
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              6ca1
2024-08-31 21:16:10,596:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,598:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,658:INFO:Soft dependency imported: xgboost: 2.0.2
2024-08-31 21:16:10,661:INFO:Soft dependency imported: catboost: 1.2.5
2024-08-31 21:16:10,662:INFO:setup() successfully completed in 0.9s...............
2024-08-31 21:16:10,664:INFO:Initializing compare_models()
2024-08-31 21:16:10,664:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 21:16:10,664:INFO:Checking exceptions
2024-08-31 21:16:10,670:INFO:Preparing display monitor
2024-08-31 21:16:10,691:INFO:Initializing Logistic Regression
2024-08-31 21:16:10,691:INFO:Total runtime is 0.0 minutes
2024-08-31 21:16:10,695:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:10,695:INFO:Initializing create_model()
2024-08-31 21:16:10,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:10,695:INFO:Checking exceptions
2024-08-31 21:16:10,695:INFO:Importing libraries
2024-08-31 21:16:10,695:INFO:Copying training dataset
2024-08-31 21:16:10,709:INFO:Defining folds
2024-08-31 21:16:10,710:INFO:Declaring metric variables
2024-08-31 21:16:10,713:INFO:Importing untrained model
2024-08-31 21:16:10,716:INFO:Logistic Regression Imported successfully
2024-08-31 21:16:10,725:INFO:Starting cross validation
2024-08-31 21:16:10,726:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:16,204:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,231:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,275:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,289:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,355:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,363:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,375:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,387:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,400:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,444:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 21:16:16,461:INFO:Calculating mean and std
2024-08-31 21:16:16,462:INFO:Creating metrics dataframe
2024-08-31 21:16:16,465:INFO:Uploading results into container
2024-08-31 21:16:16,465:INFO:Uploading model into container now
2024-08-31 21:16:16,466:INFO:_master_model_container: 1
2024-08-31 21:16:16,466:INFO:_display_container: 2
2024-08-31 21:16:16,466:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 21:16:16,466:INFO:create_model() successfully completed......................................
2024-08-31 21:16:19,549:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:19,550:INFO:Creating metrics dataframe
2024-08-31 21:16:19,556:INFO:Initializing K Neighbors Classifier
2024-08-31 21:16:19,556:INFO:Total runtime is 0.14775057236353556 minutes
2024-08-31 21:16:19,558:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:19,559:INFO:Initializing create_model()
2024-08-31 21:16:19,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:19,559:INFO:Checking exceptions
2024-08-31 21:16:19,559:INFO:Importing libraries
2024-08-31 21:16:19,559:INFO:Copying training dataset
2024-08-31 21:16:19,570:INFO:Defining folds
2024-08-31 21:16:19,571:INFO:Declaring metric variables
2024-08-31 21:16:19,573:INFO:Importing untrained model
2024-08-31 21:16:19,576:INFO:K Neighbors Classifier Imported successfully
2024-08-31 21:16:19,582:INFO:Starting cross validation
2024-08-31 21:16:19,584:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:20,211:INFO:Calculating mean and std
2024-08-31 21:16:20,213:INFO:Creating metrics dataframe
2024-08-31 21:16:20,215:INFO:Uploading results into container
2024-08-31 21:16:20,216:INFO:Uploading model into container now
2024-08-31 21:16:20,217:INFO:_master_model_container: 2
2024-08-31 21:16:20,217:INFO:_display_container: 2
2024-08-31 21:16:20,218:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 21:16:20,218:INFO:create_model() successfully completed......................................
2024-08-31 21:16:23,347:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:23,347:INFO:Creating metrics dataframe
2024-08-31 21:16:23,354:INFO:Initializing Naive Bayes
2024-08-31 21:16:23,355:INFO:Total runtime is 0.21105910539627076 minutes
2024-08-31 21:16:23,357:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:23,358:INFO:Initializing create_model()
2024-08-31 21:16:23,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:23,358:INFO:Checking exceptions
2024-08-31 21:16:23,358:INFO:Importing libraries
2024-08-31 21:16:23,358:INFO:Copying training dataset
2024-08-31 21:16:23,368:INFO:Defining folds
2024-08-31 21:16:23,369:INFO:Declaring metric variables
2024-08-31 21:16:23,372:INFO:Importing untrained model
2024-08-31 21:16:23,376:INFO:Naive Bayes Imported successfully
2024-08-31 21:16:23,381:INFO:Starting cross validation
2024-08-31 21:16:23,382:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:23,630:INFO:Calculating mean and std
2024-08-31 21:16:23,631:INFO:Creating metrics dataframe
2024-08-31 21:16:23,633:INFO:Uploading results into container
2024-08-31 21:16:23,633:INFO:Uploading model into container now
2024-08-31 21:16:23,634:INFO:_master_model_container: 3
2024-08-31 21:16:23,634:INFO:_display_container: 2
2024-08-31 21:16:23,634:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 21:16:23,634:INFO:create_model() successfully completed......................................
2024-08-31 21:16:26,732:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:26,732:INFO:Creating metrics dataframe
2024-08-31 21:16:26,738:INFO:Initializing Decision Tree Classifier
2024-08-31 21:16:26,739:INFO:Total runtime is 0.2674679636955261 minutes
2024-08-31 21:16:26,742:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:26,742:INFO:Initializing create_model()
2024-08-31 21:16:26,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:26,742:INFO:Checking exceptions
2024-08-31 21:16:26,742:INFO:Importing libraries
2024-08-31 21:16:26,742:INFO:Copying training dataset
2024-08-31 21:16:26,758:INFO:Defining folds
2024-08-31 21:16:26,758:INFO:Declaring metric variables
2024-08-31 21:16:26,762:INFO:Importing untrained model
2024-08-31 21:16:26,765:INFO:Decision Tree Classifier Imported successfully
2024-08-31 21:16:26,771:INFO:Starting cross validation
2024-08-31 21:16:26,772:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:27,399:INFO:Calculating mean and std
2024-08-31 21:16:27,400:INFO:Creating metrics dataframe
2024-08-31 21:16:27,405:INFO:Uploading results into container
2024-08-31 21:16:27,407:INFO:Uploading model into container now
2024-08-31 21:16:27,408:INFO:_master_model_container: 4
2024-08-31 21:16:27,408:INFO:_display_container: 2
2024-08-31 21:16:27,409:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-08-31 21:16:27,409:INFO:create_model() successfully completed......................................
2024-08-31 21:16:30,486:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:30,487:INFO:Creating metrics dataframe
2024-08-31 21:16:30,493:INFO:Initializing SVM - Linear Kernel
2024-08-31 21:16:30,493:INFO:Total runtime is 0.33003135919570925 minutes
2024-08-31 21:16:30,496:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:30,496:INFO:Initializing create_model()
2024-08-31 21:16:30,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:30,496:INFO:Checking exceptions
2024-08-31 21:16:30,496:INFO:Importing libraries
2024-08-31 21:16:30,496:INFO:Copying training dataset
2024-08-31 21:16:30,507:INFO:Defining folds
2024-08-31 21:16:30,507:INFO:Declaring metric variables
2024-08-31 21:16:30,510:INFO:Importing untrained model
2024-08-31 21:16:30,513:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 21:16:30,519:INFO:Starting cross validation
2024-08-31 21:16:30,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:32,005:INFO:Calculating mean and std
2024-08-31 21:16:32,005:INFO:Creating metrics dataframe
2024-08-31 21:16:32,007:INFO:Uploading results into container
2024-08-31 21:16:32,008:INFO:Uploading model into container now
2024-08-31 21:16:32,008:INFO:_master_model_container: 5
2024-08-31 21:16:32,009:INFO:_display_container: 2
2024-08-31 21:16:32,009:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 21:16:32,009:INFO:create_model() successfully completed......................................
2024-08-31 21:16:35,046:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:35,046:INFO:Creating metrics dataframe
2024-08-31 21:16:35,053:INFO:Initializing Ridge Classifier
2024-08-31 21:16:35,053:INFO:Total runtime is 0.4060305953025818 minutes
2024-08-31 21:16:35,056:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:35,056:INFO:Initializing create_model()
2024-08-31 21:16:35,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:35,056:INFO:Checking exceptions
2024-08-31 21:16:35,056:INFO:Importing libraries
2024-08-31 21:16:35,056:INFO:Copying training dataset
2024-08-31 21:16:35,067:INFO:Defining folds
2024-08-31 21:16:35,068:INFO:Declaring metric variables
2024-08-31 21:16:35,072:INFO:Importing untrained model
2024-08-31 21:16:35,074:INFO:Ridge Classifier Imported successfully
2024-08-31 21:16:35,080:INFO:Starting cross validation
2024-08-31 21:16:35,081:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:35,316:INFO:Calculating mean and std
2024-08-31 21:16:35,317:INFO:Creating metrics dataframe
2024-08-31 21:16:35,319:INFO:Uploading results into container
2024-08-31 21:16:35,319:INFO:Uploading model into container now
2024-08-31 21:16:35,319:INFO:_master_model_container: 6
2024-08-31 21:16:35,320:INFO:_display_container: 2
2024-08-31 21:16:35,320:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-08-31 21:16:35,320:INFO:create_model() successfully completed......................................
2024-08-31 21:16:38,392:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:38,392:INFO:Creating metrics dataframe
2024-08-31 21:16:38,399:INFO:Initializing Random Forest Classifier
2024-08-31 21:16:38,399:INFO:Total runtime is 0.46180291573206583 minutes
2024-08-31 21:16:38,402:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:38,402:INFO:Initializing create_model()
2024-08-31 21:16:38,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:38,402:INFO:Checking exceptions
2024-08-31 21:16:38,402:INFO:Importing libraries
2024-08-31 21:16:38,402:INFO:Copying training dataset
2024-08-31 21:16:38,415:INFO:Defining folds
2024-08-31 21:16:38,415:INFO:Declaring metric variables
2024-08-31 21:16:38,418:INFO:Importing untrained model
2024-08-31 21:16:38,420:INFO:Random Forest Classifier Imported successfully
2024-08-31 21:16:38,425:INFO:Starting cross validation
2024-08-31 21:16:38,426:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:43,711:INFO:Calculating mean and std
2024-08-31 21:16:43,712:INFO:Creating metrics dataframe
2024-08-31 21:16:43,713:INFO:Uploading results into container
2024-08-31 21:16:43,714:INFO:Uploading model into container now
2024-08-31 21:16:43,714:INFO:_master_model_container: 7
2024-08-31 21:16:43,714:INFO:_display_container: 2
2024-08-31 21:16:43,715:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-08-31 21:16:43,715:INFO:create_model() successfully completed......................................
2024-08-31 21:16:46,855:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:46,855:INFO:Creating metrics dataframe
2024-08-31 21:16:46,864:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 21:16:46,864:INFO:Total runtime is 0.6028765837351481 minutes
2024-08-31 21:16:46,869:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:46,869:INFO:Initializing create_model()
2024-08-31 21:16:46,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:46,870:INFO:Checking exceptions
2024-08-31 21:16:46,870:INFO:Importing libraries
2024-08-31 21:16:46,870:INFO:Copying training dataset
2024-08-31 21:16:46,888:INFO:Defining folds
2024-08-31 21:16:46,888:INFO:Declaring metric variables
2024-08-31 21:16:46,894:INFO:Importing untrained model
2024-08-31 21:16:46,898:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 21:16:46,905:INFO:Starting cross validation
2024-08-31 21:16:46,907:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:47,164:INFO:Calculating mean and std
2024-08-31 21:16:47,165:INFO:Creating metrics dataframe
2024-08-31 21:16:47,166:INFO:Uploading results into container
2024-08-31 21:16:47,167:INFO:Uploading model into container now
2024-08-31 21:16:47,168:INFO:_master_model_container: 8
2024-08-31 21:16:47,168:INFO:_display_container: 2
2024-08-31 21:16:47,168:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 21:16:47,168:INFO:create_model() successfully completed......................................
2024-08-31 21:16:50,068:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:50,069:INFO:Creating metrics dataframe
2024-08-31 21:16:50,075:INFO:Initializing Ada Boost Classifier
2024-08-31 21:16:50,075:INFO:Total runtime is 0.6563972274462382 minutes
2024-08-31 21:16:50,078:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:50,078:INFO:Initializing create_model()
2024-08-31 21:16:50,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:50,078:INFO:Checking exceptions
2024-08-31 21:16:50,078:INFO:Importing libraries
2024-08-31 21:16:50,078:INFO:Copying training dataset
2024-08-31 21:16:50,090:INFO:Defining folds
2024-08-31 21:16:50,090:INFO:Declaring metric variables
2024-08-31 21:16:50,093:INFO:Importing untrained model
2024-08-31 21:16:50,096:INFO:Ada Boost Classifier Imported successfully
2024-08-31 21:16:50,101:INFO:Starting cross validation
2024-08-31 21:16:50,102:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:16:50,249:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,266:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,267:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,268:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,273:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,275:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,285:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,288:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,290:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:50,300:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 21:16:52,135:INFO:Calculating mean and std
2024-08-31 21:16:52,135:INFO:Creating metrics dataframe
2024-08-31 21:16:52,136:INFO:Uploading results into container
2024-08-31 21:16:52,137:INFO:Uploading model into container now
2024-08-31 21:16:52,137:INFO:_master_model_container: 9
2024-08-31 21:16:52,137:INFO:_display_container: 2
2024-08-31 21:16:52,137:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-08-31 21:16:52,138:INFO:create_model() successfully completed......................................
2024-08-31 21:16:55,015:INFO:SubProcess create_model() end ==================================
2024-08-31 21:16:55,015:INFO:Creating metrics dataframe
2024-08-31 21:16:55,023:INFO:Initializing Gradient Boosting Classifier
2024-08-31 21:16:55,023:INFO:Total runtime is 0.7388611793518066 minutes
2024-08-31 21:16:55,026:INFO:SubProcess create_model() called ==================================
2024-08-31 21:16:55,027:INFO:Initializing create_model()
2024-08-31 21:16:55,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:16:55,027:INFO:Checking exceptions
2024-08-31 21:16:55,027:INFO:Importing libraries
2024-08-31 21:16:55,027:INFO:Copying training dataset
2024-08-31 21:16:55,037:INFO:Defining folds
2024-08-31 21:16:55,039:INFO:Declaring metric variables
2024-08-31 21:16:55,042:INFO:Importing untrained model
2024-08-31 21:16:55,045:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 21:16:55,050:INFO:Starting cross validation
2024-08-31 21:16:55,051:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:17:02,253:INFO:Calculating mean and std
2024-08-31 21:17:02,255:INFO:Creating metrics dataframe
2024-08-31 21:17:02,257:INFO:Uploading results into container
2024-08-31 21:17:02,257:INFO:Uploading model into container now
2024-08-31 21:17:02,258:INFO:_master_model_container: 10
2024-08-31 21:17:02,258:INFO:_display_container: 2
2024-08-31 21:17:02,259:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 21:17:02,259:INFO:create_model() successfully completed......................................
2024-08-31 21:17:05,143:INFO:SubProcess create_model() end ==================================
2024-08-31 21:17:05,143:INFO:Creating metrics dataframe
2024-08-31 21:17:05,155:INFO:Initializing Linear Discriminant Analysis
2024-08-31 21:17:05,155:INFO:Total runtime is 0.9077244997024536 minutes
2024-08-31 21:17:05,158:INFO:SubProcess create_model() called ==================================
2024-08-31 21:17:05,158:INFO:Initializing create_model()
2024-08-31 21:17:05,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:17:05,158:INFO:Checking exceptions
2024-08-31 21:17:05,159:INFO:Importing libraries
2024-08-31 21:17:05,159:INFO:Copying training dataset
2024-08-31 21:17:05,170:INFO:Defining folds
2024-08-31 21:17:05,170:INFO:Declaring metric variables
2024-08-31 21:17:05,172:INFO:Importing untrained model
2024-08-31 21:17:05,177:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 21:17:05,184:INFO:Starting cross validation
2024-08-31 21:17:05,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:17:05,482:INFO:Calculating mean and std
2024-08-31 21:17:05,483:INFO:Creating metrics dataframe
2024-08-31 21:17:05,485:INFO:Uploading results into container
2024-08-31 21:17:05,485:INFO:Uploading model into container now
2024-08-31 21:17:05,485:INFO:_master_model_container: 11
2024-08-31 21:17:05,486:INFO:_display_container: 2
2024-08-31 21:17:05,486:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 21:17:05,486:INFO:create_model() successfully completed......................................
2024-08-31 21:17:08,362:INFO:SubProcess create_model() end ==================================
2024-08-31 21:17:08,362:INFO:Creating metrics dataframe
2024-08-31 21:17:08,371:INFO:Initializing Extra Trees Classifier
2024-08-31 21:17:08,371:INFO:Total runtime is 0.9613289554913839 minutes
2024-08-31 21:17:08,374:INFO:SubProcess create_model() called ==================================
2024-08-31 21:17:08,374:INFO:Initializing create_model()
2024-08-31 21:17:08,374:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:17:08,374:INFO:Checking exceptions
2024-08-31 21:17:08,374:INFO:Importing libraries
2024-08-31 21:17:08,374:INFO:Copying training dataset
2024-08-31 21:17:08,385:INFO:Defining folds
2024-08-31 21:17:08,385:INFO:Declaring metric variables
2024-08-31 21:17:08,388:INFO:Importing untrained model
2024-08-31 21:17:08,391:INFO:Extra Trees Classifier Imported successfully
2024-08-31 21:17:08,396:INFO:Starting cross validation
2024-08-31 21:17:08,397:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:17:11,821:INFO:Calculating mean and std
2024-08-31 21:17:11,822:INFO:Creating metrics dataframe
2024-08-31 21:17:11,824:INFO:Uploading results into container
2024-08-31 21:17:11,824:INFO:Uploading model into container now
2024-08-31 21:17:11,825:INFO:_master_model_container: 12
2024-08-31 21:17:11,825:INFO:_display_container: 2
2024-08-31 21:17:11,825:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-08-31 21:17:11,825:INFO:create_model() successfully completed......................................
2024-08-31 21:17:14,721:INFO:SubProcess create_model() end ==================================
2024-08-31 21:17:14,721:INFO:Creating metrics dataframe
2024-08-31 21:17:14,728:INFO:Initializing Extreme Gradient Boosting
2024-08-31 21:17:14,728:INFO:Total runtime is 1.0672846833864849 minutes
2024-08-31 21:17:14,731:INFO:SubProcess create_model() called ==================================
2024-08-31 21:17:14,731:INFO:Initializing create_model()
2024-08-31 21:17:14,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:17:14,731:INFO:Checking exceptions
2024-08-31 21:17:14,731:INFO:Importing libraries
2024-08-31 21:17:14,731:INFO:Copying training dataset
2024-08-31 21:17:14,742:INFO:Defining folds
2024-08-31 21:17:14,742:INFO:Declaring metric variables
2024-08-31 21:17:14,746:INFO:Importing untrained model
2024-08-31 21:17:14,750:INFO:Extreme Gradient Boosting Imported successfully
2024-08-31 21:17:14,755:INFO:Starting cross validation
2024-08-31 21:17:14,756:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:17:15,624:INFO:Calculating mean and std
2024-08-31 21:17:15,625:INFO:Creating metrics dataframe
2024-08-31 21:17:15,627:INFO:Uploading results into container
2024-08-31 21:17:15,627:INFO:Uploading model into container now
2024-08-31 21:17:15,628:INFO:_master_model_container: 13
2024-08-31 21:17:15,628:INFO:_display_container: 2
2024-08-31 21:17:15,628:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-08-31 21:17:15,628:INFO:create_model() successfully completed......................................
2024-08-31 21:17:18,553:INFO:SubProcess create_model() end ==================================
2024-08-31 21:17:18,553:INFO:Creating metrics dataframe
2024-08-31 21:17:18,561:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 21:17:18,561:INFO:Total runtime is 1.1311651309331259 minutes
2024-08-31 21:17:18,565:INFO:SubProcess create_model() called ==================================
2024-08-31 21:17:18,565:INFO:Initializing create_model()
2024-08-31 21:17:18,565:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:17:18,565:INFO:Checking exceptions
2024-08-31 21:17:18,565:INFO:Importing libraries
2024-08-31 21:17:18,565:INFO:Copying training dataset
2024-08-31 21:17:18,576:INFO:Defining folds
2024-08-31 21:17:18,576:INFO:Declaring metric variables
2024-08-31 21:17:18,580:INFO:Importing untrained model
2024-08-31 21:17:18,583:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:17:18,588:INFO:Starting cross validation
2024-08-31 21:17:18,589:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:17:20,391:INFO:Calculating mean and std
2024-08-31 21:17:20,393:INFO:Creating metrics dataframe
2024-08-31 21:17:20,396:INFO:Uploading results into container
2024-08-31 21:17:20,397:INFO:Uploading model into container now
2024-08-31 21:17:20,397:INFO:_master_model_container: 14
2024-08-31 21:17:20,398:INFO:_display_container: 2
2024-08-31 21:17:20,399:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:17:20,399:INFO:create_model() successfully completed......................................
2024-08-31 21:17:23,283:INFO:SubProcess create_model() end ==================================
2024-08-31 21:17:23,283:INFO:Creating metrics dataframe
2024-08-31 21:17:23,290:INFO:Initializing CatBoost Classifier
2024-08-31 21:17:23,290:INFO:Total runtime is 1.209989599386851 minutes
2024-08-31 21:17:23,292:INFO:SubProcess create_model() called ==================================
2024-08-31 21:17:23,294:INFO:Initializing create_model()
2024-08-31 21:17:23,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:17:23,294:INFO:Checking exceptions
2024-08-31 21:17:23,294:INFO:Importing libraries
2024-08-31 21:17:23,294:INFO:Copying training dataset
2024-08-31 21:17:23,305:INFO:Defining folds
2024-08-31 21:17:23,305:INFO:Declaring metric variables
2024-08-31 21:17:23,308:INFO:Importing untrained model
2024-08-31 21:17:23,312:INFO:CatBoost Classifier Imported successfully
2024-08-31 21:17:23,317:INFO:Starting cross validation
2024-08-31 21:17:23,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:18:11,603:INFO:Calculating mean and std
2024-08-31 21:18:11,606:INFO:Creating metrics dataframe
2024-08-31 21:18:11,609:INFO:Uploading results into container
2024-08-31 21:18:11,610:INFO:Uploading model into container now
2024-08-31 21:18:11,610:INFO:_master_model_container: 15
2024-08-31 21:18:11,610:INFO:_display_container: 2
2024-08-31 21:18:11,610:INFO:<catboost.core.CatBoostClassifier object at 0x00000266ABB48A10>
2024-08-31 21:18:11,610:INFO:create_model() successfully completed......................................
2024-08-31 21:18:14,578:INFO:SubProcess create_model() end ==================================
2024-08-31 21:18:14,578:INFO:Creating metrics dataframe
2024-08-31 21:18:14,588:INFO:Initializing Dummy Classifier
2024-08-31 21:18:14,588:INFO:Total runtime is 2.064947505791982 minutes
2024-08-31 21:18:14,590:INFO:SubProcess create_model() called ==================================
2024-08-31 21:18:14,591:INFO:Initializing create_model()
2024-08-31 21:18:14,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266DAFC3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:18:14,591:INFO:Checking exceptions
2024-08-31 21:18:14,591:INFO:Importing libraries
2024-08-31 21:18:14,591:INFO:Copying training dataset
2024-08-31 21:18:14,602:INFO:Defining folds
2024-08-31 21:18:14,602:INFO:Declaring metric variables
2024-08-31 21:18:14,605:INFO:Importing untrained model
2024-08-31 21:18:14,607:INFO:Dummy Classifier Imported successfully
2024-08-31 21:18:14,612:INFO:Starting cross validation
2024-08-31 21:18:14,614:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:18:14,775:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,786:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,790:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,793:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,793:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,797:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,803:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,807:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,810:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 21:18:14,815:INFO:Calculating mean and std
2024-08-31 21:18:14,816:INFO:Creating metrics dataframe
2024-08-31 21:18:14,817:INFO:Uploading results into container
2024-08-31 21:18:14,818:INFO:Uploading model into container now
2024-08-31 21:18:14,818:INFO:_master_model_container: 16
2024-08-31 21:18:14,818:INFO:_display_container: 2
2024-08-31 21:18:14,818:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-08-31 21:18:14,818:INFO:create_model() successfully completed......................................
2024-08-31 21:18:17,719:INFO:SubProcess create_model() end ==================================
2024-08-31 21:18:17,719:INFO:Creating metrics dataframe
2024-08-31 21:18:17,736:INFO:Initializing create_model()
2024-08-31 21:18:17,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:18:17,736:INFO:Checking exceptions
2024-08-31 21:18:17,738:INFO:Importing libraries
2024-08-31 21:18:17,738:INFO:Copying training dataset
2024-08-31 21:18:17,748:INFO:Defining folds
2024-08-31 21:18:17,748:INFO:Declaring metric variables
2024-08-31 21:18:17,748:INFO:Importing untrained model
2024-08-31 21:18:17,748:INFO:Declaring custom model
2024-08-31 21:18:17,749:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:18:17,749:INFO:Cross validation set to False
2024-08-31 21:18:17,751:INFO:Fitting Model
2024-08-31 21:18:17,836:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:18:17,837:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:18:17,839:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.
2024-08-31 21:18:17,839:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:18:17,839:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:18:17,839:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:18:17,839:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:18:17,839:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:18:18,021:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:18:18,021:INFO:create_model() successfully completed......................................
2024-08-31 21:18:21,212:INFO:_master_model_container: 16
2024-08-31 21:18:21,212:INFO:_display_container: 2
2024-08-31 21:18:21,213:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:18:21,213:INFO:compare_models() successfully completed......................................
2024-08-31 21:18:21,213:INFO:Initializing tune_model()
2024-08-31 21:18:21,215:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-08-31 21:18:21,215:INFO:Checking exceptions
2024-08-31 21:18:21,233:INFO:Copying training dataset
2024-08-31 21:18:21,247:INFO:Checking base model
2024-08-31 21:18:21,247:INFO:Base model : Light Gradient Boosting Machine
2024-08-31 21:18:21,253:INFO:Declaring metric variables
2024-08-31 21:18:21,255:INFO:Defining Hyperparameters
2024-08-31 21:18:24,657:INFO:Tuning with n_jobs=-1
2024-08-31 21:18:24,657:INFO:Initializing RandomizedSearchCV
2024-08-31 21:18:56,187:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-08-31 21:18:56,189:INFO:Hyperparameter search completed
2024-08-31 21:18:56,189:INFO:SubProcess create_model() called ==================================
2024-08-31 21:18:56,190:INFO:Initializing create_model()
2024-08-31 21:18:56,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000266ABD80710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-08-31 21:18:56,190:INFO:Checking exceptions
2024-08-31 21:18:56,190:INFO:Importing libraries
2024-08-31 21:18:56,191:INFO:Copying training dataset
2024-08-31 21:18:56,217:INFO:Defining folds
2024-08-31 21:18:56,217:INFO:Declaring metric variables
2024-08-31 21:18:56,222:INFO:Importing untrained model
2024-08-31 21:18:56,222:INFO:Declaring custom model
2024-08-31 21:18:56,227:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:18:56,237:INFO:Starting cross validation
2024-08-31 21:18:56,240:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:19:02,984:INFO:Calculating mean and std
2024-08-31 21:19:02,986:INFO:Creating metrics dataframe
2024-08-31 21:19:02,999:INFO:Finalizing model
2024-08-31 21:19:03,152:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 21:19:03,152:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 21:19:03,152:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 21:19:03,177:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:19:03,178:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-08-31 21:19:03,178:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-08-31 21:19:03,178:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-08-31 21:19:03,178:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:19:03,182:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001032 seconds.
2024-08-31 21:19:03,182:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:19:03,182:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:19:03,182:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:19:03,182:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:19:03,183:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:19:03,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,619:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,680:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,772:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,785:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-08-31 21:19:03,792:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-08-31 21:19:03,811:INFO:Uploading results into container
2024-08-31 21:19:03,812:INFO:Uploading model into container now
2024-08-31 21:19:03,813:INFO:_master_model_container: 17
2024-08-31 21:19:03,813:INFO:_display_container: 3
2024-08-31 21:19:03,814:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:19:03,814:INFO:create_model() successfully completed......................................
2024-08-31 21:19:06,925:INFO:SubProcess create_model() end ==================================
2024-08-31 21:19:06,925:INFO:choose_better activated
2024-08-31 21:19:06,927:INFO:SubProcess create_model() called ==================================
2024-08-31 21:19:06,927:INFO:Initializing create_model()
2024-08-31 21:19:06,927:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:19:06,927:INFO:Checking exceptions
2024-08-31 21:19:06,928:INFO:Importing libraries
2024-08-31 21:19:06,928:INFO:Copying training dataset
2024-08-31 21:19:06,940:INFO:Defining folds
2024-08-31 21:19:06,940:INFO:Declaring metric variables
2024-08-31 21:19:06,940:INFO:Importing untrained model
2024-08-31 21:19:06,940:INFO:Declaring custom model
2024-08-31 21:19:06,941:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:19:06,941:INFO:Starting cross validation
2024-08-31 21:19:06,941:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 21:19:08,897:INFO:Calculating mean and std
2024-08-31 21:19:08,897:INFO:Creating metrics dataframe
2024-08-31 21:19:08,899:INFO:Finalizing model
2024-08-31 21:19:09,020:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:19:09,020:INFO:[LightGBM] [Info] Number of positive: 13834, number of negative: 13834
2024-08-31 21:19:09,022:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000656 seconds.
2024-08-31 21:19:09,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:19:09,022:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:19:09,023:INFO:[LightGBM] [Info] Total Bins 3808
2024-08-31 21:19:09,023:INFO:[LightGBM] [Info] Number of data points in the train set: 27668, number of used features: 15
2024-08-31 21:19:09,023:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:19:09,266:INFO:Uploading results into container
2024-08-31 21:19:09,267:INFO:Uploading model into container now
2024-08-31 21:19:09,267:INFO:_master_model_container: 18
2024-08-31 21:19:09,268:INFO:_display_container: 4
2024-08-31 21:19:09,268:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:19:09,268:INFO:create_model() successfully completed......................................
2024-08-31 21:19:12,243:INFO:SubProcess create_model() end ==================================
2024-08-31 21:19:12,243:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.7056
2024-08-31 21:19:12,245:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-08-31 21:19:12,245:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-08-31 21:19:12,245:INFO:choose_better completed
2024-08-31 21:19:12,245:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-08-31 21:19:12,253:INFO:_master_model_container: 18
2024-08-31 21:19:12,253:INFO:_display_container: 3
2024-08-31 21:19:12,253:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:19:12,253:INFO:tune_model() successfully completed......................................
2024-08-31 21:19:15,242:INFO:Initializing finalize_model()
2024-08-31 21:19:15,242:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-08-31 21:19:15,242:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 21:19:15,248:INFO:Initializing create_model()
2024-08-31 21:19:15,249:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 21:19:15,249:INFO:Checking exceptions
2024-08-31 21:19:15,250:INFO:Importing libraries
2024-08-31 21:19:15,250:INFO:Copying training dataset
2024-08-31 21:19:15,250:INFO:Defining folds
2024-08-31 21:19:15,250:INFO:Declaring metric variables
2024-08-31 21:19:15,251:INFO:Importing untrained model
2024-08-31 21:19:15,251:INFO:Declaring custom model
2024-08-31 21:19:15,251:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 21:19:15,252:INFO:Cross validation set to False
2024-08-31 21:19:15,252:INFO:Fitting Model
2024-08-31 21:19:15,375:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-08-31 21:19:15,375:INFO:[LightGBM] [Info] Number of positive: 19763, number of negative: 19763
2024-08-31 21:19:15,378:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.
2024-08-31 21:19:15,378:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-08-31 21:19:15,378:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-08-31 21:19:15,378:INFO:[LightGBM] [Info] Total Bins 3807
2024-08-31 21:19:15,378:INFO:[LightGBM] [Info] Number of data points in the train set: 39526, number of used features: 15
2024-08-31 21:19:15,379:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-08-31 21:19:15,558:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 21:19:15,558:INFO:create_model() successfully completed......................................
2024-08-31 21:19:18,464:INFO:_master_model_container: 18
2024-08-31 21:19:18,464:INFO:_display_container: 3
2024-08-31 21:19:18,470:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-08-31 21:19:18,470:INFO:finalize_model() successfully completed......................................
2024-08-31 21:19:21,402:INFO:Initializing predict_model()
2024-08-31 21:19:21,402:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266AB31FA60>)
2024-08-31 21:19:21,402:INFO:Checking exceptions
2024-08-31 21:19:21,402:INFO:Preloading libraries
2024-08-31 21:19:21,405:INFO:Set up data.
2024-08-31 21:19:21,412:INFO:Set up index.
2024-08-31 21:22:00,826:INFO:Initializing predict_model()
2024-08-31 21:22:00,826:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000266ACBF7650>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000266AB58DA80>)
2024-08-31 21:22:00,826:INFO:Checking exceptions
2024-08-31 21:22:00,827:INFO:Preloading libraries
2024-08-31 21:22:00,829:INFO:Set up data.
2024-08-31 21:22:00,837:INFO:Set up index.
2024-09-04 20:51:20,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 20:51:20,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 20:51:20,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 20:51:20,107:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-04 20:51:20,764:INFO:PyCaret ClassificationExperiment
2024-09-04 20:51:20,764:INFO:Logging name: clf-default-name
2024-09-04 20:51:20,764:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 20:51:20,764:INFO:version 3.3.2
2024-09-04 20:51:20,764:INFO:Initializing setup()
2024-09-04 20:51:20,764:INFO:self.USI: 4cfb
2024-09-04 20:51:20,764:INFO:self._variable_keys: {'pipeline', '_ml_usecase', 'y_train', 'y', 'y_test', 'exp_id', 'seed', 'X', 'exp_name_log', 'fold_generator', 'gpu_n_jobs_param', 'fold_shuffle_param', 'fold_groups_param', 'data', 'memory', 'n_jobs_param', 'is_multiclass', '_available_plots', 'idx', 'log_plots_param', 'X_train', 'X_test', 'html_param', 'target_param', 'logging_param', 'USI', 'gpu_param', 'fix_imbalance'}
2024-09-04 20:51:20,764:INFO:Checking environment
2024-09-04 20:51:20,764:INFO:python_version: 3.11.9
2024-09-04 20:51:20,764:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-04 20:51:20,765:INFO:machine: AMD64
2024-09-04 20:51:20,765:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-04 20:51:20,771:INFO:Memory: svmem(total=16867028992, available=3036348416, percent=82.0, used=13830680576, free=3036348416)
2024-09-04 20:51:20,771:INFO:Physical Core: 6
2024-09-04 20:51:20,771:INFO:Logical Core: 12
2024-09-04 20:51:20,771:INFO:Checking libraries
2024-09-04 20:51:20,771:INFO:System:
2024-09-04 20:51:20,771:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-04 20:51:20,771:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-09-04 20:51:20,771:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-04 20:51:20,771:INFO:PyCaret required dependencies:
2024-09-04 20:51:20,773:INFO:                 pip: 23.2.1
2024-09-04 20:51:20,774:INFO:          setuptools: 67.8.0
2024-09-04 20:51:20,774:INFO:             pycaret: 3.3.2
2024-09-04 20:51:20,774:INFO:             IPython: 8.14.0
2024-09-04 20:51:20,774:INFO:          ipywidgets: 8.1.5
2024-09-04 20:51:20,774:INFO:                tqdm: 4.66.5
2024-09-04 20:51:20,774:INFO:               numpy: 1.24.3
2024-09-04 20:51:20,774:INFO:              pandas: 2.0.3
2024-09-04 20:51:20,774:INFO:              jinja2: 3.1.4
2024-09-04 20:51:20,774:INFO:               scipy: 1.10.1
2024-09-04 20:51:20,774:INFO:              joblib: 1.2.0
2024-09-04 20:51:20,774:INFO:             sklearn: 1.4.2
2024-09-04 20:51:20,774:INFO:                pyod: 2.0.1
2024-09-04 20:51:20,774:INFO:            imblearn: 0.12.3
2024-09-04 20:51:20,774:INFO:   category_encoders: 2.6.3
2024-09-04 20:51:20,774:INFO:            lightgbm: 4.5.0
2024-09-04 20:51:20,774:INFO:               numba: 0.60.0
2024-09-04 20:51:20,774:INFO:            requests: 2.32.3
2024-09-04 20:51:20,774:INFO:          matplotlib: 3.7.1
2024-09-04 20:51:20,774:INFO:          scikitplot: 0.3.7
2024-09-04 20:51:20,774:INFO:         yellowbrick: 1.5
2024-09-04 20:51:20,774:INFO:              plotly: 5.16.1
2024-09-04 20:51:20,774:INFO:    plotly-resampler: Not installed
2024-09-04 20:51:20,774:INFO:             kaleido: 0.2.1
2024-09-04 20:51:20,774:INFO:           schemdraw: 0.15
2024-09-04 20:51:20,774:INFO:         statsmodels: 0.14.2
2024-09-04 20:51:20,774:INFO:              sktime: 0.26.0
2024-09-04 20:51:20,774:INFO:               tbats: 1.1.3
2024-09-04 20:51:20,774:INFO:            pmdarima: 2.0.4
2024-09-04 20:51:20,774:INFO:              psutil: 5.9.0
2024-09-04 20:51:20,774:INFO:          markupsafe: 2.1.3
2024-09-04 20:51:20,775:INFO:             pickle5: Not installed
2024-09-04 20:51:20,775:INFO:         cloudpickle: 3.0.0
2024-09-04 20:51:20,775:INFO:         deprecation: 2.1.0
2024-09-04 20:51:20,775:INFO:              xxhash: 3.5.0
2024-09-04 20:51:20,775:INFO:           wurlitzer: Not installed
2024-09-04 20:51:20,775:INFO:PyCaret optional dependencies:
2024-09-04 20:51:26,707:INFO:                shap: Not installed
2024-09-04 20:51:26,707:INFO:           interpret: Not installed
2024-09-04 20:51:26,707:INFO:                umap: Not installed
2024-09-04 20:51:26,707:INFO:     ydata_profiling: Not installed
2024-09-04 20:51:26,707:INFO:  explainerdashboard: Not installed
2024-09-04 20:51:26,707:INFO:             autoviz: Not installed
2024-09-04 20:51:26,707:INFO:           fairlearn: Not installed
2024-09-04 20:51:26,707:INFO:          deepchecks: Not installed
2024-09-04 20:51:26,707:INFO:             xgboost: 2.0.2
2024-09-04 20:51:26,707:INFO:            catboost: 1.2.5
2024-09-04 20:51:26,708:INFO:              kmodes: Not installed
2024-09-04 20:51:26,708:INFO:             mlxtend: Not installed
2024-09-04 20:51:26,708:INFO:       statsforecast: Not installed
2024-09-04 20:51:26,708:INFO:        tune_sklearn: Not installed
2024-09-04 20:51:26,708:INFO:                 ray: Not installed
2024-09-04 20:51:26,708:INFO:            hyperopt: Not installed
2024-09-04 20:51:26,708:INFO:              optuna: 4.0.0
2024-09-04 20:51:26,708:INFO:               skopt: Not installed
2024-09-04 20:51:26,708:INFO:              mlflow: Not installed
2024-09-04 20:51:26,708:INFO:              gradio: 4.41.0
2024-09-04 20:51:26,708:INFO:             fastapi: 0.112.1
2024-09-04 20:51:26,708:INFO:             uvicorn: 0.30.6
2024-09-04 20:51:26,708:INFO:              m2cgen: Not installed
2024-09-04 20:51:26,708:INFO:           evidently: Not installed
2024-09-04 20:51:26,708:INFO:               fugue: Not installed
2024-09-04 20:51:26,708:INFO:           streamlit: 1.38.0
2024-09-04 20:51:26,708:INFO:             prophet: Not installed
2024-09-04 20:51:26,709:INFO:None
2024-09-04 20:51:26,709:INFO:Set up data.
2024-09-04 20:51:26,731:INFO:Set up folding strategy.
2024-09-04 20:51:26,731:INFO:Set up train/test split.
2024-09-04 20:51:26,757:INFO:Set up index.
2024-09-04 20:51:26,759:INFO:Assigning column types.
2024-09-04 20:51:26,776:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 20:51:26,847:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 20:51:26,854:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:51:26,928:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:26,933:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,007:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 20:51:27,008:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:51:27,041:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,044:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,045:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 20:51:27,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:51:27,132:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,135:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,190:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:51:27,220:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,223:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,224:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 20:51:27,318:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,323:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,433:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,436:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:27,442:INFO:Preparing preprocessing pipeline...
2024-09-04 20:51:27,445:INFO:Set up simple imputation.
2024-09-04 20:51:27,448:INFO:Set up column name cleaning.
2024-09-04 20:51:27,511:INFO:Finished creating preprocessing pipeline.
2024-09-04 20:51:27,520:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-04 20:51:27,520:INFO:Creating final display dataframe.
2024-09-04 20:51:27,770:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              4cfb
2024-09-04 20:51:27,909:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:27,914:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:28,015:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:51:28,019:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:51:28,021:INFO:setup() successfully completed in 7.26s...............
2024-09-04 20:51:28,021:INFO:Initializing compare_models()
2024-09-04 20:51:28,021:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, include=None, exclude=None, fold=5, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-04 20:51:28,021:INFO:Checking exceptions
2024-09-04 20:51:28,033:INFO:Preparing display monitor
2024-09-04 20:51:28,066:INFO:Initializing Logistic Regression
2024-09-04 20:51:28,066:INFO:Total runtime is 8.79367192586263e-06 minutes
2024-09-04 20:51:28,070:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:28,070:INFO:Initializing create_model()
2024-09-04 20:51:28,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:28,070:INFO:Checking exceptions
2024-09-04 20:51:28,071:INFO:Importing libraries
2024-09-04 20:51:28,071:INFO:Copying training dataset
2024-09-04 20:51:28,090:INFO:Defining folds
2024-09-04 20:51:28,091:INFO:Declaring metric variables
2024-09-04 20:51:28,095:INFO:Importing untrained model
2024-09-04 20:51:28,100:INFO:Logistic Regression Imported successfully
2024-09-04 20:51:28,108:INFO:Starting cross validation
2024-09-04 20:51:28,110:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:36,760:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 20:51:36,782:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 20:51:36,806:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 20:51:36,811:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 20:51:36,830:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 20:51:36,867:INFO:Calculating mean and std
2024-09-04 20:51:36,869:INFO:Creating metrics dataframe
2024-09-04 20:51:36,872:INFO:Uploading results into container
2024-09-04 20:51:36,872:INFO:Uploading model into container now
2024-09-04 20:51:36,874:INFO:_master_model_container: 1
2024-09-04 20:51:36,874:INFO:_display_container: 2
2024-09-04 20:51:36,876:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-04 20:51:36,876:INFO:create_model() successfully completed......................................
2024-09-04 20:51:37,049:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:37,049:INFO:Creating metrics dataframe
2024-09-04 20:51:37,063:INFO:Initializing K Neighbors Classifier
2024-09-04 20:51:37,063:INFO:Total runtime is 0.14996193647384642 minutes
2024-09-04 20:51:37,067:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:37,068:INFO:Initializing create_model()
2024-09-04 20:51:37,068:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:37,068:INFO:Checking exceptions
2024-09-04 20:51:37,068:INFO:Importing libraries
2024-09-04 20:51:37,069:INFO:Copying training dataset
2024-09-04 20:51:37,095:INFO:Defining folds
2024-09-04 20:51:37,095:INFO:Declaring metric variables
2024-09-04 20:51:37,102:INFO:Importing untrained model
2024-09-04 20:51:37,107:INFO:K Neighbors Classifier Imported successfully
2024-09-04 20:51:37,123:INFO:Starting cross validation
2024-09-04 20:51:37,125:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:40,969:INFO:Calculating mean and std
2024-09-04 20:51:40,971:INFO:Creating metrics dataframe
2024-09-04 20:51:40,975:INFO:Uploading results into container
2024-09-04 20:51:40,976:INFO:Uploading model into container now
2024-09-04 20:51:40,977:INFO:_master_model_container: 2
2024-09-04 20:51:40,977:INFO:_display_container: 2
2024-09-04 20:51:40,978:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-04 20:51:40,978:INFO:create_model() successfully completed......................................
2024-09-04 20:51:41,149:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:41,149:INFO:Creating metrics dataframe
2024-09-04 20:51:41,157:INFO:Initializing Naive Bayes
2024-09-04 20:51:41,157:INFO:Total runtime is 0.21819212834040325 minutes
2024-09-04 20:51:41,161:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:41,161:INFO:Initializing create_model()
2024-09-04 20:51:41,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:41,161:INFO:Checking exceptions
2024-09-04 20:51:41,161:INFO:Importing libraries
2024-09-04 20:51:41,161:INFO:Copying training dataset
2024-09-04 20:51:41,177:INFO:Defining folds
2024-09-04 20:51:41,177:INFO:Declaring metric variables
2024-09-04 20:51:41,182:INFO:Importing untrained model
2024-09-04 20:51:41,185:INFO:Naive Bayes Imported successfully
2024-09-04 20:51:41,192:INFO:Starting cross validation
2024-09-04 20:51:41,193:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:43,717:INFO:Calculating mean and std
2024-09-04 20:51:43,719:INFO:Creating metrics dataframe
2024-09-04 20:51:43,722:INFO:Uploading results into container
2024-09-04 20:51:43,722:INFO:Uploading model into container now
2024-09-04 20:51:43,723:INFO:_master_model_container: 3
2024-09-04 20:51:43,723:INFO:_display_container: 2
2024-09-04 20:51:43,723:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-04 20:51:43,724:INFO:create_model() successfully completed......................................
2024-09-04 20:51:43,893:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:43,893:INFO:Creating metrics dataframe
2024-09-04 20:51:43,901:INFO:Initializing Decision Tree Classifier
2024-09-04 20:51:43,902:INFO:Total runtime is 0.26393614610036215 minutes
2024-09-04 20:51:43,905:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:43,905:INFO:Initializing create_model()
2024-09-04 20:51:43,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:43,905:INFO:Checking exceptions
2024-09-04 20:51:43,905:INFO:Importing libraries
2024-09-04 20:51:43,906:INFO:Copying training dataset
2024-09-04 20:51:43,923:INFO:Defining folds
2024-09-04 20:51:43,923:INFO:Declaring metric variables
2024-09-04 20:51:43,928:INFO:Importing untrained model
2024-09-04 20:51:43,933:INFO:Decision Tree Classifier Imported successfully
2024-09-04 20:51:43,941:INFO:Starting cross validation
2024-09-04 20:51:43,943:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:44,137:INFO:Calculating mean and std
2024-09-04 20:51:44,139:INFO:Creating metrics dataframe
2024-09-04 20:51:44,141:INFO:Uploading results into container
2024-09-04 20:51:44,142:INFO:Uploading model into container now
2024-09-04 20:51:44,143:INFO:_master_model_container: 4
2024-09-04 20:51:44,143:INFO:_display_container: 2
2024-09-04 20:51:44,144:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-04 20:51:44,144:INFO:create_model() successfully completed......................................
2024-09-04 20:51:44,304:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:44,304:INFO:Creating metrics dataframe
2024-09-04 20:51:44,317:INFO:Initializing SVM - Linear Kernel
2024-09-04 20:51:44,317:INFO:Total runtime is 0.2708660165468852 minutes
2024-09-04 20:51:44,322:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:44,322:INFO:Initializing create_model()
2024-09-04 20:51:44,323:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:44,323:INFO:Checking exceptions
2024-09-04 20:51:44,323:INFO:Importing libraries
2024-09-04 20:51:44,323:INFO:Copying training dataset
2024-09-04 20:51:44,341:INFO:Defining folds
2024-09-04 20:51:44,341:INFO:Declaring metric variables
2024-09-04 20:51:44,346:INFO:Importing untrained model
2024-09-04 20:51:44,351:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 20:51:44,358:INFO:Starting cross validation
2024-09-04 20:51:44,359:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:44,754:INFO:Calculating mean and std
2024-09-04 20:51:44,755:INFO:Creating metrics dataframe
2024-09-04 20:51:44,758:INFO:Uploading results into container
2024-09-04 20:51:44,759:INFO:Uploading model into container now
2024-09-04 20:51:44,760:INFO:_master_model_container: 5
2024-09-04 20:51:44,760:INFO:_display_container: 2
2024-09-04 20:51:44,760:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 20:51:44,760:INFO:create_model() successfully completed......................................
2024-09-04 20:51:44,933:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:44,933:INFO:Creating metrics dataframe
2024-09-04 20:51:44,943:INFO:Initializing Ridge Classifier
2024-09-04 20:51:44,943:INFO:Total runtime is 0.28128765821456914 minutes
2024-09-04 20:51:44,948:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:44,949:INFO:Initializing create_model()
2024-09-04 20:51:44,950:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:44,950:INFO:Checking exceptions
2024-09-04 20:51:44,950:INFO:Importing libraries
2024-09-04 20:51:44,950:INFO:Copying training dataset
2024-09-04 20:51:44,969:INFO:Defining folds
2024-09-04 20:51:44,969:INFO:Declaring metric variables
2024-09-04 20:51:44,974:INFO:Importing untrained model
2024-09-04 20:51:44,980:INFO:Ridge Classifier Imported successfully
2024-09-04 20:51:44,989:INFO:Starting cross validation
2024-09-04 20:51:44,991:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:45,133:INFO:Calculating mean and std
2024-09-04 20:51:45,134:INFO:Creating metrics dataframe
2024-09-04 20:51:45,137:INFO:Uploading results into container
2024-09-04 20:51:45,137:INFO:Uploading model into container now
2024-09-04 20:51:45,138:INFO:_master_model_container: 6
2024-09-04 20:51:45,138:INFO:_display_container: 2
2024-09-04 20:51:45,138:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-04 20:51:45,139:INFO:create_model() successfully completed......................................
2024-09-04 20:51:45,288:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:45,288:INFO:Creating metrics dataframe
2024-09-04 20:51:45,297:INFO:Initializing Random Forest Classifier
2024-09-04 20:51:45,297:INFO:Total runtime is 0.28718767563501996 minutes
2024-09-04 20:51:45,300:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:45,301:INFO:Initializing create_model()
2024-09-04 20:51:45,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:45,301:INFO:Checking exceptions
2024-09-04 20:51:45,301:INFO:Importing libraries
2024-09-04 20:51:45,301:INFO:Copying training dataset
2024-09-04 20:51:45,317:INFO:Defining folds
2024-09-04 20:51:45,317:INFO:Declaring metric variables
2024-09-04 20:51:45,321:INFO:Importing untrained model
2024-09-04 20:51:45,324:INFO:Random Forest Classifier Imported successfully
2024-09-04 20:51:45,330:INFO:Starting cross validation
2024-09-04 20:51:45,331:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:47,205:INFO:Calculating mean and std
2024-09-04 20:51:47,211:INFO:Creating metrics dataframe
2024-09-04 20:51:47,221:INFO:Uploading results into container
2024-09-04 20:51:47,223:INFO:Uploading model into container now
2024-09-04 20:51:47,225:INFO:_master_model_container: 7
2024-09-04 20:51:47,225:INFO:_display_container: 2
2024-09-04 20:51:47,226:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-04 20:51:47,226:INFO:create_model() successfully completed......................................
2024-09-04 20:51:47,421:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:47,421:INFO:Creating metrics dataframe
2024-09-04 20:51:47,434:INFO:Initializing Quadratic Discriminant Analysis
2024-09-04 20:51:47,434:INFO:Total runtime is 0.32280150254567463 minutes
2024-09-04 20:51:47,439:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:47,439:INFO:Initializing create_model()
2024-09-04 20:51:47,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:47,439:INFO:Checking exceptions
2024-09-04 20:51:47,439:INFO:Importing libraries
2024-09-04 20:51:47,440:INFO:Copying training dataset
2024-09-04 20:51:47,459:INFO:Defining folds
2024-09-04 20:51:47,460:INFO:Declaring metric variables
2024-09-04 20:51:47,465:INFO:Importing untrained model
2024-09-04 20:51:47,469:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-04 20:51:47,477:INFO:Starting cross validation
2024-09-04 20:51:47,478:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:47,603:INFO:Calculating mean and std
2024-09-04 20:51:47,604:INFO:Creating metrics dataframe
2024-09-04 20:51:47,606:INFO:Uploading results into container
2024-09-04 20:51:47,607:INFO:Uploading model into container now
2024-09-04 20:51:47,608:INFO:_master_model_container: 8
2024-09-04 20:51:47,608:INFO:_display_container: 2
2024-09-04 20:51:47,608:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-04 20:51:47,608:INFO:create_model() successfully completed......................................
2024-09-04 20:51:47,752:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:47,752:INFO:Creating metrics dataframe
2024-09-04 20:51:47,765:INFO:Initializing Ada Boost Classifier
2024-09-04 20:51:47,765:INFO:Total runtime is 0.3283274412155151 minutes
2024-09-04 20:51:47,769:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:47,770:INFO:Initializing create_model()
2024-09-04 20:51:47,770:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:47,770:INFO:Checking exceptions
2024-09-04 20:51:47,770:INFO:Importing libraries
2024-09-04 20:51:47,770:INFO:Copying training dataset
2024-09-04 20:51:47,787:INFO:Defining folds
2024-09-04 20:51:47,787:INFO:Declaring metric variables
2024-09-04 20:51:47,791:INFO:Importing untrained model
2024-09-04 20:51:47,795:INFO:Ada Boost Classifier Imported successfully
2024-09-04 20:51:47,804:INFO:Starting cross validation
2024-09-04 20:51:47,805:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:47,846:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 20:51:47,851:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 20:51:47,852:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 20:51:47,855:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 20:51:47,862:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 20:51:48,679:INFO:Calculating mean and std
2024-09-04 20:51:48,680:INFO:Creating metrics dataframe
2024-09-04 20:51:48,682:INFO:Uploading results into container
2024-09-04 20:51:48,684:INFO:Uploading model into container now
2024-09-04 20:51:48,684:INFO:_master_model_container: 9
2024-09-04 20:51:48,684:INFO:_display_container: 2
2024-09-04 20:51:48,685:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-04 20:51:48,685:INFO:create_model() successfully completed......................................
2024-09-04 20:51:48,824:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:48,824:INFO:Creating metrics dataframe
2024-09-04 20:51:48,834:INFO:Initializing Gradient Boosting Classifier
2024-09-04 20:51:48,834:INFO:Total runtime is 0.3461475213368733 minutes
2024-09-04 20:51:48,839:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:48,839:INFO:Initializing create_model()
2024-09-04 20:51:48,839:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:48,839:INFO:Checking exceptions
2024-09-04 20:51:48,839:INFO:Importing libraries
2024-09-04 20:51:48,839:INFO:Copying training dataset
2024-09-04 20:51:48,852:INFO:Defining folds
2024-09-04 20:51:48,852:INFO:Declaring metric variables
2024-09-04 20:51:48,855:INFO:Importing untrained model
2024-09-04 20:51:48,859:INFO:Gradient Boosting Classifier Imported successfully
2024-09-04 20:51:48,867:INFO:Starting cross validation
2024-09-04 20:51:48,867:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:51,537:INFO:Calculating mean and std
2024-09-04 20:51:51,539:INFO:Creating metrics dataframe
2024-09-04 20:51:51,542:INFO:Uploading results into container
2024-09-04 20:51:51,543:INFO:Uploading model into container now
2024-09-04 20:51:51,543:INFO:_master_model_container: 10
2024-09-04 20:51:51,544:INFO:_display_container: 2
2024-09-04 20:51:51,545:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-04 20:51:51,545:INFO:create_model() successfully completed......................................
2024-09-04 20:51:51,722:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:51,722:INFO:Creating metrics dataframe
2024-09-04 20:51:51,736:INFO:Initializing Linear Discriminant Analysis
2024-09-04 20:51:51,737:INFO:Total runtime is 0.39452463785807285 minutes
2024-09-04 20:51:51,741:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:51,741:INFO:Initializing create_model()
2024-09-04 20:51:51,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:51,741:INFO:Checking exceptions
2024-09-04 20:51:51,741:INFO:Importing libraries
2024-09-04 20:51:51,742:INFO:Copying training dataset
2024-09-04 20:51:51,760:INFO:Defining folds
2024-09-04 20:51:51,760:INFO:Declaring metric variables
2024-09-04 20:51:51,774:INFO:Importing untrained model
2024-09-04 20:51:51,787:INFO:Linear Discriminant Analysis Imported successfully
2024-09-04 20:51:51,802:INFO:Starting cross validation
2024-09-04 20:51:51,805:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:52,025:INFO:Calculating mean and std
2024-09-04 20:51:52,027:INFO:Creating metrics dataframe
2024-09-04 20:51:52,029:INFO:Uploading results into container
2024-09-04 20:51:52,029:INFO:Uploading model into container now
2024-09-04 20:51:52,030:INFO:_master_model_container: 11
2024-09-04 20:51:52,030:INFO:_display_container: 2
2024-09-04 20:51:52,030:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-04 20:51:52,030:INFO:create_model() successfully completed......................................
2024-09-04 20:51:52,203:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:52,203:INFO:Creating metrics dataframe
2024-09-04 20:51:52,213:INFO:Initializing Extra Trees Classifier
2024-09-04 20:51:52,213:INFO:Total runtime is 0.40245169003804515 minutes
2024-09-04 20:51:52,216:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:52,217:INFO:Initializing create_model()
2024-09-04 20:51:52,217:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:52,217:INFO:Checking exceptions
2024-09-04 20:51:52,217:INFO:Importing libraries
2024-09-04 20:51:52,217:INFO:Copying training dataset
2024-09-04 20:51:52,233:INFO:Defining folds
2024-09-04 20:51:52,233:INFO:Declaring metric variables
2024-09-04 20:51:52,237:INFO:Importing untrained model
2024-09-04 20:51:52,242:INFO:Extra Trees Classifier Imported successfully
2024-09-04 20:51:52,250:INFO:Starting cross validation
2024-09-04 20:51:52,251:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:53,796:INFO:Calculating mean and std
2024-09-04 20:51:53,797:INFO:Creating metrics dataframe
2024-09-04 20:51:53,802:INFO:Uploading results into container
2024-09-04 20:51:53,803:INFO:Uploading model into container now
2024-09-04 20:51:53,804:INFO:_master_model_container: 12
2024-09-04 20:51:53,804:INFO:_display_container: 2
2024-09-04 20:51:53,804:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-04 20:51:53,805:INFO:create_model() successfully completed......................................
2024-09-04 20:51:54,053:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:54,053:INFO:Creating metrics dataframe
2024-09-04 20:51:54,063:INFO:Initializing Extreme Gradient Boosting
2024-09-04 20:51:54,063:INFO:Total runtime is 0.43329607645670565 minutes
2024-09-04 20:51:54,067:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:54,067:INFO:Initializing create_model()
2024-09-04 20:51:54,068:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:54,068:INFO:Checking exceptions
2024-09-04 20:51:54,068:INFO:Importing libraries
2024-09-04 20:51:54,068:INFO:Copying training dataset
2024-09-04 20:51:54,085:INFO:Defining folds
2024-09-04 20:51:54,085:INFO:Declaring metric variables
2024-09-04 20:51:54,090:INFO:Importing untrained model
2024-09-04 20:51:54,096:INFO:Extreme Gradient Boosting Imported successfully
2024-09-04 20:51:54,102:INFO:Starting cross validation
2024-09-04 20:51:54,104:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:54,600:INFO:Calculating mean and std
2024-09-04 20:51:54,601:INFO:Creating metrics dataframe
2024-09-04 20:51:54,603:INFO:Uploading results into container
2024-09-04 20:51:54,603:INFO:Uploading model into container now
2024-09-04 20:51:54,604:INFO:_master_model_container: 13
2024-09-04 20:51:54,604:INFO:_display_container: 2
2024-09-04 20:51:54,605:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-04 20:51:54,605:INFO:create_model() successfully completed......................................
2024-09-04 20:51:54,749:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:54,749:INFO:Creating metrics dataframe
2024-09-04 20:51:54,761:INFO:Initializing Light Gradient Boosting Machine
2024-09-04 20:51:54,761:INFO:Total runtime is 0.44493105014165235 minutes
2024-09-04 20:51:54,764:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:54,765:INFO:Initializing create_model()
2024-09-04 20:51:54,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:54,765:INFO:Checking exceptions
2024-09-04 20:51:54,765:INFO:Importing libraries
2024-09-04 20:51:54,765:INFO:Copying training dataset
2024-09-04 20:51:54,780:INFO:Defining folds
2024-09-04 20:51:54,780:INFO:Declaring metric variables
2024-09-04 20:51:54,783:INFO:Importing untrained model
2024-09-04 20:51:54,789:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:51:54,796:INFO:Starting cross validation
2024-09-04 20:51:54,797:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:51:55,854:INFO:Calculating mean and std
2024-09-04 20:51:55,856:INFO:Creating metrics dataframe
2024-09-04 20:51:55,859:INFO:Uploading results into container
2024-09-04 20:51:55,860:INFO:Uploading model into container now
2024-09-04 20:51:55,861:INFO:_master_model_container: 14
2024-09-04 20:51:55,862:INFO:_display_container: 2
2024-09-04 20:51:55,862:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:51:55,863:INFO:create_model() successfully completed......................................
2024-09-04 20:51:56,031:INFO:SubProcess create_model() end ==================================
2024-09-04 20:51:56,031:INFO:Creating metrics dataframe
2024-09-04 20:51:56,043:INFO:Initializing CatBoost Classifier
2024-09-04 20:51:56,043:INFO:Total runtime is 0.4662840286890665 minutes
2024-09-04 20:51:56,046:INFO:SubProcess create_model() called ==================================
2024-09-04 20:51:56,047:INFO:Initializing create_model()
2024-09-04 20:51:56,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:51:56,047:INFO:Checking exceptions
2024-09-04 20:51:56,047:INFO:Importing libraries
2024-09-04 20:51:56,047:INFO:Copying training dataset
2024-09-04 20:51:56,063:INFO:Defining folds
2024-09-04 20:51:56,063:INFO:Declaring metric variables
2024-09-04 20:51:56,069:INFO:Importing untrained model
2024-09-04 20:51:56,072:INFO:CatBoost Classifier Imported successfully
2024-09-04 20:51:56,080:INFO:Starting cross validation
2024-09-04 20:51:56,081:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:52:16,162:INFO:Calculating mean and std
2024-09-04 20:52:16,164:INFO:Creating metrics dataframe
2024-09-04 20:52:16,168:INFO:Uploading results into container
2024-09-04 20:52:16,169:INFO:Uploading model into container now
2024-09-04 20:52:16,169:INFO:_master_model_container: 15
2024-09-04 20:52:16,170:INFO:_display_container: 2
2024-09-04 20:52:16,170:INFO:<catboost.core.CatBoostClassifier object at 0x000001D469DEEDD0>
2024-09-04 20:52:16,171:INFO:create_model() successfully completed......................................
2024-09-04 20:52:16,421:INFO:SubProcess create_model() end ==================================
2024-09-04 20:52:16,422:INFO:Creating metrics dataframe
2024-09-04 20:52:16,453:INFO:Initializing Dummy Classifier
2024-09-04 20:52:16,453:INFO:Total runtime is 0.8064623912175495 minutes
2024-09-04 20:52:16,461:INFO:SubProcess create_model() called ==================================
2024-09-04 20:52:16,461:INFO:Initializing create_model()
2024-09-04 20:52:16,462:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46B20DE10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:52:16,462:INFO:Checking exceptions
2024-09-04 20:52:16,462:INFO:Importing libraries
2024-09-04 20:52:16,462:INFO:Copying training dataset
2024-09-04 20:52:16,498:INFO:Defining folds
2024-09-04 20:52:16,498:INFO:Declaring metric variables
2024-09-04 20:52:16,509:INFO:Importing untrained model
2024-09-04 20:52:16,518:INFO:Dummy Classifier Imported successfully
2024-09-04 20:52:16,536:INFO:Starting cross validation
2024-09-04 20:52:16,538:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:52:16,629:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 20:52:16,638:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 20:52:16,647:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 20:52:16,650:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 20:52:16,654:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 20:52:16,668:INFO:Calculating mean and std
2024-09-04 20:52:16,671:INFO:Creating metrics dataframe
2024-09-04 20:52:16,676:INFO:Uploading results into container
2024-09-04 20:52:16,679:INFO:Uploading model into container now
2024-09-04 20:52:16,680:INFO:_master_model_container: 16
2024-09-04 20:52:16,680:INFO:_display_container: 2
2024-09-04 20:52:16,681:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-04 20:52:16,681:INFO:create_model() successfully completed......................................
2024-09-04 20:52:16,871:INFO:SubProcess create_model() end ==================================
2024-09-04 20:52:16,871:INFO:Creating metrics dataframe
2024-09-04 20:52:16,904:INFO:Initializing create_model()
2024-09-04 20:52:16,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:52:16,906:INFO:Checking exceptions
2024-09-04 20:52:16,909:INFO:Importing libraries
2024-09-04 20:52:16,909:INFO:Copying training dataset
2024-09-04 20:52:16,933:INFO:Defining folds
2024-09-04 20:52:16,933:INFO:Declaring metric variables
2024-09-04 20:52:16,934:INFO:Importing untrained model
2024-09-04 20:52:16,934:INFO:Declaring custom model
2024-09-04 20:52:16,934:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:52:16,935:INFO:Cross validation set to False
2024-09-04 20:52:16,935:INFO:Fitting Model
2024-09-04 20:52:16,992:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:52:16,993:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:52:16,995:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.
2024-09-04 20:52:16,995:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:52:16,995:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:52:16,996:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:52:16,996:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:52:16,996:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:52:16,996:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:52:17,222:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:52:17,222:INFO:create_model() successfully completed......................................
2024-09-04 20:52:17,452:INFO:_master_model_container: 16
2024-09-04 20:52:17,453:INFO:_display_container: 2
2024-09-04 20:52:17,454:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:52:17,454:INFO:compare_models() successfully completed......................................
2024-09-04 20:52:17,457:INFO:Initializing create_model()
2024-09-04 20:52:17,457:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:52:17,457:INFO:Checking exceptions
2024-09-04 20:52:17,481:INFO:Importing libraries
2024-09-04 20:52:17,481:INFO:Copying training dataset
2024-09-04 20:52:17,521:INFO:Defining folds
2024-09-04 20:52:17,521:INFO:Declaring metric variables
2024-09-04 20:52:17,525:INFO:Importing untrained model
2024-09-04 20:52:17,525:INFO:Declaring custom model
2024-09-04 20:52:17,531:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:52:17,543:INFO:Starting cross validation
2024-09-04 20:52:17,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:52:19,330:INFO:Calculating mean and std
2024-09-04 20:52:19,332:INFO:Creating metrics dataframe
2024-09-04 20:52:19,339:INFO:Finalizing model
2024-09-04 20:52:19,386:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:52:19,386:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:52:19,389:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001126 seconds.
2024-09-04 20:52:19,389:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:52:19,389:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:52:19,389:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:52:19,389:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:52:19,390:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:52:19,390:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:52:19,625:INFO:Uploading results into container
2024-09-04 20:52:19,627:INFO:Uploading model into container now
2024-09-04 20:52:19,645:INFO:_master_model_container: 17
2024-09-04 20:52:19,645:INFO:_display_container: 3
2024-09-04 20:52:19,646:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:52:19,646:INFO:create_model() successfully completed......................................
2024-09-04 20:52:19,824:INFO:Initializing tune_model()
2024-09-04 20:52:19,824:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-04 20:52:19,824:INFO:Checking exceptions
2024-09-04 20:52:19,842:INFO:Copying training dataset
2024-09-04 20:52:19,853:INFO:Checking base model
2024-09-04 20:52:19,853:INFO:Base model : Light Gradient Boosting Machine
2024-09-04 20:52:19,857:INFO:Declaring metric variables
2024-09-04 20:52:19,865:INFO:Defining Hyperparameters
2024-09-04 20:52:20,011:INFO:Tuning with n_jobs=-1
2024-09-04 20:52:20,011:INFO:Initializing RandomizedSearchCV
2024-09-04 20:52:57,972:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-09-04 20:52:57,975:INFO:Hyperparameter search completed
2024-09-04 20:52:57,975:INFO:SubProcess create_model() called ==================================
2024-09-04 20:52:57,977:INFO:Initializing create_model()
2024-09-04 20:52:57,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D46444D810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-09-04 20:52:57,978:INFO:Checking exceptions
2024-09-04 20:52:57,978:INFO:Importing libraries
2024-09-04 20:52:57,978:INFO:Copying training dataset
2024-09-04 20:52:58,005:INFO:Defining folds
2024-09-04 20:52:58,005:INFO:Declaring metric variables
2024-09-04 20:52:58,011:INFO:Importing untrained model
2024-09-04 20:52:58,012:INFO:Declaring custom model
2024-09-04 20:52:58,019:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:52:58,029:INFO:Starting cross validation
2024-09-04 20:52:58,031:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:53:01,495:INFO:Calculating mean and std
2024-09-04 20:53:01,497:INFO:Creating metrics dataframe
2024-09-04 20:53:01,510:INFO:Finalizing model
2024-09-04 20:53:01,562:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 20:53:01,562:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 20:53:01,563:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 20:53:01,581:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:01,581:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 20:53:01,581:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 20:53:01,581:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 20:53:01,582:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:53:01,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.
2024-09-04 20:53:01,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:01,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:01,585:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:53:01,586:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:53:01,588:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:53:01,588:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:53:01,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:01,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,031:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,137:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,151:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,259:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:02,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:02,339:INFO:Uploading results into container
2024-09-04 20:53:02,341:INFO:Uploading model into container now
2024-09-04 20:53:02,343:INFO:_master_model_container: 18
2024-09-04 20:53:02,343:INFO:_display_container: 4
2024-09-04 20:53:02,345:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:02,345:INFO:create_model() successfully completed......................................
2024-09-04 20:53:02,548:INFO:SubProcess create_model() end ==================================
2024-09-04 20:53:02,548:INFO:choose_better activated
2024-09-04 20:53:02,553:INFO:SubProcess create_model() called ==================================
2024-09-04 20:53:02,554:INFO:Initializing create_model()
2024-09-04 20:53:02,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:53:02,554:INFO:Checking exceptions
2024-09-04 20:53:02,557:INFO:Importing libraries
2024-09-04 20:53:02,557:INFO:Copying training dataset
2024-09-04 20:53:02,577:INFO:Defining folds
2024-09-04 20:53:02,577:INFO:Declaring metric variables
2024-09-04 20:53:02,577:INFO:Importing untrained model
2024-09-04 20:53:02,578:INFO:Declaring custom model
2024-09-04 20:53:02,579:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:02,580:INFO:Starting cross validation
2024-09-04 20:53:02,581:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:53:04,525:INFO:Calculating mean and std
2024-09-04 20:53:04,526:INFO:Creating metrics dataframe
2024-09-04 20:53:04,528:INFO:Finalizing model
2024-09-04 20:53:04,581:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:04,581:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:53:04,584:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.
2024-09-04 20:53:04,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:04,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:04,585:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:53:04,585:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:53:04,585:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:53:04,586:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:53:04,780:INFO:Uploading results into container
2024-09-04 20:53:04,781:INFO:Uploading model into container now
2024-09-04 20:53:04,782:INFO:_master_model_container: 19
2024-09-04 20:53:04,782:INFO:_display_container: 5
2024-09-04 20:53:04,783:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:04,783:INFO:create_model() successfully completed......................................
2024-09-04 20:53:04,953:INFO:SubProcess create_model() end ==================================
2024-09-04 20:53:04,953:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-09-04 20:53:04,954:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-09-04 20:53:04,954:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-09-04 20:53:04,954:INFO:choose_better completed
2024-09-04 20:53:04,954:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-04 20:53:04,965:INFO:_master_model_container: 19
2024-09-04 20:53:04,965:INFO:_display_container: 4
2024-09-04 20:53:04,966:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:04,966:INFO:tune_model() successfully completed......................................
2024-09-04 20:53:05,144:INFO:Initializing finalize_model()
2024-09-04 20:53:05,144:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 20:53:05,144:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:05,153:INFO:Initializing create_model()
2024-09-04 20:53:05,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:53:05,154:INFO:Checking exceptions
2024-09-04 20:53:05,156:INFO:Importing libraries
2024-09-04 20:53:05,156:INFO:Copying training dataset
2024-09-04 20:53:05,156:INFO:Defining folds
2024-09-04 20:53:05,156:INFO:Declaring metric variables
2024-09-04 20:53:05,157:INFO:Importing untrained model
2024-09-04 20:53:05,157:INFO:Declaring custom model
2024-09-04 20:53:05,157:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:05,158:INFO:Cross validation set to False
2024-09-04 20:53:05,158:INFO:Fitting Model
2024-09-04 20:53:05,206:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:05,207:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-09-04 20:53:05,210:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.
2024-09-04 20:53:05,210:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:05,210:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:05,210:INFO:[LightGBM] [Info] Total Bins 681
2024-09-04 20:53:05,211:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-09-04 20:53:05,211:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-09-04 20:53:05,211:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-09-04 20:53:05,378:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 20:53:05,378:INFO:create_model() successfully completed......................................
2024-09-04 20:53:05,544:INFO:_master_model_container: 19
2024-09-04 20:53:05,544:INFO:_display_container: 4
2024-09-04 20:53:05,549:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 20:53:05,549:INFO:finalize_model() successfully completed......................................
2024-09-04 20:53:05,699:INFO:Initializing predict_model()
2024-09-04 20:53:05,699:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D46B12E340>)
2024-09-04 20:53:05,699:INFO:Checking exceptions
2024-09-04 20:53:05,699:INFO:Preloading libraries
2024-09-04 20:53:05,702:INFO:Set up data.
2024-09-04 20:53:05,708:INFO:Set up index.
2024-09-04 20:53:05,951:INFO:Initializing evaluate_model()
2024-09-04 20:53:05,952:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-04 20:53:05,976:INFO:Initializing plot_model()
2024-09-04 20:53:05,976:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D4633A99D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-04 20:53:05,976:INFO:Checking exceptions
2024-09-04 20:53:05,983:INFO:Preloading libraries
2024-09-04 20:53:05,990:INFO:Copying training dataset
2024-09-04 20:53:05,990:INFO:Plot type: pipeline
2024-09-04 20:53:06,225:INFO:Visual Rendered Successfully
2024-09-04 20:53:06,367:INFO:plot_model() successfully completed......................................
2024-09-04 20:53:06,399:INFO:PyCaret ClassificationExperiment
2024-09-04 20:53:06,399:INFO:Logging name: clf-default-name
2024-09-04 20:53:06,399:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 20:53:06,399:INFO:version 3.3.2
2024-09-04 20:53:06,399:INFO:Initializing setup()
2024-09-04 20:53:06,399:INFO:self.USI: 160b
2024-09-04 20:53:06,399:INFO:self._variable_keys: {'pipeline', '_ml_usecase', 'y_train', 'y', 'y_test', 'exp_id', 'seed', 'X', 'exp_name_log', 'fold_generator', 'gpu_n_jobs_param', 'fold_shuffle_param', 'fold_groups_param', 'data', 'memory', 'n_jobs_param', 'is_multiclass', '_available_plots', 'idx', 'log_plots_param', 'X_train', 'X_test', 'html_param', 'target_param', 'logging_param', 'USI', 'gpu_param', 'fix_imbalance'}
2024-09-04 20:53:06,399:INFO:Checking environment
2024-09-04 20:53:06,399:INFO:python_version: 3.11.9
2024-09-04 20:53:06,400:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-04 20:53:06,400:INFO:machine: AMD64
2024-09-04 20:53:06,400:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-04 20:53:06,407:INFO:Memory: svmem(total=16867028992, available=765116416, percent=95.5, used=16101912576, free=765116416)
2024-09-04 20:53:06,407:INFO:Physical Core: 6
2024-09-04 20:53:06,407:INFO:Logical Core: 12
2024-09-04 20:53:06,407:INFO:Checking libraries
2024-09-04 20:53:06,407:INFO:System:
2024-09-04 20:53:06,407:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-04 20:53:06,407:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-09-04 20:53:06,407:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-04 20:53:06,407:INFO:PyCaret required dependencies:
2024-09-04 20:53:06,407:INFO:                 pip: 23.2.1
2024-09-04 20:53:06,407:INFO:          setuptools: 67.8.0
2024-09-04 20:53:06,407:INFO:             pycaret: 3.3.2
2024-09-04 20:53:06,407:INFO:             IPython: 8.14.0
2024-09-04 20:53:06,408:INFO:          ipywidgets: 8.1.5
2024-09-04 20:53:06,408:INFO:                tqdm: 4.66.5
2024-09-04 20:53:06,408:INFO:               numpy: 1.24.3
2024-09-04 20:53:06,408:INFO:              pandas: 2.0.3
2024-09-04 20:53:06,408:INFO:              jinja2: 3.1.4
2024-09-04 20:53:06,408:INFO:               scipy: 1.10.1
2024-09-04 20:53:06,408:INFO:              joblib: 1.2.0
2024-09-04 20:53:06,408:INFO:             sklearn: 1.4.2
2024-09-04 20:53:06,408:INFO:                pyod: 2.0.1
2024-09-04 20:53:06,408:INFO:            imblearn: 0.12.3
2024-09-04 20:53:06,408:INFO:   category_encoders: 2.6.3
2024-09-04 20:53:06,408:INFO:            lightgbm: 4.5.0
2024-09-04 20:53:06,408:INFO:               numba: 0.60.0
2024-09-04 20:53:06,408:INFO:            requests: 2.32.3
2024-09-04 20:53:06,408:INFO:          matplotlib: 3.7.1
2024-09-04 20:53:06,408:INFO:          scikitplot: 0.3.7
2024-09-04 20:53:06,408:INFO:         yellowbrick: 1.5
2024-09-04 20:53:06,408:INFO:              plotly: 5.16.1
2024-09-04 20:53:06,409:INFO:    plotly-resampler: Not installed
2024-09-04 20:53:06,409:INFO:             kaleido: 0.2.1
2024-09-04 20:53:06,409:INFO:           schemdraw: 0.15
2024-09-04 20:53:06,409:INFO:         statsmodels: 0.14.2
2024-09-04 20:53:06,409:INFO:              sktime: 0.26.0
2024-09-04 20:53:06,409:INFO:               tbats: 1.1.3
2024-09-04 20:53:06,409:INFO:            pmdarima: 2.0.4
2024-09-04 20:53:06,409:INFO:              psutil: 5.9.0
2024-09-04 20:53:06,409:INFO:          markupsafe: 2.1.3
2024-09-04 20:53:06,409:INFO:             pickle5: Not installed
2024-09-04 20:53:06,409:INFO:         cloudpickle: 3.0.0
2024-09-04 20:53:06,409:INFO:         deprecation: 2.1.0
2024-09-04 20:53:06,409:INFO:              xxhash: 3.5.0
2024-09-04 20:53:06,409:INFO:           wurlitzer: Not installed
2024-09-04 20:53:06,409:INFO:PyCaret optional dependencies:
2024-09-04 20:53:06,409:INFO:                shap: Not installed
2024-09-04 20:53:06,409:INFO:           interpret: Not installed
2024-09-04 20:53:06,409:INFO:                umap: Not installed
2024-09-04 20:53:06,409:INFO:     ydata_profiling: Not installed
2024-09-04 20:53:06,409:INFO:  explainerdashboard: Not installed
2024-09-04 20:53:06,409:INFO:             autoviz: Not installed
2024-09-04 20:53:06,410:INFO:           fairlearn: Not installed
2024-09-04 20:53:06,410:INFO:          deepchecks: Not installed
2024-09-04 20:53:06,410:INFO:             xgboost: 2.0.2
2024-09-04 20:53:06,410:INFO:            catboost: 1.2.5
2024-09-04 20:53:06,410:INFO:              kmodes: Not installed
2024-09-04 20:53:06,410:INFO:             mlxtend: Not installed
2024-09-04 20:53:06,410:INFO:       statsforecast: Not installed
2024-09-04 20:53:06,410:INFO:        tune_sklearn: Not installed
2024-09-04 20:53:06,410:INFO:                 ray: Not installed
2024-09-04 20:53:06,410:INFO:            hyperopt: Not installed
2024-09-04 20:53:06,410:INFO:              optuna: 4.0.0
2024-09-04 20:53:06,410:INFO:               skopt: Not installed
2024-09-04 20:53:06,410:INFO:              mlflow: Not installed
2024-09-04 20:53:06,410:INFO:              gradio: 4.41.0
2024-09-04 20:53:06,410:INFO:             fastapi: 0.112.1
2024-09-04 20:53:06,410:INFO:             uvicorn: 0.30.6
2024-09-04 20:53:06,410:INFO:              m2cgen: Not installed
2024-09-04 20:53:06,410:INFO:           evidently: Not installed
2024-09-04 20:53:06,410:INFO:               fugue: Not installed
2024-09-04 20:53:06,410:INFO:           streamlit: 1.38.0
2024-09-04 20:53:06,410:INFO:             prophet: Not installed
2024-09-04 20:53:06,410:INFO:None
2024-09-04 20:53:06,410:INFO:Set up data.
2024-09-04 20:53:06,424:INFO:Set up folding strategy.
2024-09-04 20:53:06,424:INFO:Set up train/test split.
2024-09-04 20:53:06,438:INFO:Set up index.
2024-09-04 20:53:06,439:INFO:Assigning column types.
2024-09-04 20:53:06,450:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 20:53:06,494:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,495:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,520:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,523:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,570:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,571:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,601:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,604:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,604:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 20:53:06,660:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,697:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,701:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,757:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 20:53:06,790:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,792:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,793:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 20:53:06,878:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,881:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,970:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:06,973:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:06,976:INFO:Preparing preprocessing pipeline...
2024-09-04 20:53:06,980:INFO:Set up simple imputation.
2024-09-04 20:53:06,981:INFO:Set up column name cleaning.
2024-09-04 20:53:07,032:INFO:Finished creating preprocessing pipeline.
2024-09-04 20:53:07,037:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-04 20:53:07,037:INFO:Creating final display dataframe.
2024-09-04 20:53:07,222:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            income
2                   Target type            Binary
3           Original data shape       (26035, 16)
4        Transformed data shape       (26035, 16)
5   Transformed train set shape       (18224, 16)
6    Transformed test set shape        (7811, 16)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              160b
2024-09-04 20:53:07,314:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:07,317:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:07,397:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 20:53:07,399:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 20:53:07,401:INFO:setup() successfully completed in 1.0s...............
2024-09-04 20:53:07,401:INFO:Initializing create_model()
2024-09-04 20:53:07,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:53:07,401:INFO:Checking exceptions
2024-09-04 20:53:07,415:INFO:Importing libraries
2024-09-04 20:53:07,416:INFO:Copying training dataset
2024-09-04 20:53:07,431:INFO:Defining folds
2024-09-04 20:53:07,431:INFO:Declaring metric variables
2024-09-04 20:53:07,435:INFO:Importing untrained model
2024-09-04 20:53:07,439:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:07,449:INFO:Starting cross validation
2024-09-04 20:53:07,450:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:53:10,275:INFO:Calculating mean and std
2024-09-04 20:53:10,277:INFO:Creating metrics dataframe
2024-09-04 20:53:10,286:INFO:Finalizing model
2024-09-04 20:53:10,337:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:10,337:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:53:10,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000852 seconds.
2024-09-04 20:53:10,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:10,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:10,340:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:53:10,340:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:53:10,340:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:53:10,341:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:53:10,613:INFO:Uploading results into container
2024-09-04 20:53:10,615:INFO:Uploading model into container now
2024-09-04 20:53:10,634:INFO:_master_model_container: 1
2024-09-04 20:53:10,635:INFO:_display_container: 2
2024-09-04 20:53:10,636:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:10,636:INFO:create_model() successfully completed......................................
2024-09-04 20:53:10,820:INFO:Initializing tune_model()
2024-09-04 20:53:10,820:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-04 20:53:10,820:INFO:Checking exceptions
2024-09-04 20:53:10,841:INFO:Copying training dataset
2024-09-04 20:53:10,852:INFO:Checking base model
2024-09-04 20:53:10,852:INFO:Base model : Light Gradient Boosting Machine
2024-09-04 20:53:10,857:INFO:Declaring metric variables
2024-09-04 20:53:10,862:INFO:Defining Hyperparameters
2024-09-04 20:53:11,043:INFO:Tuning with n_jobs=-1
2024-09-04 20:53:11,043:INFO:Initializing RandomizedSearchCV
2024-09-04 20:53:40,264:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-09-04 20:53:40,266:INFO:Hyperparameter search completed
2024-09-04 20:53:40,266:INFO:SubProcess create_model() called ==================================
2024-09-04 20:53:40,267:INFO:Initializing create_model()
2024-09-04 20:53:40,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D460BE4ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-09-04 20:53:40,268:INFO:Checking exceptions
2024-09-04 20:53:40,268:INFO:Importing libraries
2024-09-04 20:53:40,268:INFO:Copying training dataset
2024-09-04 20:53:40,299:INFO:Defining folds
2024-09-04 20:53:40,299:INFO:Declaring metric variables
2024-09-04 20:53:40,305:INFO:Importing untrained model
2024-09-04 20:53:40,305:INFO:Declaring custom model
2024-09-04 20:53:40,312:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:40,322:INFO:Starting cross validation
2024-09-04 20:53:40,324:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:53:43,357:INFO:Calculating mean and std
2024-09-04 20:53:43,359:INFO:Creating metrics dataframe
2024-09-04 20:53:43,368:INFO:Finalizing model
2024-09-04 20:53:43,402:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 20:53:43,403:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 20:53:43,403:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 20:53:43,415:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:43,415:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 20:53:43,415:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 20:53:43,415:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 20:53:43,415:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:53:43,418:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.
2024-09-04 20:53:43,418:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:43,418:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:43,418:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:53:43,418:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:53:43,419:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:53:43,420:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:53:43,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:43,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:43,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:43,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:43,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,984:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:43,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:43,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:44,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:44,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,052:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:44,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 20:53:44,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 20:53:44,164:INFO:Uploading results into container
2024-09-04 20:53:44,173:INFO:Uploading model into container now
2024-09-04 20:53:44,174:INFO:_master_model_container: 2
2024-09-04 20:53:44,174:INFO:_display_container: 3
2024-09-04 20:53:44,177:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:44,177:INFO:create_model() successfully completed......................................
2024-09-04 20:53:44,446:INFO:SubProcess create_model() end ==================================
2024-09-04 20:53:44,446:INFO:choose_better activated
2024-09-04 20:53:44,450:INFO:SubProcess create_model() called ==================================
2024-09-04 20:53:44,452:INFO:Initializing create_model()
2024-09-04 20:53:44,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:53:44,452:INFO:Checking exceptions
2024-09-04 20:53:44,454:INFO:Importing libraries
2024-09-04 20:53:44,455:INFO:Copying training dataset
2024-09-04 20:53:44,472:INFO:Defining folds
2024-09-04 20:53:44,472:INFO:Declaring metric variables
2024-09-04 20:53:44,472:INFO:Importing untrained model
2024-09-04 20:53:44,473:INFO:Declaring custom model
2024-09-04 20:53:44,473:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:44,474:INFO:Starting cross validation
2024-09-04 20:53:44,475:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 20:53:46,220:INFO:Calculating mean and std
2024-09-04 20:53:46,221:INFO:Creating metrics dataframe
2024-09-04 20:53:46,224:INFO:Finalizing model
2024-09-04 20:53:46,290:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:46,290:INFO:[LightGBM] [Info] Number of positive: 4390, number of negative: 13834
2024-09-04 20:53:46,293:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.
2024-09-04 20:53:46,293:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:46,293:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:46,294:INFO:[LightGBM] [Info] Total Bins 665
2024-09-04 20:53:46,294:INFO:[LightGBM] [Info] Number of data points in the train set: 18224, number of used features: 15
2024-09-04 20:53:46,296:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240891 -> initscore=-1.147800
2024-09-04 20:53:46,297:INFO:[LightGBM] [Info] Start training from score -1.147800
2024-09-04 20:53:46,646:INFO:Uploading results into container
2024-09-04 20:53:46,647:INFO:Uploading model into container now
2024-09-04 20:53:46,648:INFO:_master_model_container: 3
2024-09-04 20:53:46,648:INFO:_display_container: 4
2024-09-04 20:53:46,649:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:46,649:INFO:create_model() successfully completed......................................
2024-09-04 20:53:46,841:INFO:SubProcess create_model() end ==================================
2024-09-04 20:53:46,842:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6982
2024-09-04 20:53:46,843:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6971
2024-09-04 20:53:46,843:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-09-04 20:53:46,843:INFO:choose_better completed
2024-09-04 20:53:46,844:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-04 20:53:46,855:INFO:_master_model_container: 3
2024-09-04 20:53:46,855:INFO:_display_container: 3
2024-09-04 20:53:46,856:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:46,856:INFO:tune_model() successfully completed......................................
2024-09-04 20:53:47,028:INFO:Initializing finalize_model()
2024-09-04 20:53:47,029:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 20:53:47,029:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 20:53:47,037:INFO:Initializing create_model()
2024-09-04 20:53:47,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 20:53:47,037:INFO:Checking exceptions
2024-09-04 20:53:47,039:INFO:Importing libraries
2024-09-04 20:53:47,039:INFO:Copying training dataset
2024-09-04 20:53:47,040:INFO:Defining folds
2024-09-04 20:53:47,040:INFO:Declaring metric variables
2024-09-04 20:53:47,040:INFO:Importing untrained model
2024-09-04 20:53:47,040:INFO:Declaring custom model
2024-09-04 20:53:47,040:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 20:53:47,041:INFO:Cross validation set to False
2024-09-04 20:53:47,041:INFO:Fitting Model
2024-09-04 20:53:47,081:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 20:53:47,082:INFO:[LightGBM] [Info] Number of positive: 6272, number of negative: 19763
2024-09-04 20:53:47,085:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.
2024-09-04 20:53:47,085:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 20:53:47,085:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 20:53:47,085:INFO:[LightGBM] [Info] Total Bins 681
2024-09-04 20:53:47,085:INFO:[LightGBM] [Info] Number of data points in the train set: 26035, number of used features: 15
2024-09-04 20:53:47,085:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240906 -> initscore=-1.147716
2024-09-04 20:53:47,085:INFO:[LightGBM] [Info] Start training from score -1.147716
2024-09-04 20:53:47,257:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 20:53:47,258:INFO:create_model() successfully completed......................................
2024-09-04 20:53:47,445:INFO:_master_model_container: 3
2024-09-04 20:53:47,445:INFO:_display_container: 3
2024-09-04 20:53:47,455:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 20:53:47,455:INFO:finalize_model() successfully completed......................................
2024-09-04 20:53:47,638:INFO:Initializing evaluate_model()
2024-09-04 20:53:47,639:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-04 20:53:47,656:INFO:Initializing plot_model()
2024-09-04 20:53:47,656:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-04 20:53:47,656:INFO:Checking exceptions
2024-09-04 20:53:47,663:INFO:Preloading libraries
2024-09-04 20:53:47,671:INFO:Copying training dataset
2024-09-04 20:53:47,671:INFO:Plot type: pipeline
2024-09-04 20:53:47,811:INFO:Visual Rendered Successfully
2024-09-04 20:53:47,966:INFO:plot_model() successfully completed......................................
2024-09-04 20:53:48,892:INFO:Initializing predict_model()
2024-09-04 20:53:48,892:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D460BE19D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D46B724720>)
2024-09-04 20:53:48,893:INFO:Checking exceptions
2024-09-04 20:53:48,893:INFO:Preloading libraries
2024-09-04 20:53:48,897:INFO:Set up data.
2024-09-04 20:53:48,919:INFO:Set up index.
2024-09-04 21:04:12,699:INFO:PyCaret ClassificationExperiment
2024-09-04 21:04:12,699:INFO:Logging name: clf-default-name
2024-09-04 21:04:12,699:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 21:04:12,699:INFO:version 3.3.2
2024-09-04 21:04:12,699:INFO:Initializing setup()
2024-09-04 21:04:12,699:INFO:self.USI: b06f
2024-09-04 21:04:12,699:INFO:self._variable_keys: {'pipeline', '_ml_usecase', 'y_train', 'y', 'y_test', 'exp_id', 'seed', 'X', 'exp_name_log', 'fold_generator', 'gpu_n_jobs_param', 'fold_shuffle_param', 'fold_groups_param', 'data', 'memory', 'n_jobs_param', 'is_multiclass', '_available_plots', 'idx', 'log_plots_param', 'X_train', 'X_test', 'html_param', 'target_param', 'logging_param', 'USI', 'gpu_param', 'fix_imbalance'}
2024-09-04 21:04:12,699:INFO:Checking environment
2024-09-04 21:04:12,699:INFO:python_version: 3.11.9
2024-09-04 21:04:12,699:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-04 21:04:12,700:INFO:machine: AMD64
2024-09-04 21:04:12,700:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-04 21:04:12,705:INFO:Memory: svmem(total=16867028992, available=2676125696, percent=84.1, used=14190903296, free=2676125696)
2024-09-04 21:04:12,706:INFO:Physical Core: 6
2024-09-04 21:04:12,706:INFO:Logical Core: 12
2024-09-04 21:04:12,706:INFO:Checking libraries
2024-09-04 21:04:12,706:INFO:System:
2024-09-04 21:04:12,706:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-04 21:04:12,706:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-09-04 21:04:12,706:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-04 21:04:12,706:INFO:PyCaret required dependencies:
2024-09-04 21:04:12,706:INFO:                 pip: 23.2.1
2024-09-04 21:04:12,706:INFO:          setuptools: 67.8.0
2024-09-04 21:04:12,706:INFO:             pycaret: 3.3.2
2024-09-04 21:04:12,706:INFO:             IPython: 8.14.0
2024-09-04 21:04:12,706:INFO:          ipywidgets: 8.1.5
2024-09-04 21:04:12,706:INFO:                tqdm: 4.66.5
2024-09-04 21:04:12,707:INFO:               numpy: 1.24.3
2024-09-04 21:04:12,707:INFO:              pandas: 2.0.3
2024-09-04 21:04:12,707:INFO:              jinja2: 3.1.4
2024-09-04 21:04:12,707:INFO:               scipy: 1.10.1
2024-09-04 21:04:12,707:INFO:              joblib: 1.2.0
2024-09-04 21:04:12,707:INFO:             sklearn: 1.4.2
2024-09-04 21:04:12,707:INFO:                pyod: 2.0.1
2024-09-04 21:04:12,707:INFO:            imblearn: 0.12.3
2024-09-04 21:04:12,707:INFO:   category_encoders: 2.6.3
2024-09-04 21:04:12,707:INFO:            lightgbm: 4.5.0
2024-09-04 21:04:12,707:INFO:               numba: 0.60.0
2024-09-04 21:04:12,707:INFO:            requests: 2.32.3
2024-09-04 21:04:12,707:INFO:          matplotlib: 3.7.1
2024-09-04 21:04:12,707:INFO:          scikitplot: 0.3.7
2024-09-04 21:04:12,707:INFO:         yellowbrick: 1.5
2024-09-04 21:04:12,707:INFO:              plotly: 5.16.1
2024-09-04 21:04:12,707:INFO:    plotly-resampler: Not installed
2024-09-04 21:04:12,707:INFO:             kaleido: 0.2.1
2024-09-04 21:04:12,707:INFO:           schemdraw: 0.15
2024-09-04 21:04:12,707:INFO:         statsmodels: 0.14.2
2024-09-04 21:04:12,707:INFO:              sktime: 0.26.0
2024-09-04 21:04:12,707:INFO:               tbats: 1.1.3
2024-09-04 21:04:12,707:INFO:            pmdarima: 2.0.4
2024-09-04 21:04:12,707:INFO:              psutil: 5.9.0
2024-09-04 21:04:12,707:INFO:          markupsafe: 2.1.3
2024-09-04 21:04:12,707:INFO:             pickle5: Not installed
2024-09-04 21:04:12,707:INFO:         cloudpickle: 3.0.0
2024-09-04 21:04:12,707:INFO:         deprecation: 2.1.0
2024-09-04 21:04:12,707:INFO:              xxhash: 3.5.0
2024-09-04 21:04:12,707:INFO:           wurlitzer: Not installed
2024-09-04 21:04:12,708:INFO:PyCaret optional dependencies:
2024-09-04 21:04:12,708:INFO:                shap: Not installed
2024-09-04 21:04:12,708:INFO:           interpret: Not installed
2024-09-04 21:04:12,708:INFO:                umap: Not installed
2024-09-04 21:04:12,708:INFO:     ydata_profiling: Not installed
2024-09-04 21:04:12,708:INFO:  explainerdashboard: Not installed
2024-09-04 21:04:12,708:INFO:             autoviz: Not installed
2024-09-04 21:04:12,708:INFO:           fairlearn: Not installed
2024-09-04 21:04:12,708:INFO:          deepchecks: Not installed
2024-09-04 21:04:12,708:INFO:             xgboost: 2.0.2
2024-09-04 21:04:12,708:INFO:            catboost: 1.2.5
2024-09-04 21:04:12,708:INFO:              kmodes: Not installed
2024-09-04 21:04:12,708:INFO:             mlxtend: Not installed
2024-09-04 21:04:12,708:INFO:       statsforecast: Not installed
2024-09-04 21:04:12,708:INFO:        tune_sklearn: Not installed
2024-09-04 21:04:12,708:INFO:                 ray: Not installed
2024-09-04 21:04:12,708:INFO:            hyperopt: Not installed
2024-09-04 21:04:12,709:INFO:              optuna: 4.0.0
2024-09-04 21:04:12,709:INFO:               skopt: Not installed
2024-09-04 21:04:12,709:INFO:              mlflow: Not installed
2024-09-04 21:04:12,709:INFO:              gradio: 4.41.0
2024-09-04 21:04:12,709:INFO:             fastapi: 0.112.1
2024-09-04 21:04:12,709:INFO:             uvicorn: 0.30.6
2024-09-04 21:04:12,709:INFO:              m2cgen: Not installed
2024-09-04 21:04:12,709:INFO:           evidently: Not installed
2024-09-04 21:04:12,709:INFO:               fugue: Not installed
2024-09-04 21:04:12,709:INFO:           streamlit: 1.38.0
2024-09-04 21:04:12,709:INFO:             prophet: Not installed
2024-09-04 21:04:12,709:INFO:None
2024-09-04 21:04:12,709:INFO:Set up data.
2024-09-04 21:04:12,722:INFO:Set up folding strategy.
2024-09-04 21:04:12,723:INFO:Set up train/test split.
2024-09-04 21:04:12,738:INFO:Set up index.
2024-09-04 21:04:12,739:INFO:Assigning column types.
2024-09-04 21:04:12,749:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 21:04:12,789:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 21:04:12,790:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:04:12,819:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:12,822:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:12,868:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 21:04:12,869:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:04:12,901:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:12,904:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:12,905:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 21:04:12,948:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:04:12,973:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:12,975:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:13,021:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:04:13,079:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:13,081:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:13,082:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 21:04:13,179:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:13,185:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:13,270:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:13,275:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:13,277:INFO:Preparing preprocessing pipeline...
2024-09-04 21:04:13,279:INFO:Set up simple imputation.
2024-09-04 21:04:13,280:INFO:Set up imbalanced handling.
2024-09-04 21:04:13,281:INFO:Set up column name cleaning.
2024-09-04 21:04:14,531:INFO:Finished creating preprocessing pipeline.
2024-09-04 21:04:14,542:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak',
                                             'no_income']...
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTETomek(n_jobs=None,
                                                                                   random_state=2021,
                                                                                   sampling_strategy='auto',
                                                                                   smote=None,
                                                                                   tomek=None)))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-04 21:04:14,543:INFO:Creating final display dataframe.
2024-09-04 21:04:15,697:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3           Original data shape   
4        Transformed data shape   
5   Transformed train set shape   
6    Transformed test set shape   
7              Numeric features   
8                    Preprocess   
9               Imputation type   
10           Numeric imputation   
11       Categorical imputation   
12                Fix imbalance   
13         Fix imbalance method   
14               Fold Generator   
15                  Fold Number   
16                     CPU Jobs   
17                      Use GPU   
18               Log Experiment   
19              Experiment Name   
20                          USI   

                                                Value  
0                                                6789  
1                                              Target  
2                                              Binary  
3                                         (28022, 17)  
4                                         (26911, 17)  
5                                         (18504, 17)  
6                                          (8407, 17)  
7                                                  16  
8                                                True  
9                                              simple  
10                                               mean  
11                                               mode  
12                                               True  
13  SMOTETomek(n_jobs=None, random_state=2021, sam...  
14                                    StratifiedKFold  
15                                                 10  
16                                                 -1  
17                                              False  
18                                              False  
19                                   clf-default-name  
20                                               b06f  
2024-09-04 21:04:15,808:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:15,815:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:15,932:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:04:15,937:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:04:15,940:INFO:setup() successfully completed in 3.24s...............
2024-09-04 21:04:55,553:INFO:Initializing create_model()
2024-09-04 21:04:55,553:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=lightgbm, fold=2, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:04:55,554:INFO:Checking exceptions
2024-09-04 21:04:55,574:INFO:Importing libraries
2024-09-04 21:04:55,574:INFO:Copying training dataset
2024-09-04 21:04:55,605:INFO:Defining folds
2024-09-04 21:04:55,606:INFO:Declaring metric variables
2024-09-04 21:04:55,615:INFO:Importing untrained model
2024-09-04 21:04:55,622:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:04:55,642:INFO:Starting cross validation
2024-09-04 21:04:55,646:INFO:Cross validating with StratifiedKFold(n_splits=2, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:05:03,520:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
1 fits failed out of a total of 2.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 8604 elements, new values have 0 elements

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-09-04 21:05:03,521:INFO:Calculating mean and std
2024-09-04 21:05:03,523:INFO:Creating metrics dataframe
2024-09-04 21:05:03,533:INFO:Finalizing model
2024-09-04 21:05:04,188:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:05:04,189:INFO:[LightGBM] [Info] Number of positive: 9252, number of negative: 9252
2024-09-04 21:05:04,193:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001443 seconds.
2024-09-04 21:05:04,193:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:05:04,193:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:05:04,193:INFO:[LightGBM] [Info] Total Bins 1030
2024-09-04 21:05:04,194:INFO:[LightGBM] [Info] Number of data points in the train set: 18504, number of used features: 15
2024-09-04 21:05:04,194:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:05:04,413:INFO:Uploading results into container
2024-09-04 21:05:04,414:INFO:Uploading model into container now
2024-09-04 21:05:04,426:INFO:_master_model_container: 1
2024-09-04 21:05:04,427:INFO:_display_container: 2
2024-09-04 21:05:04,428:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:05:04,428:INFO:create_model() successfully completed......................................
2024-09-04 21:05:17,990:INFO:Initializing create_model()
2024-09-04 21:05:17,990:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:05:17,990:INFO:Checking exceptions
2024-09-04 21:05:18,012:INFO:Importing libraries
2024-09-04 21:05:18,013:INFO:Copying training dataset
2024-09-04 21:05:18,041:INFO:Defining folds
2024-09-04 21:05:18,041:INFO:Declaring metric variables
2024-09-04 21:05:18,047:INFO:Importing untrained model
2024-09-04 21:05:18,053:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:05:18,073:INFO:Starting cross validation
2024-09-04 21:05:18,075:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:05:29,390:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
3 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 16470 elements, new values have 0 elements

--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 16432 elements, new values have 0 elements

--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 16402 elements, new values have 0 elements

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-09-04 21:05:29,391:INFO:Calculating mean and std
2024-09-04 21:05:29,394:INFO:Creating metrics dataframe
2024-09-04 21:05:29,402:INFO:Finalizing model
2024-09-04 21:05:30,162:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:05:30,162:INFO:[LightGBM] [Info] Number of positive: 9252, number of negative: 9252
2024-09-04 21:05:30,166:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001332 seconds.
2024-09-04 21:05:30,166:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:05:30,166:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:05:30,166:INFO:[LightGBM] [Info] Total Bins 1030
2024-09-04 21:05:30,166:INFO:[LightGBM] [Info] Number of data points in the train set: 18504, number of used features: 15
2024-09-04 21:05:30,167:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:05:30,337:INFO:Uploading results into container
2024-09-04 21:05:30,338:INFO:Uploading model into container now
2024-09-04 21:05:30,354:INFO:_master_model_container: 2
2024-09-04 21:05:30,354:INFO:_display_container: 3
2024-09-04 21:05:30,355:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:05:30,355:INFO:create_model() successfully completed......................................
2024-09-04 21:06:07,946:INFO:Initializing create_model()
2024-09-04 21:06:07,946:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=lightgbm, fold=15, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:06:07,947:INFO:Checking exceptions
2024-09-04 21:06:07,969:INFO:Importing libraries
2024-09-04 21:06:07,969:INFO:Copying training dataset
2024-09-04 21:06:07,994:INFO:Defining folds
2024-09-04 21:06:07,994:INFO:Declaring metric variables
2024-09-04 21:06:07,998:INFO:Importing untrained model
2024-09-04 21:06:08,003:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:06:08,015:INFO:Starting cross validation
2024-09-04 21:06:08,017:INFO:Cross validating with StratifiedKFold(n_splits=15, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:06:17,225:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
3 fits failed out of a total of 15.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 17122 elements, new values have 0 elements

--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 17098 elements, new values have 0 elements

--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 273, in fit
    X, y, _ = self._fit(X, y, routed_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 256, in _fit
    X, y = self._memory_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\pipeline.py", line 80, in _transform_one
    output = transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 255, in transform
    output = self.transformer.transform(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 583, in transform
    X_new.index = new_index
    ^^^^^^^^^^^
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 6002, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\generic.py", line 730, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\managers.py", line 225, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 17128 elements, new values have 0 elements

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-09-04 21:06:17,225:INFO:Calculating mean and std
2024-09-04 21:06:17,227:INFO:Creating metrics dataframe
2024-09-04 21:06:17,236:INFO:Finalizing model
2024-09-04 21:06:17,882:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:06:17,882:INFO:[LightGBM] [Info] Number of positive: 9252, number of negative: 9252
2024-09-04 21:06:17,884:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.
2024-09-04 21:06:17,884:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:06:17,884:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:06:17,884:INFO:[LightGBM] [Info] Total Bins 1030
2024-09-04 21:06:17,885:INFO:[LightGBM] [Info] Number of data points in the train set: 18504, number of used features: 15
2024-09-04 21:06:17,885:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:06:18,078:INFO:Uploading results into container
2024-09-04 21:06:18,080:INFO:Uploading model into container now
2024-09-04 21:06:18,097:INFO:_master_model_container: 3
2024-09-04 21:06:18,097:INFO:_display_container: 4
2024-09-04 21:06:18,098:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:06:18,098:INFO:create_model() successfully completed......................................
2024-09-04 21:08:18,068:INFO:Initializing finalize_model()
2024-09-04 21:08:18,069:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 21:08:18,070:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:08:18,083:INFO:Initializing create_model()
2024-09-04 21:08:18,083:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:08:18,084:INFO:Checking exceptions
2024-09-04 21:08:18,087:INFO:Importing libraries
2024-09-04 21:08:18,087:INFO:Copying training dataset
2024-09-04 21:08:18,090:INFO:Defining folds
2024-09-04 21:08:18,090:INFO:Declaring metric variables
2024-09-04 21:08:18,090:INFO:Importing untrained model
2024-09-04 21:08:18,090:INFO:Declaring custom model
2024-09-04 21:08:18,091:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:08:18,093:INFO:Cross validation set to False
2024-09-04 21:08:18,093:INFO:Fitting Model
2024-09-04 21:08:44,786:INFO:Initializing finalize_model()
2024-09-04 21:08:44,787:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 21:08:44,788:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:08:44,801:INFO:Initializing create_model()
2024-09-04 21:08:44,802:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:08:44,803:INFO:Checking exceptions
2024-09-04 21:08:44,806:INFO:Importing libraries
2024-09-04 21:08:44,806:INFO:Copying training dataset
2024-09-04 21:08:44,808:INFO:Defining folds
2024-09-04 21:08:44,808:INFO:Declaring metric variables
2024-09-04 21:08:44,808:INFO:Importing untrained model
2024-09-04 21:08:44,809:INFO:Declaring custom model
2024-09-04 21:08:44,811:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:08:44,813:INFO:Cross validation set to False
2024-09-04 21:08:44,813:INFO:Fitting Model
2024-09-04 21:10:20,072:INFO:Initializing finalize_model()
2024-09-04 21:10:20,073:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 21:10:20,073:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:10:20,086:INFO:Initializing create_model()
2024-09-04 21:10:20,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:10:20,087:INFO:Checking exceptions
2024-09-04 21:10:20,091:INFO:Importing libraries
2024-09-04 21:10:20,092:INFO:Copying training dataset
2024-09-04 21:10:20,094:INFO:Defining folds
2024-09-04 21:10:20,094:INFO:Declaring metric variables
2024-09-04 21:10:20,094:INFO:Importing untrained model
2024-09-04 21:10:20,095:INFO:Declaring custom model
2024-09-04 21:10:20,096:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:10:20,098:INFO:Cross validation set to False
2024-09-04 21:10:20,099:INFO:Fitting Model
2024-09-04 21:12:14,060:INFO:Initializing finalize_model()
2024-09-04 21:12:14,060:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 21:12:14,061:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:12:14,069:INFO:Initializing create_model()
2024-09-04 21:12:14,069:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B285FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6789, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:12:14,069:INFO:Checking exceptions
2024-09-04 21:12:14,070:INFO:Importing libraries
2024-09-04 21:12:14,071:INFO:Copying training dataset
2024-09-04 21:12:14,072:INFO:Defining folds
2024-09-04 21:12:14,073:INFO:Declaring metric variables
2024-09-04 21:12:14,073:INFO:Importing untrained model
2024-09-04 21:12:14,073:INFO:Declaring custom model
2024-09-04 21:12:14,075:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:12:14,076:INFO:Cross validation set to False
2024-09-04 21:12:14,076:INFO:Fitting Model
2024-09-04 21:14:00,939:INFO:PyCaret ClassificationExperiment
2024-09-04 21:14:00,939:INFO:Logging name: clf-default-name
2024-09-04 21:14:00,939:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-04 21:14:00,939:INFO:version 3.3.2
2024-09-04 21:14:00,939:INFO:Initializing setup()
2024-09-04 21:14:00,939:INFO:self.USI: d488
2024-09-04 21:14:00,939:INFO:self._variable_keys: {'pipeline', '_ml_usecase', 'y_train', 'y', 'y_test', 'exp_id', 'seed', 'X', 'exp_name_log', 'fold_generator', 'gpu_n_jobs_param', 'fold_shuffle_param', 'fold_groups_param', 'data', 'memory', 'n_jobs_param', 'is_multiclass', '_available_plots', 'idx', 'log_plots_param', 'X_train', 'X_test', 'html_param', 'target_param', 'logging_param', 'USI', 'gpu_param', 'fix_imbalance'}
2024-09-04 21:14:00,939:INFO:Checking environment
2024-09-04 21:14:00,939:INFO:python_version: 3.11.9
2024-09-04 21:14:00,939:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-04 21:14:00,939:INFO:machine: AMD64
2024-09-04 21:14:00,939:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-04 21:14:00,945:INFO:Memory: svmem(total=16867028992, available=2304094208, percent=86.3, used=14562934784, free=2304094208)
2024-09-04 21:14:00,945:INFO:Physical Core: 6
2024-09-04 21:14:00,945:INFO:Logical Core: 12
2024-09-04 21:14:00,945:INFO:Checking libraries
2024-09-04 21:14:00,945:INFO:System:
2024-09-04 21:14:00,945:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-04 21:14:00,945:INFO:executable: c:\Users\ardav\miniconda3\envs\vsc\python.exe
2024-09-04 21:14:00,945:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-04 21:14:00,945:INFO:PyCaret required dependencies:
2024-09-04 21:14:00,945:INFO:                 pip: 23.2.1
2024-09-04 21:14:00,945:INFO:          setuptools: 67.8.0
2024-09-04 21:14:00,946:INFO:             pycaret: 3.3.2
2024-09-04 21:14:00,946:INFO:             IPython: 8.14.0
2024-09-04 21:14:00,946:INFO:          ipywidgets: 8.1.5
2024-09-04 21:14:00,946:INFO:                tqdm: 4.66.5
2024-09-04 21:14:00,946:INFO:               numpy: 1.24.3
2024-09-04 21:14:00,946:INFO:              pandas: 2.0.3
2024-09-04 21:14:00,946:INFO:              jinja2: 3.1.4
2024-09-04 21:14:00,946:INFO:               scipy: 1.10.1
2024-09-04 21:14:00,946:INFO:              joblib: 1.2.0
2024-09-04 21:14:00,946:INFO:             sklearn: 1.4.2
2024-09-04 21:14:00,946:INFO:                pyod: 2.0.1
2024-09-04 21:14:00,946:INFO:            imblearn: 0.12.3
2024-09-04 21:14:00,946:INFO:   category_encoders: 2.6.3
2024-09-04 21:14:00,946:INFO:            lightgbm: 4.5.0
2024-09-04 21:14:00,946:INFO:               numba: 0.60.0
2024-09-04 21:14:00,946:INFO:            requests: 2.32.3
2024-09-04 21:14:00,946:INFO:          matplotlib: 3.7.1
2024-09-04 21:14:00,946:INFO:          scikitplot: 0.3.7
2024-09-04 21:14:00,946:INFO:         yellowbrick: 1.5
2024-09-04 21:14:00,946:INFO:              plotly: 5.16.1
2024-09-04 21:14:00,946:INFO:    plotly-resampler: Not installed
2024-09-04 21:14:00,946:INFO:             kaleido: 0.2.1
2024-09-04 21:14:00,946:INFO:           schemdraw: 0.15
2024-09-04 21:14:00,946:INFO:         statsmodels: 0.14.2
2024-09-04 21:14:00,946:INFO:              sktime: 0.26.0
2024-09-04 21:14:00,946:INFO:               tbats: 1.1.3
2024-09-04 21:14:00,946:INFO:            pmdarima: 2.0.4
2024-09-04 21:14:00,946:INFO:              psutil: 5.9.0
2024-09-04 21:14:00,946:INFO:          markupsafe: 2.1.3
2024-09-04 21:14:00,946:INFO:             pickle5: Not installed
2024-09-04 21:14:00,946:INFO:         cloudpickle: 3.0.0
2024-09-04 21:14:00,946:INFO:         deprecation: 2.1.0
2024-09-04 21:14:00,946:INFO:              xxhash: 3.5.0
2024-09-04 21:14:00,946:INFO:           wurlitzer: Not installed
2024-09-04 21:14:00,946:INFO:PyCaret optional dependencies:
2024-09-04 21:14:00,947:INFO:                shap: Not installed
2024-09-04 21:14:00,947:INFO:           interpret: Not installed
2024-09-04 21:14:00,947:INFO:                umap: Not installed
2024-09-04 21:14:00,947:INFO:     ydata_profiling: Not installed
2024-09-04 21:14:00,947:INFO:  explainerdashboard: Not installed
2024-09-04 21:14:00,947:INFO:             autoviz: Not installed
2024-09-04 21:14:00,947:INFO:           fairlearn: Not installed
2024-09-04 21:14:00,947:INFO:          deepchecks: Not installed
2024-09-04 21:14:00,947:INFO:             xgboost: 2.0.2
2024-09-04 21:14:00,947:INFO:            catboost: 1.2.5
2024-09-04 21:14:00,947:INFO:              kmodes: Not installed
2024-09-04 21:14:00,947:INFO:             mlxtend: Not installed
2024-09-04 21:14:00,947:INFO:       statsforecast: Not installed
2024-09-04 21:14:00,947:INFO:        tune_sklearn: Not installed
2024-09-04 21:14:00,947:INFO:                 ray: Not installed
2024-09-04 21:14:00,947:INFO:            hyperopt: Not installed
2024-09-04 21:14:00,947:INFO:              optuna: 4.0.0
2024-09-04 21:14:00,947:INFO:               skopt: Not installed
2024-09-04 21:14:00,947:INFO:              mlflow: Not installed
2024-09-04 21:14:00,947:INFO:              gradio: 4.41.0
2024-09-04 21:14:00,947:INFO:             fastapi: 0.112.1
2024-09-04 21:14:00,947:INFO:             uvicorn: 0.30.6
2024-09-04 21:14:00,947:INFO:              m2cgen: Not installed
2024-09-04 21:14:00,947:INFO:           evidently: Not installed
2024-09-04 21:14:00,947:INFO:               fugue: Not installed
2024-09-04 21:14:00,947:INFO:           streamlit: 1.38.0
2024-09-04 21:14:00,947:INFO:             prophet: Not installed
2024-09-04 21:14:00,947:INFO:None
2024-09-04 21:14:00,947:INFO:Set up data.
2024-09-04 21:14:00,962:INFO:Set up folding strategy.
2024-09-04 21:14:00,962:INFO:Set up train/test split.
2024-09-04 21:14:00,974:INFO:Set up index.
2024-09-04 21:14:00,975:INFO:Assigning column types.
2024-09-04 21:14:00,986:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-04 21:14:01,022:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,023:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,048:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,050:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,090:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,090:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,116:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,119:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,119:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-04 21:14:01,159:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,184:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,187:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,226:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-04 21:14:01,249:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,255:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,258:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-04 21:14:01,361:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,364:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,443:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:01,446:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:01,448:INFO:Preparing preprocessing pipeline...
2024-09-04 21:14:01,451:INFO:Set up simple imputation.
2024-09-04 21:14:01,451:INFO:Set up imbalanced handling.
2024-09-04 21:14:01,453:INFO:Set up column name cleaning.
2024-09-04 21:14:02,043:INFO:Finished creating preprocessing pipeline.
2024-09-04 21:14:02,051:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ardav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer...
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTETomek(n_jobs=None,
                                                                                   random_state=2021,
                                                                                   sampling_strategy='auto',
                                                                                   smote=None,
                                                                                   tomek=None)))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-04 21:14:02,052:INFO:Creating final display dataframe.
2024-09-04 21:14:02,631:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3           Original data shape   
4        Transformed data shape   
5   Transformed train set shape   
6    Transformed test set shape   
7              Numeric features   
8                    Preprocess   
9               Imputation type   
10           Numeric imputation   
11       Categorical imputation   
12                Fix imbalance   
13         Fix imbalance method   
14               Fold Generator   
15                  Fold Number   
16                     CPU Jobs   
17                      Use GPU   
18               Log Experiment   
19              Experiment Name   
20                          USI   

                                                Value  
0                                                 123  
1                                              income  
2                                              Binary  
3                                         (26035, 16)  
4                                         (32159, 16)  
5                                         (24348, 16)  
6                                          (7811, 16)  
7                                                  15  
8                                                True  
9                                              simple  
10                                               mean  
11                                               mode  
12                                               True  
13  SMOTETomek(n_jobs=None, random_state=2021, sam...  
14                                    StratifiedKFold  
15                                                 10  
16                                                 -1  
17                                              False  
18                                              False  
19                                   clf-default-name  
20                                               d488  
2024-09-04 21:14:02,722:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:02,725:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:02,814:INFO:Soft dependency imported: xgboost: 2.0.2
2024-09-04 21:14:02,816:INFO:Soft dependency imported: catboost: 1.2.5
2024-09-04 21:14:02,817:INFO:setup() successfully completed in 1.88s...............
2024-09-04 21:14:02,817:INFO:Initializing compare_models()
2024-09-04 21:14:02,818:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-04 21:14:02,818:INFO:Checking exceptions
2024-09-04 21:14:02,826:INFO:Preparing display monitor
2024-09-04 21:14:02,853:INFO:Initializing Logistic Regression
2024-09-04 21:14:02,853:INFO:Total runtime is 8.881092071533203e-06 minutes
2024-09-04 21:14:02,859:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:02,859:INFO:Initializing create_model()
2024-09-04 21:14:02,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:02,860:INFO:Checking exceptions
2024-09-04 21:14:02,860:INFO:Importing libraries
2024-09-04 21:14:02,860:INFO:Copying training dataset
2024-09-04 21:14:02,884:INFO:Defining folds
2024-09-04 21:14:02,884:INFO:Declaring metric variables
2024-09-04 21:14:02,889:INFO:Importing untrained model
2024-09-04 21:14:02,895:INFO:Logistic Regression Imported successfully
2024-09-04 21:14:02,904:INFO:Starting cross validation
2024-09-04 21:14:02,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:15,311:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,332:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,499:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,505:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,623:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,678:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,735:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,774:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,790:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,816:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-04 21:14:15,846:INFO:Calculating mean and std
2024-09-04 21:14:15,847:INFO:Creating metrics dataframe
2024-09-04 21:14:15,850:INFO:Uploading results into container
2024-09-04 21:14:15,852:INFO:Uploading model into container now
2024-09-04 21:14:15,853:INFO:_master_model_container: 1
2024-09-04 21:14:15,854:INFO:_display_container: 2
2024-09-04 21:14:15,855:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-04 21:14:15,855:INFO:create_model() successfully completed......................................
2024-09-04 21:14:16,071:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:16,071:INFO:Creating metrics dataframe
2024-09-04 21:14:16,078:INFO:Initializing K Neighbors Classifier
2024-09-04 21:14:16,078:INFO:Total runtime is 0.22041971683502198 minutes
2024-09-04 21:14:16,081:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:16,082:INFO:Initializing create_model()
2024-09-04 21:14:16,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:16,082:INFO:Checking exceptions
2024-09-04 21:14:16,082:INFO:Importing libraries
2024-09-04 21:14:16,082:INFO:Copying training dataset
2024-09-04 21:14:16,097:INFO:Defining folds
2024-09-04 21:14:16,097:INFO:Declaring metric variables
2024-09-04 21:14:16,100:INFO:Importing untrained model
2024-09-04 21:14:16,105:INFO:K Neighbors Classifier Imported successfully
2024-09-04 21:14:16,112:INFO:Starting cross validation
2024-09-04 21:14:16,114:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:20,417:INFO:Calculating mean and std
2024-09-04 21:14:20,418:INFO:Creating metrics dataframe
2024-09-04 21:14:20,420:INFO:Uploading results into container
2024-09-04 21:14:20,422:INFO:Uploading model into container now
2024-09-04 21:14:20,423:INFO:_master_model_container: 2
2024-09-04 21:14:20,423:INFO:_display_container: 2
2024-09-04 21:14:20,424:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-04 21:14:20,424:INFO:create_model() successfully completed......................................
2024-09-04 21:14:20,617:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:20,618:INFO:Creating metrics dataframe
2024-09-04 21:14:20,625:INFO:Initializing Naive Bayes
2024-09-04 21:14:20,625:INFO:Total runtime is 0.29620882670084636 minutes
2024-09-04 21:14:20,629:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:20,629:INFO:Initializing create_model()
2024-09-04 21:14:20,629:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:20,629:INFO:Checking exceptions
2024-09-04 21:14:20,630:INFO:Importing libraries
2024-09-04 21:14:20,630:INFO:Copying training dataset
2024-09-04 21:14:20,644:INFO:Defining folds
2024-09-04 21:14:20,644:INFO:Declaring metric variables
2024-09-04 21:14:20,648:INFO:Importing untrained model
2024-09-04 21:14:20,651:INFO:Naive Bayes Imported successfully
2024-09-04 21:14:20,663:INFO:Starting cross validation
2024-09-04 21:14:20,666:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:22,077:INFO:Calculating mean and std
2024-09-04 21:14:22,079:INFO:Creating metrics dataframe
2024-09-04 21:14:22,083:INFO:Uploading results into container
2024-09-04 21:14:22,084:INFO:Uploading model into container now
2024-09-04 21:14:22,085:INFO:_master_model_container: 3
2024-09-04 21:14:22,085:INFO:_display_container: 2
2024-09-04 21:14:22,085:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-04 21:14:22,086:INFO:create_model() successfully completed......................................
2024-09-04 21:14:22,294:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:22,295:INFO:Creating metrics dataframe
2024-09-04 21:14:22,303:INFO:Initializing Decision Tree Classifier
2024-09-04 21:14:22,303:INFO:Total runtime is 0.32416011095047 minutes
2024-09-04 21:14:22,306:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:22,307:INFO:Initializing create_model()
2024-09-04 21:14:22,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:22,307:INFO:Checking exceptions
2024-09-04 21:14:22,307:INFO:Importing libraries
2024-09-04 21:14:22,307:INFO:Copying training dataset
2024-09-04 21:14:22,323:INFO:Defining folds
2024-09-04 21:14:22,323:INFO:Declaring metric variables
2024-09-04 21:14:22,327:INFO:Importing untrained model
2024-09-04 21:14:22,331:INFO:Decision Tree Classifier Imported successfully
2024-09-04 21:14:22,338:INFO:Starting cross validation
2024-09-04 21:14:22,339:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:24,068:INFO:Calculating mean and std
2024-09-04 21:14:24,070:INFO:Creating metrics dataframe
2024-09-04 21:14:24,072:INFO:Uploading results into container
2024-09-04 21:14:24,073:INFO:Uploading model into container now
2024-09-04 21:14:24,073:INFO:_master_model_container: 4
2024-09-04 21:14:24,074:INFO:_display_container: 2
2024-09-04 21:14:24,074:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-04 21:14:24,074:INFO:create_model() successfully completed......................................
2024-09-04 21:14:24,293:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:24,293:INFO:Creating metrics dataframe
2024-09-04 21:14:24,300:INFO:Initializing SVM - Linear Kernel
2024-09-04 21:14:24,300:INFO:Total runtime is 0.3574581662813823 minutes
2024-09-04 21:14:24,304:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:24,305:INFO:Initializing create_model()
2024-09-04 21:14:24,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:24,305:INFO:Checking exceptions
2024-09-04 21:14:24,305:INFO:Importing libraries
2024-09-04 21:14:24,305:INFO:Copying training dataset
2024-09-04 21:14:24,321:INFO:Defining folds
2024-09-04 21:14:24,322:INFO:Declaring metric variables
2024-09-04 21:14:24,326:INFO:Importing untrained model
2024-09-04 21:14:24,331:INFO:SVM - Linear Kernel Imported successfully
2024-09-04 21:14:24,339:INFO:Starting cross validation
2024-09-04 21:14:24,341:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:27,054:INFO:Calculating mean and std
2024-09-04 21:14:27,055:INFO:Creating metrics dataframe
2024-09-04 21:14:27,057:INFO:Uploading results into container
2024-09-04 21:14:27,058:INFO:Uploading model into container now
2024-09-04 21:14:27,058:INFO:_master_model_container: 5
2024-09-04 21:14:27,059:INFO:_display_container: 2
2024-09-04 21:14:27,059:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-04 21:14:27,059:INFO:create_model() successfully completed......................................
2024-09-04 21:14:27,245:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:27,245:INFO:Creating metrics dataframe
2024-09-04 21:14:27,253:INFO:Initializing Ridge Classifier
2024-09-04 21:14:27,253:INFO:Total runtime is 0.4066672444343567 minutes
2024-09-04 21:14:27,257:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:27,257:INFO:Initializing create_model()
2024-09-04 21:14:27,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:27,257:INFO:Checking exceptions
2024-09-04 21:14:27,257:INFO:Importing libraries
2024-09-04 21:14:27,258:INFO:Copying training dataset
2024-09-04 21:14:27,270:INFO:Defining folds
2024-09-04 21:14:27,271:INFO:Declaring metric variables
2024-09-04 21:14:27,275:INFO:Importing untrained model
2024-09-04 21:14:27,279:INFO:Ridge Classifier Imported successfully
2024-09-04 21:14:27,286:INFO:Starting cross validation
2024-09-04 21:14:27,289:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:28,669:INFO:Calculating mean and std
2024-09-04 21:14:28,671:INFO:Creating metrics dataframe
2024-09-04 21:14:28,673:INFO:Uploading results into container
2024-09-04 21:14:28,674:INFO:Uploading model into container now
2024-09-04 21:14:28,674:INFO:_master_model_container: 6
2024-09-04 21:14:28,674:INFO:_display_container: 2
2024-09-04 21:14:28,675:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-04 21:14:28,675:INFO:create_model() successfully completed......................................
2024-09-04 21:14:28,875:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:28,875:INFO:Creating metrics dataframe
2024-09-04 21:14:28,884:INFO:Initializing Random Forest Classifier
2024-09-04 21:14:28,884:INFO:Total runtime is 0.4338516712188721 minutes
2024-09-04 21:14:28,888:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:28,888:INFO:Initializing create_model()
2024-09-04 21:14:28,889:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:28,889:INFO:Checking exceptions
2024-09-04 21:14:28,889:INFO:Importing libraries
2024-09-04 21:14:28,889:INFO:Copying training dataset
2024-09-04 21:14:28,903:INFO:Defining folds
2024-09-04 21:14:28,903:INFO:Declaring metric variables
2024-09-04 21:14:28,907:INFO:Importing untrained model
2024-09-04 21:14:28,911:INFO:Random Forest Classifier Imported successfully
2024-09-04 21:14:28,917:INFO:Starting cross validation
2024-09-04 21:14:28,919:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:35,193:INFO:Calculating mean and std
2024-09-04 21:14:35,195:INFO:Creating metrics dataframe
2024-09-04 21:14:35,199:INFO:Uploading results into container
2024-09-04 21:14:35,201:INFO:Uploading model into container now
2024-09-04 21:14:35,201:INFO:_master_model_container: 7
2024-09-04 21:14:35,202:INFO:_display_container: 2
2024-09-04 21:14:35,202:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-04 21:14:35,203:INFO:create_model() successfully completed......................................
2024-09-04 21:14:35,423:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:35,423:INFO:Creating metrics dataframe
2024-09-04 21:14:35,433:INFO:Initializing Quadratic Discriminant Analysis
2024-09-04 21:14:35,433:INFO:Total runtime is 0.5430010477701823 minutes
2024-09-04 21:14:35,437:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:35,437:INFO:Initializing create_model()
2024-09-04 21:14:35,437:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:35,437:INFO:Checking exceptions
2024-09-04 21:14:35,437:INFO:Importing libraries
2024-09-04 21:14:35,437:INFO:Copying training dataset
2024-09-04 21:14:35,461:INFO:Defining folds
2024-09-04 21:14:35,462:INFO:Declaring metric variables
2024-09-04 21:14:35,468:INFO:Importing untrained model
2024-09-04 21:14:35,476:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-04 21:14:35,487:INFO:Starting cross validation
2024-09-04 21:14:35,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:36,911:INFO:Calculating mean and std
2024-09-04 21:14:36,912:INFO:Creating metrics dataframe
2024-09-04 21:14:36,915:INFO:Uploading results into container
2024-09-04 21:14:36,916:INFO:Uploading model into container now
2024-09-04 21:14:36,916:INFO:_master_model_container: 8
2024-09-04 21:14:36,916:INFO:_display_container: 2
2024-09-04 21:14:36,917:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-04 21:14:36,917:INFO:create_model() successfully completed......................................
2024-09-04 21:14:37,114:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:37,114:INFO:Creating metrics dataframe
2024-09-04 21:14:37,124:INFO:Initializing Ada Boost Classifier
2024-09-04 21:14:37,124:INFO:Total runtime is 0.5711918552716574 minutes
2024-09-04 21:14:37,128:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:37,128:INFO:Initializing create_model()
2024-09-04 21:14:37,128:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:37,128:INFO:Checking exceptions
2024-09-04 21:14:37,128:INFO:Importing libraries
2024-09-04 21:14:37,128:INFO:Copying training dataset
2024-09-04 21:14:37,142:INFO:Defining folds
2024-09-04 21:14:37,142:INFO:Declaring metric variables
2024-09-04 21:14:37,145:INFO:Importing untrained model
2024-09-04 21:14:37,148:INFO:Ada Boost Classifier Imported successfully
2024-09-04 21:14:37,156:INFO:Starting cross validation
2024-09-04 21:14:37,158:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:38,393:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,397:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,399:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,400:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,427:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,436:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,442:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,446:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,461:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:38,468:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-04 21:14:40,766:INFO:Calculating mean and std
2024-09-04 21:14:40,768:INFO:Creating metrics dataframe
2024-09-04 21:14:40,770:INFO:Uploading results into container
2024-09-04 21:14:40,771:INFO:Uploading model into container now
2024-09-04 21:14:40,772:INFO:_master_model_container: 9
2024-09-04 21:14:40,772:INFO:_display_container: 2
2024-09-04 21:14:40,772:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-04 21:14:40,772:INFO:create_model() successfully completed......................................
2024-09-04 21:14:40,980:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:40,980:INFO:Creating metrics dataframe
2024-09-04 21:14:40,990:INFO:Initializing Gradient Boosting Classifier
2024-09-04 21:14:40,990:INFO:Total runtime is 0.6356219490369162 minutes
2024-09-04 21:14:40,993:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:40,994:INFO:Initializing create_model()
2024-09-04 21:14:40,994:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:40,994:INFO:Checking exceptions
2024-09-04 21:14:40,994:INFO:Importing libraries
2024-09-04 21:14:40,994:INFO:Copying training dataset
2024-09-04 21:14:41,015:INFO:Defining folds
2024-09-04 21:14:41,015:INFO:Declaring metric variables
2024-09-04 21:14:41,019:INFO:Importing untrained model
2024-09-04 21:14:41,022:INFO:Gradient Boosting Classifier Imported successfully
2024-09-04 21:14:41,031:INFO:Starting cross validation
2024-09-04 21:14:41,034:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:50,318:INFO:Calculating mean and std
2024-09-04 21:14:50,319:INFO:Creating metrics dataframe
2024-09-04 21:14:50,322:INFO:Uploading results into container
2024-09-04 21:14:50,322:INFO:Uploading model into container now
2024-09-04 21:14:50,323:INFO:_master_model_container: 10
2024-09-04 21:14:50,323:INFO:_display_container: 2
2024-09-04 21:14:50,323:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-04 21:14:50,323:INFO:create_model() successfully completed......................................
2024-09-04 21:14:50,516:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:50,516:INFO:Creating metrics dataframe
2024-09-04 21:14:50,525:INFO:Initializing Linear Discriminant Analysis
2024-09-04 21:14:50,525:INFO:Total runtime is 0.7945380886395773 minutes
2024-09-04 21:14:50,528:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:50,529:INFO:Initializing create_model()
2024-09-04 21:14:50,529:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:50,529:INFO:Checking exceptions
2024-09-04 21:14:50,529:INFO:Importing libraries
2024-09-04 21:14:50,529:INFO:Copying training dataset
2024-09-04 21:14:50,543:INFO:Defining folds
2024-09-04 21:14:50,544:INFO:Declaring metric variables
2024-09-04 21:14:50,549:INFO:Importing untrained model
2024-09-04 21:14:50,552:INFO:Linear Discriminant Analysis Imported successfully
2024-09-04 21:14:50,560:INFO:Starting cross validation
2024-09-04 21:14:50,561:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:52,009:INFO:Calculating mean and std
2024-09-04 21:14:52,011:INFO:Creating metrics dataframe
2024-09-04 21:14:52,014:INFO:Uploading results into container
2024-09-04 21:14:52,016:INFO:Uploading model into container now
2024-09-04 21:14:52,017:INFO:_master_model_container: 11
2024-09-04 21:14:52,017:INFO:_display_container: 2
2024-09-04 21:14:52,017:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-04 21:14:52,018:INFO:create_model() successfully completed......................................
2024-09-04 21:14:52,229:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:52,229:INFO:Creating metrics dataframe
2024-09-04 21:14:52,240:INFO:Initializing Extra Trees Classifier
2024-09-04 21:14:52,240:INFO:Total runtime is 0.8231150070826213 minutes
2024-09-04 21:14:52,243:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:52,244:INFO:Initializing create_model()
2024-09-04 21:14:52,244:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:52,244:INFO:Checking exceptions
2024-09-04 21:14:52,244:INFO:Importing libraries
2024-09-04 21:14:52,244:INFO:Copying training dataset
2024-09-04 21:14:52,259:INFO:Defining folds
2024-09-04 21:14:52,259:INFO:Declaring metric variables
2024-09-04 21:14:52,262:INFO:Importing untrained model
2024-09-04 21:14:52,266:INFO:Extra Trees Classifier Imported successfully
2024-09-04 21:14:52,273:INFO:Starting cross validation
2024-09-04 21:14:52,275:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:57,063:INFO:Calculating mean and std
2024-09-04 21:14:57,066:INFO:Creating metrics dataframe
2024-09-04 21:14:57,070:INFO:Uploading results into container
2024-09-04 21:14:57,071:INFO:Uploading model into container now
2024-09-04 21:14:57,072:INFO:_master_model_container: 12
2024-09-04 21:14:57,072:INFO:_display_container: 2
2024-09-04 21:14:57,073:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-04 21:14:57,074:INFO:create_model() successfully completed......................................
2024-09-04 21:14:57,309:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:57,309:INFO:Creating metrics dataframe
2024-09-04 21:14:57,320:INFO:Initializing Extreme Gradient Boosting
2024-09-04 21:14:57,320:INFO:Total runtime is 0.9077814022699993 minutes
2024-09-04 21:14:57,324:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:57,325:INFO:Initializing create_model()
2024-09-04 21:14:57,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:57,326:INFO:Checking exceptions
2024-09-04 21:14:57,326:INFO:Importing libraries
2024-09-04 21:14:57,326:INFO:Copying training dataset
2024-09-04 21:14:57,345:INFO:Defining folds
2024-09-04 21:14:57,345:INFO:Declaring metric variables
2024-09-04 21:14:57,349:INFO:Importing untrained model
2024-09-04 21:14:57,354:INFO:Extreme Gradient Boosting Imported successfully
2024-09-04 21:14:57,362:INFO:Starting cross validation
2024-09-04 21:14:57,365:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:14:59,511:INFO:Calculating mean and std
2024-09-04 21:14:59,513:INFO:Creating metrics dataframe
2024-09-04 21:14:59,516:INFO:Uploading results into container
2024-09-04 21:14:59,517:INFO:Uploading model into container now
2024-09-04 21:14:59,517:INFO:_master_model_container: 13
2024-09-04 21:14:59,517:INFO:_display_container: 2
2024-09-04 21:14:59,518:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-04 21:14:59,518:INFO:create_model() successfully completed......................................
2024-09-04 21:14:59,717:INFO:SubProcess create_model() end ==================================
2024-09-04 21:14:59,717:INFO:Creating metrics dataframe
2024-09-04 21:14:59,727:INFO:Initializing Light Gradient Boosting Machine
2024-09-04 21:14:59,728:INFO:Total runtime is 0.9479098002115887 minutes
2024-09-04 21:14:59,731:INFO:SubProcess create_model() called ==================================
2024-09-04 21:14:59,731:INFO:Initializing create_model()
2024-09-04 21:14:59,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:14:59,732:INFO:Checking exceptions
2024-09-04 21:14:59,732:INFO:Importing libraries
2024-09-04 21:14:59,732:INFO:Copying training dataset
2024-09-04 21:14:59,749:INFO:Defining folds
2024-09-04 21:14:59,749:INFO:Declaring metric variables
2024-09-04 21:14:59,752:INFO:Importing untrained model
2024-09-04 21:14:59,756:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:14:59,765:INFO:Starting cross validation
2024-09-04 21:14:59,767:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:15:03,063:INFO:Calculating mean and std
2024-09-04 21:15:03,065:INFO:Creating metrics dataframe
2024-09-04 21:15:03,067:INFO:Uploading results into container
2024-09-04 21:15:03,068:INFO:Uploading model into container now
2024-09-04 21:15:03,069:INFO:_master_model_container: 14
2024-09-04 21:15:03,069:INFO:_display_container: 2
2024-09-04 21:15:03,070:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:15:03,070:INFO:create_model() successfully completed......................................
2024-09-04 21:15:03,324:INFO:SubProcess create_model() end ==================================
2024-09-04 21:15:03,324:INFO:Creating metrics dataframe
2024-09-04 21:15:03,336:INFO:Initializing CatBoost Classifier
2024-09-04 21:15:03,337:INFO:Total runtime is 1.0080609679222108 minutes
2024-09-04 21:15:03,340:INFO:SubProcess create_model() called ==================================
2024-09-04 21:15:03,340:INFO:Initializing create_model()
2024-09-04 21:15:03,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:15:03,340:INFO:Checking exceptions
2024-09-04 21:15:03,341:INFO:Importing libraries
2024-09-04 21:15:03,341:INFO:Copying training dataset
2024-09-04 21:15:03,355:INFO:Defining folds
2024-09-04 21:15:03,356:INFO:Declaring metric variables
2024-09-04 21:15:03,360:INFO:Importing untrained model
2024-09-04 21:15:03,364:INFO:CatBoost Classifier Imported successfully
2024-09-04 21:15:03,372:INFO:Starting cross validation
2024-09-04 21:15:03,373:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:16:03,912:INFO:Calculating mean and std
2024-09-04 21:16:03,915:INFO:Creating metrics dataframe
2024-09-04 21:16:03,917:INFO:Uploading results into container
2024-09-04 21:16:03,918:INFO:Uploading model into container now
2024-09-04 21:16:03,919:INFO:_master_model_container: 15
2024-09-04 21:16:03,920:INFO:_display_container: 2
2024-09-04 21:16:03,920:INFO:<catboost.core.CatBoostClassifier object at 0x000001D4742B6B10>
2024-09-04 21:16:03,920:INFO:create_model() successfully completed......................................
2024-09-04 21:16:04,180:INFO:SubProcess create_model() end ==================================
2024-09-04 21:16:04,181:INFO:Creating metrics dataframe
2024-09-04 21:16:04,195:INFO:Initializing Dummy Classifier
2024-09-04 21:16:04,196:INFO:Total runtime is 2.0223828474680583 minutes
2024-09-04 21:16:04,200:INFO:SubProcess create_model() called ==================================
2024-09-04 21:16:04,200:INFO:Initializing create_model()
2024-09-04 21:16:04,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D476194750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:16:04,201:INFO:Checking exceptions
2024-09-04 21:16:04,201:INFO:Importing libraries
2024-09-04 21:16:04,201:INFO:Copying training dataset
2024-09-04 21:16:04,227:INFO:Defining folds
2024-09-04 21:16:04,227:INFO:Declaring metric variables
2024-09-04 21:16:04,233:INFO:Importing untrained model
2024-09-04 21:16:04,240:INFO:Dummy Classifier Imported successfully
2024-09-04 21:16:04,254:INFO:Starting cross validation
2024-09-04 21:16:04,258:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:16:05,345:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,399:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,534:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,671:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,676:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,863:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,963:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:05,971:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:06,078:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:06,163:WARNING:c:\Users\ardav\miniconda3\envs\vsc\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-04 21:16:06,171:INFO:Calculating mean and std
2024-09-04 21:16:06,173:INFO:Creating metrics dataframe
2024-09-04 21:16:06,176:INFO:Uploading results into container
2024-09-04 21:16:06,176:INFO:Uploading model into container now
2024-09-04 21:16:06,177:INFO:_master_model_container: 16
2024-09-04 21:16:06,177:INFO:_display_container: 2
2024-09-04 21:16:06,178:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-04 21:16:06,178:INFO:create_model() successfully completed......................................
2024-09-04 21:16:06,399:INFO:SubProcess create_model() end ==================================
2024-09-04 21:16:06,399:INFO:Creating metrics dataframe
2024-09-04 21:16:06,429:INFO:Initializing create_model()
2024-09-04 21:16:06,429:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:16:06,430:INFO:Checking exceptions
2024-09-04 21:16:06,432:INFO:Importing libraries
2024-09-04 21:16:06,432:INFO:Copying training dataset
2024-09-04 21:16:06,454:INFO:Defining folds
2024-09-04 21:16:06,454:INFO:Declaring metric variables
2024-09-04 21:16:06,454:INFO:Importing untrained model
2024-09-04 21:16:06,454:INFO:Declaring custom model
2024-09-04 21:16:06,455:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:16:06,458:INFO:Cross validation set to False
2024-09-04 21:16:06,458:INFO:Fitting Model
2024-09-04 21:16:07,150:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:16:07,153:INFO:[LightGBM] [Info] Number of positive: 12174, number of negative: 12174
2024-09-04 21:16:07,158:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003868 seconds.
2024-09-04 21:16:07,158:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-04 21:16:07,158:INFO:[LightGBM] [Info] Total Bins 3806
2024-09-04 21:16:07,159:INFO:[LightGBM] [Info] Number of data points in the train set: 24348, number of used features: 15
2024-09-04 21:16:07,159:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:16:07,560:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:16:07,560:INFO:create_model() successfully completed......................................
2024-09-04 21:16:07,835:INFO:_master_model_container: 16
2024-09-04 21:16:07,836:INFO:_display_container: 2
2024-09-04 21:16:07,836:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:16:07,836:INFO:compare_models() successfully completed......................................
2024-09-04 21:16:07,838:INFO:Initializing tune_model()
2024-09-04 21:16:07,839:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-04 21:16:07,839:INFO:Checking exceptions
2024-09-04 21:16:07,872:INFO:Copying training dataset
2024-09-04 21:16:07,888:INFO:Checking base model
2024-09-04 21:16:07,889:INFO:Base model : Light Gradient Boosting Machine
2024-09-04 21:16:07,895:INFO:Declaring metric variables
2024-09-04 21:16:07,902:INFO:Defining Hyperparameters
2024-09-04 21:16:08,163:INFO:Tuning with n_jobs=-1
2024-09-04 21:16:08,164:INFO:Initializing RandomizedSearchCV
2024-09-04 21:17:21,254:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2024-09-04 21:17:21,256:INFO:Hyperparameter search completed
2024-09-04 21:17:21,256:INFO:SubProcess create_model() called ==================================
2024-09-04 21:17:21,257:INFO:Initializing create_model()
2024-09-04 21:17:21,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D4742C2890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2024-09-04 21:17:21,257:INFO:Checking exceptions
2024-09-04 21:17:21,258:INFO:Importing libraries
2024-09-04 21:17:21,258:INFO:Copying training dataset
2024-09-04 21:17:21,281:INFO:Defining folds
2024-09-04 21:17:21,281:INFO:Declaring metric variables
2024-09-04 21:17:21,285:INFO:Importing untrained model
2024-09-04 21:17:21,285:INFO:Declaring custom model
2024-09-04 21:17:21,291:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:17:21,298:INFO:Starting cross validation
2024-09-04 21:17:21,300:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:17:25,731:INFO:Calculating mean and std
2024-09-04 21:17:25,732:INFO:Creating metrics dataframe
2024-09-04 21:17:25,738:INFO:Finalizing model
2024-09-04 21:17:26,491:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 21:17:26,491:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 21:17:26,491:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 21:17:26,508:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:17:26,508:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2024-09-04 21:17:26,509:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-09-04 21:17:26,509:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2024-09-04 21:17:26,509:INFO:[LightGBM] [Info] Number of positive: 12174, number of negative: 12174
2024-09-04 21:17:26,513:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.
2024-09-04 21:17:26,513:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:17:26,513:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:17:26,513:INFO:[LightGBM] [Info] Total Bins 3806
2024-09-04 21:17:26,513:INFO:[LightGBM] [Info] Number of data points in the train set: 24348, number of used features: 15
2024-09-04 21:17:26,514:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:17:26,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,954:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 21:17:26,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-09-04 21:17:26,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:26,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-04 21:17:27,144:INFO:Uploading results into container
2024-09-04 21:17:27,146:INFO:Uploading model into container now
2024-09-04 21:17:27,146:INFO:_master_model_container: 17
2024-09-04 21:17:27,146:INFO:_display_container: 3
2024-09-04 21:17:27,147:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:17:27,147:INFO:create_model() successfully completed......................................
2024-09-04 21:17:27,386:INFO:SubProcess create_model() end ==================================
2024-09-04 21:17:27,387:INFO:choose_better activated
2024-09-04 21:17:27,391:INFO:SubProcess create_model() called ==================================
2024-09-04 21:17:27,392:INFO:Initializing create_model()
2024-09-04 21:17:27,392:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:17:27,392:INFO:Checking exceptions
2024-09-04 21:17:27,393:INFO:Importing libraries
2024-09-04 21:17:27,393:INFO:Copying training dataset
2024-09-04 21:17:27,407:INFO:Defining folds
2024-09-04 21:17:27,407:INFO:Declaring metric variables
2024-09-04 21:17:27,407:INFO:Importing untrained model
2024-09-04 21:17:27,407:INFO:Declaring custom model
2024-09-04 21:17:27,407:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:17:27,408:INFO:Starting cross validation
2024-09-04 21:17:27,409:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-04 21:17:30,392:INFO:Calculating mean and std
2024-09-04 21:17:30,392:INFO:Creating metrics dataframe
2024-09-04 21:17:30,395:INFO:Finalizing model
2024-09-04 21:17:30,909:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:17:30,910:INFO:[LightGBM] [Info] Number of positive: 12174, number of negative: 12174
2024-09-04 21:17:30,912:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.
2024-09-04 21:17:30,912:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:17:30,912:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:17:30,913:INFO:[LightGBM] [Info] Total Bins 3806
2024-09-04 21:17:30,913:INFO:[LightGBM] [Info] Number of data points in the train set: 24348, number of used features: 15
2024-09-04 21:17:30,913:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:17:31,097:INFO:Uploading results into container
2024-09-04 21:17:31,098:INFO:Uploading model into container now
2024-09-04 21:17:31,099:INFO:_master_model_container: 18
2024-09-04 21:17:31,099:INFO:_display_container: 4
2024-09-04 21:17:31,099:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:17:31,099:INFO:create_model() successfully completed......................................
2024-09-04 21:17:31,308:INFO:SubProcess create_model() end ==================================
2024-09-04 21:17:31,309:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.7045
2024-09-04 21:17:31,309:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=260, n_jobs=-1, num_leaves=70, objective=None,
               random_state=123, reg_alpha=2, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6959
2024-09-04 21:17:31,310:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-09-04 21:17:31,310:INFO:choose_better completed
2024-09-04 21:17:31,310:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-04 21:17:31,319:INFO:_master_model_container: 18
2024-09-04 21:17:31,320:INFO:_display_container: 3
2024-09-04 21:17:31,320:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:17:31,320:INFO:tune_model() successfully completed......................................
2024-09-04 21:17:31,522:INFO:Initializing finalize_model()
2024-09-04 21:17:31,523:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-04 21:17:31,523:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-04 21:17:31,531:INFO:Initializing create_model()
2024-09-04 21:17:31,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-04 21:17:31,531:INFO:Checking exceptions
2024-09-04 21:17:31,532:INFO:Importing libraries
2024-09-04 21:17:31,532:INFO:Copying training dataset
2024-09-04 21:17:31,533:INFO:Defining folds
2024-09-04 21:17:31,533:INFO:Declaring metric variables
2024-09-04 21:17:31,533:INFO:Importing untrained model
2024-09-04 21:17:31,533:INFO:Declaring custom model
2024-09-04 21:17:31,533:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-04 21:17:31,534:INFO:Cross validation set to False
2024-09-04 21:17:31,534:INFO:Fitting Model
2024-09-04 21:17:32,264:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-04 21:17:32,265:INFO:[LightGBM] [Info] Number of positive: 17788, number of negative: 17788
2024-09-04 21:17:32,269:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001250 seconds.
2024-09-04 21:17:32,269:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-04 21:17:32,269:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-04 21:17:32,269:INFO:[LightGBM] [Info] Total Bins 3809
2024-09-04 21:17:32,269:INFO:[LightGBM] [Info] Number of data points in the train set: 35576, number of used features: 15
2024-09-04 21:17:32,270:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-09-04 21:17:32,557:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 21:17:32,557:INFO:create_model() successfully completed......................................
2024-09-04 21:17:32,779:INFO:_master_model_container: 18
2024-09-04 21:17:32,779:INFO:_display_container: 3
2024-09-04 21:17:32,788:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-09-04 21:17:32,788:INFO:finalize_model() successfully completed......................................
2024-09-04 21:17:32,984:INFO:Initializing predict_model()
2024-09-04 21:17:32,984:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D46692B1A0>)
2024-09-04 21:17:32,984:INFO:Checking exceptions
2024-09-04 21:17:32,984:INFO:Preloading libraries
2024-09-04 21:17:32,986:INFO:Set up data.
2024-09-04 21:17:32,991:INFO:Set up index.
2024-09-04 21:17:33,534:INFO:Initializing predict_model()
2024-09-04 21:17:33,535:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D46B59A910>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Tahun Kelahiran',
                                             'Kelas Pekerjaan', 'fnlwgt',
                                             'Pendidikan', 'Jenjang Pendidikan',
                                             'Status', 'Pekerjaan', 'Hubungan',
                                             'Etnis', 'sex', 'pendapatan',
                                             'pengeluaran', 'hours per week',
                                             'Asal Negara', 'jumlah_anak'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D4770C1120>)
2024-09-04 21:17:33,535:INFO:Checking exceptions
2024-09-04 21:17:33,536:INFO:Preloading libraries
2024-09-04 21:17:33,539:INFO:Set up data.
2024-09-04 21:17:33,548:INFO:Set up index.
